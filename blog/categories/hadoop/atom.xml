<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | Tug's Blog]]></title>
  <link href="http://tgrall.github.io/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://tgrall.github.io/"/>
  <updated>2020-05-16T18:24:59+02:00</updated>
  <id>http://tgrall.github.io/</id>
  <author>
    <name><![CDATA[Tug Grall]]></name>
    <email><![CDATA[tugdual@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Convert a CSV File to Apache Parquet With Drill]]></title>
    <link href="http://tgrall.github.io/blog/2015/08/17/convert-csv-file-to-apache-parquet-dot-dot-dot-with-drill/"/>
    <updated>2015-08-17T14:07:00+02:00</updated>
    <id>http://tgrall.github.io/blog/2015/08/17/convert-csv-file-to-apache-parquet-dot-dot-dot-with-drill</id>
    <content type="html"><![CDATA[<p>A very common use case when working with Hadoop is to store and query simple files (CSV, TSV, &hellip;); then to get better performance and efficient storage convert these files into more efficient format, for example <a href="https://parquet.apache.org/">Apache Parquet</a>.</p>

<p><a href="https://parquet.apache.org/">Apache Parquet</a> is a <a href="https://en.wikipedia.org/wiki/Column-oriented_DBMS">columnar storage format</a> available to any project in the Hadoop ecosystem. Apache Parquet has the following characteristics:</p>

<ul>
<li>Self-describing</li>
<li>Columnar format</li>
<li>Language-independent</li>
</ul>


<p>Let&rsquo;s take a concrete example, you can find many interesting Open Data sources that distribute data as CSV files- or equivalent format-. So you can store them into your distributed file system and use them in your applications/jobs/analytics queries. This is not the most efficient way especially when we know that these data won&rsquo;t move that often. So instead of simply storing the CSV let&rsquo;s copy this information into Parquet.</p>

<h3>How to convert CSV files into Parquet files?</h3>

<p>You can use code to achieve this, as you can see in the <a href="https://github.com/Parquet/parquet-compatibility/blob/master/parquet-compat/src/test/java/parquet/compat/test/ConvertUtils.java">ConvertUtils</a> sample/test class. You can use a simpler way with Apache Drill. Drill allows you save the result of a query as Parquet files.</p>

<p>The following steps will show you how to do convert a simple CSV into a Parquet file using Drill.</p>

<!-- more -->


<h4>Prerequisites</h4>

<ul>
<li>Apache Drill : Standalone <a href="https://drill.apache.org/">Apache Drill</a> or using <a href="https://www.mapr.com/products/mapr-sandbox-hadoop/download-sandbox-drill">Apache Drill Sandbox from MapR</a></li>
<li>Some CSV Files: for example <a href="http://www.flysfo.com/media/facts-statistics/air-traffic-statistics">Passenger Dataset from SFO Air Traffic Statistics</a></li>
</ul>


<h4>Querying the CSV file</h4>

<p>Let&rsquo;s execute a basic query:</p>

<pre><code class="sql">SELECT *
FROM dfs.`/opendata/Passenger/SFO_Passenger_Data/MonthlyPassengerData_200507_to_201503.csv`
LIMIT 5;

["200507","ATA Airlines","TZ","ATA Airlines","TZ","Domestic","US","Deplaned","Low Fare","Terminal 1","B","27271\r"]
...
...
</code></pre>

<p>As you can see, by default Drill processes each line as an array of columns, all values being simple String. So if you need to do some operations with these values (projection or where clause) you must use the column index, and cast the value to the proper type. You can see a simple example below:</p>

<pre><code>SELECT
columns[0] as `DATE`,
columns[1] as `AIRLINE`,
CAST(columns[11] AS DOUBLE) as `PASSENGER_COUNT`
FROM dfs.`/opendata/Passenger/SFO_Passenger_Data/*.csv`
WHERE CAST(columns[11] AS DOUBLE) &lt; 5
;

+---------+-----------------------------------+------------------+
|  DATE   |              AIRLINE              | PASSENGER_COUNT  |
+---------+-----------------------------------+------------------+
| 200610  | United Airlines - Pre 07/01/2013  | 2.0              |
...
...
</code></pre>

<p>We are now ready to create our Parquet files using the &ldquo;Create Table As Select&rdquo; (aka <a href="http://drill.apache.org/docs/create-table-as-ctas-command/">CTAS</a>)</p>

<pre><code class="sql">alter session set `store.format`='parquet';


CREATE TABLE dfs.tmp.`/stats/airport_data/` AS
SELECT
CAST(SUBSTR(columns[0],1,4) AS INT)  `YEAR`,
CAST(SUBSTR(columns[0],5,2) AS INT) `MONTH`,
columns[1] as `AIRLINE`,
columns[2] as `IATA_CODE`,
columns[3] as `AIRLINE_2`,
columns[4] as `IATA_CODE_2`,
columns[5] as `GEO_SUMMARY`,
columns[6] as `GEO_REGION`,
columns[7] as `ACTIVITY_CODE`,
columns[8] as `PRICE_CODE`,
columns[9] as `TERMINAL`,
columns[10] as `BOARDING_AREA`,
CAST(columns[11] AS DOUBLE) as `PASSENGER_COUNT`
FROM dfs.`/opendata/Passenger/SFO_Passenger_Data/*.csv`
</code></pre>

<p>That&rsquo;s it! You have now a Parquet file, a single file in our case since our dataset is really small. Apache Drill will create multiples files for the tables depending of the size and configuration your environment.</p>

<p>I invite you to read this Chapter in the Apache Drill documentation to learn more about <a href="https://drill.apache.org/docs/parquet-format/">Drill and Parquet</a>.</p>

<h3>Query Parquet Files</h3>

<p>Now that you have created your Parquet files you can use them in any of your Hadoop processes, but you can also use them in Drill, as follow:</p>

<pre><code>
SELECT *
FROM dfs.tmp.`/stats/airport_data/*`
</code></pre>

<h2>Conclusion</h2>

<p>In this article you have learned how to convert a CSV file using an Apache Drill query.</p>

<p>You can do that with any source supported by Drill, for example from JSON to Parquet, or even a complex join query between multiple data sources. You can also chose a different output format for example JSON, or a CSV.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Big Data... Is Hadoop the Good Way to Start?]]></title>
    <link href="http://tgrall.github.io/blog/2014/11/25/big-data-dot-dot-dot-is-hadoop-the-good-way-to-start/"/>
    <updated>2014-11-25T07:27:36+01:00</updated>
    <id>http://tgrall.github.io/blog/2014/11/25/big-data-dot-dot-dot-is-hadoop-the-good-way-to-start</id>
    <content type="html"><![CDATA[<p>In the past 2 years, I have met many developers, architects that are working on “big data” projects. This sounds amazing, but quite often the truth is not that amazing.</p>

<h4>TL;TR</h4>

<p>You believe that you have a big data project?</p>

<ul>
<li>Do not start with the installation of an Hadoop Cluster &ndash; the &ldquo;<em>how</em>&rdquo;</li>
<li>Start to talk to business people to understand their problem &ndash; the &ldquo;<em>why</em>&rdquo;</li>
<li>Understand the data you must process</li>
<li>Look at the volume &ndash; very often it is not &ldquo;that&rdquo; big</li>
<li>Then implement it, and take a simple approach, for example start with MongoDB + Apache Spark</li>
</ul>


<p><img class="center center <a" src="href="http://cdn.meme.am/instances/500x/47510205.jpg">http://cdn.meme.am/instances/500x/47510205.jpg</a>&#8221; width=&#8221;320&#8221; title=&#8221;&lsquo;Big Data&rsquo;&#8221; ></p>

<!-- more -->


<h2>The infamous &ldquo;big data project&rdquo;</h2>

<p>A typical discussion would look like:</p>

<p>Me: <em>“Can you tell me more about this project, what do you do with your data?”</em></p>

<p>Mr. Big Bytes: <em>“Sure, we have a 40 nodes Hadoop cluster&hellip;&ldquo;</em></p>

<p>Me: <em>“This is cool but which type of data do you store, and what is the use case, business value?&ldquo;</em></p>

<p>Mr. Big Bytes: <em>“We store, all the logs of our applications, we have hundreds of gigabits…&#8221;</em></p>

<p>After a long blank: <em>“We have not yet started to analyze these data. For now it is jut  &lsquo;us, the IT team,&rsquo; we store the data, like that soon we will be able to do interesting things with them&#8221;</em></p>

<p>You can meet the same person few months later; the cluster is still sitting here, with no activity on it. I even met some consultants telling me they received calls from their customer asking the following:</p>

<p><em>“Hmmm, we have an Hadoop cluster installed, can you help us to find what to do with it?&ldquo;</em></p>

<p><em>Wrong! That is wrong!!!!!</em> This means that the IT Team has spent lot of time for nothing, at least for the business; and I am not even sure the team has learned something technically.</p>

<h3>Start with the &ldquo;Why&rdquo; not with the &ldquo;How&rdquo;!</h3>

<p>The solution to this, could be obvious, start your “big data project” answering the “why/what” questions first! The “how”, the implementation, will come later.</p>

<p>I am sure that most of the enterprises will benefit of a so called “big data project”, but it is really important to understand the problems first. <em>And these problems are not technical…</em> at least at the beginning. So you must spend time with the business persons to understand what could help them. Let&rsquo;s take some examples.</p>

<p>You are working in a bank or insurance, business people will be more than happy to predict when/why customer will be leaving the company by doing some <em>churn analysis</em>; or it will be nice to be able to see when it makes lot of sense <em>to sell new contracts</em>, service to existing customers.</p>

<p>You are working in retail/commerce, your business will be happy to see if they can <u>adjust the price</u> to the market, or <u>provide precise recommendations</u> to a user from an analysis of other customer behavior.</p>

<p>We can find many other examples. But as you can see we are not talking about technology, just business and possible benefits. In fact nothing new, compare with the applications you are building, you need first to have some requirements/ideas to build a product. Here we just need to have some &ldquo;data input&rdquo; to see <u>how we can enrich the data</u> with some business value.</p>

<p>Once you have started to ask all these questions you will start to see some input, and possible processing around them:</p>

<ul>
<li>You are an insurance, you customers has no contact with your representative, or the customer satisfaction is medium/bad; you start to see some customer name in quotes coming from price comparison website…. hmm you can guess that they are looking for a new insurance.</li>
<li>Still in the insurance, when your customer are close to the requirement age, or has teenagers learning how to drive, moving to college, you know that you have some opportunity to sell new contract, or adapt existing ones to the new needs</li>
<li>In retail, you may want to look to all customers and what they have ordered, and based on this be able to recommend some products to a customer that &ldquo;looks&rdquo; the same.</li>
<li>Another very common use case these days, you want to do some sentiment analysis of social networks to see how your brand is perceived by your community</li>
</ul>


<p>As you can see now, we can start to think about the data we have to use and the type of processing we have to do on them.</p>

<h3>Let&rsquo;s now talk about the &ldquo;How&rdquo;</h3>

<p>Now that you have a better idea about what you want to do, it does not mean that you should dive into a large cluster installation.</p>

<p>Before that, you should continue to analyze the data:</p>

<ul>
<li>What is the structure of the data that I have to analyze?</li>
<li>How big is my dataset?</li>
<li>How much data I have to ingest on a period of time (minute, hour, day, &hellip;)</li>
</ul>


<p>All these questions will help you to understand better your application. This is where it is often interesting too, and we realize that for most of us the &ldquo;big data&rdquo; is not that big!</p>

<p>I was working the other day with a Telco company in Belgium, and I was talking about possible new project. I simply said:</p>

<ul>
<li>Belgium is what, 11+ millions of people</li>
<li>If you store a 50kb object for each person this represent:</li>
<li>Your full dataset will be 524Gb, yes not even a Terabyte!</li>
</ul>


<p>Do you need a large Hadoop cluster to store and process this? You can use it, but you do not need to! You can find something smaller, and easier to start with.</p>

<p>Any database will do the job, starting with MongoDB. I think it is really interesting to start this project with a MongoDB cluster, not only because it will allow you to scale out as much as you need, but also because you will leverage the flexibility of the document model. This will allow you to store any type of data, and easily adapt the structure to the new data, or requirements.</p>

<p>Storing the data is only one part of the equation. The other part is how you achieve the data processing. Lately I am playing a lot with <a href="https://spark.apache.org/">Apache Spark</a>. Spark provides a very powerful engine for large scale data processing, and it is a lot simpler than Map Reduce jobs. In addition to this, you can run Spark without Hadoop! This means you can connect you Spark to your MongoDB, with the <a href="http://docs.mongodb.org/ecosystem/tools/hadoop/">MongoDB Hadoop Connector</a> and other data sources and directly execute job on your main database.</p>

<p>What I like also about this approach, you can when you dataset starts to grow, and it become harder to process all the data on your operational database, you can easily add Hadoop; and keep most of your data processing layer intact, and only change the data sources information. In this case you will connect MongoDB and Hadoop to get/push the data into HDFS, once again using the MongoDB Hadoop Connector.</p>

<h3>Conclusion</h3>

<p>Too many times, projects are driven by technology instead of focusing on the business value. This is particularly true around big data projects. So be sure you start by understanding the business problem, and find the data that could help to solve it.</p>

<p>Once you have the business problem and the data, select the good technology, that could be very simple, simple files and python scripts, or more often a database like MongoDB with a data processing layer like Spark. And start to move to Hadoop when it is really mandatory&hellip; a very, very, very, large dataset.</p>
]]></content>
  </entry>
  
</feed>
