<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Streaming | Tug's Blog]]></title>
  <link href="http://tgrall.github.io/blog/categories/streaming/atom.xml" rel="self"/>
  <link href="http://tgrall.github.io/"/>
  <updated>2019-09-27T15:06:57+02:00</updated>
  <id>http://tgrall.github.io/</id>
  <author>
    <name><![CDATA[Tug Grall]]></name>
    <email><![CDATA[tugdual@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Getting Started With Redis Streams &amp; Java]]></title>
    <link href="http://tgrall.github.io/blog/2019/09/02/getting-with-redis-streams-and-java/"/>
    <updated>2019-09-02T09:24:24+02:00</updated>
    <id>http://tgrall.github.io/blog/2019/09/02/getting-with-redis-streams-and-java</id>
    <content type="html"><![CDATA[<p>As you may have seen, I have joined <a href="https://www.redislabs.com">Redis Labs</a> a month ago; one of the first task as a new hire is to learn more about Redis. So I learned, and I am still learning.</p>

<p>This is when I discovered <a href="https://redis.io/topics/streams-intro">Redis Streams</a>. I am a big fan of streaming-based applications so it is natural that I start with a small blog post explaining how to use Redis Streams and Java.</p>

<p><strong><em>What is Redis Streams?</em></strong></p>

<p>Redis Streams is a Redis Data Type, that represents a log so you can add new information/message in an append-only mode <em>(this is not 100% accurate since you can remove messages from the log)</em>. Using Redis Streams you can build &ldquo;Kafka Like&rdquo; applications, what I mean by that you can:</p>

<ul>
<li>create applications that publish and consume messages (nothing extraordinary here, you could already do that with Redi Pub/Sub)</li>
<li>consume messages that are published even when your client application (consumer) is not running. This is a big difference with Redis Pub/Sub</li>
<li>consume messages starting a specific offset, for example, read the whole history, or only new messages</li>
</ul>


<p>In addition to this, Redis Streams has the concept of <strong>Consumer Groups</strong>. Redis Streams Consumer Groups, like Apache Kafka ones, allows the client applications to consume messages in a distributed fashion (multiple clients), providing an easy way to scale and create highly available systems.</p>

<p><img class="center" src="/images/posts/getting-with-redis-streams-and-java/redis-streams-101-img-1.png"></p>

<p>Enroll in the <a href="https://university.redislabs.com/courses/course-v1:redislabs+RU202+2019_03/about">Redis University: Redis Streams</a> to learn more and get certified.</p>

<p><strong><em>Sample Application</em></strong></p>

<p>The <a href="https://github.com/tgrall/redis-streams-101-java">redis-streams-101-java GitHub Repository</a> contains sample code that shows how to</p>

<ul>
<li>post messages to a streams</li>
<li>consume messages using a consumer group</li>
</ul>


<!--more -->


<h4>Prerequisites</h4>

<ul>
<li>Redis 5.x, you have here multiple options:

<ul>
<li><a href="https://redis.io">Download</a> and install Redis Community</li>
<li>Install and Run a Docker image: <a href="https://hub.docker.com/_/redis">Community</a> or <a href="https://hub.docker.com/r/redislabs/redis">Redis Enterprise</a></li>
<li>Create a online instance on <a href="https://redislabs.com/redis-enterprise/essentials/">Redis Labs Cloud</a> (30mb for free)</li>
</ul>
</li>
<li>Java 8 or later</li>
<li>Apache Maven 3.5.x</li>
<li>Git</li>
</ul>


<h3>Java &amp; Redis Streams</h3>

<p>Redis has many Java clients developed by the community, as you can see on the <a href="https://redis.io/clients#java">Redis.io site</a>.</p>

<p>It looks, based on my short experience with Redis so far, that the most complete one around Redis Streams support is <a href="https://lettuce.io">Lettuce</a>, this is the one I will be using in the following code.</p>

<h4>1- Adding Lettuce to Your Maven Project</h4>

<p>Add the following dependency to your project file:</p>

<pre><code class="xml">        &lt;dependency&gt;
            &lt;groupId&gt;io.lettuce&lt;/groupId&gt;
            &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt;
            &lt;version&gt;5.1.8.RELEASE&lt;/version&gt;
        &lt;/dependency&gt;
</code></pre>

<h4>2- Connecting to Redis</h4>

<p>Import the following classes</p>

<pre><code class="java">import io.lettuce.core.*;
import io.lettuce.core.api.StatefulRedisConnection;
import io.lettuce.core.api.sync.RedisCommands;
</code></pre>

<p>Then connect with:</p>

<pre><code class="java">RedisClient redisClient = RedisClient.create("redis://password@host:port"); // change to reflect your environment
StatefulRedisConnection&lt;String, String&gt; connection = redisClient.connect();
RedisCommands&lt;String, String&gt; syncCommands = connection.sync();
</code></pre>

<p>When your application is done with the connection you should disconnect with the following code:</p>

<pre><code class="java">connection.close();
redisClient.shutdown();
</code></pre>

<h4>3- Sending Message to Streams</h4>

<p>Once you have a connection you can send a message. In this example, I will let Redis generate the message ID, which is time-based, and the body will be built using a Map representing IoT data, for example, a weather data capturing Wind speed and direction in real-time.</p>

<pre><code class="java">    public static void main(String[] args) {

        RedisClient redisClient = RedisClient.create("redis://localhost:6379"); // change to reflect your environment
        StatefulRedisConnection&lt;String, String&gt; connection = redisClient.connect();
        RedisCommands&lt;String, String&gt; syncCommands = connection.sync();

        Map&lt;String, String&gt; messageBody = new HashMap&lt;&gt;();
        messageBody.put( "speed", "15" );
        messageBody.put( "direction", "270" );
        messageBody.put( "sensor_ts", String.valueOf(System.currentTimeMillis()) );

        String messageId = syncCommands.xadd(
                "weather_sensor:wind",
                messageBody);

        System.out.println( String.format("Message %s : %s posted", messageId, messageBody) );

        connection.close();
        redisClient.shutdown();

    }
</code></pre>

<p>Let me explain the code:</p>

<ul>
<li>Lines 3-5 are used to connect to Redis</li>
<li>Lines 7-10 are used to create the message body, using a Map, since Redis Streams messages are string key/values.</li>
<li>Lines 12-14 call the <code>syncCommands.xadd()</code> method using the streams key &ldquo;weather_sensor:wind&rdquo; and the message body itself

<ul>
<li>this method returns the message ID.</li>
</ul>
</li>
<li>line 16 just print the message ID and content</li>
<li>the lines 18-19 close the connection and client.</li>
</ul>


<p>The complete producer code is available <a href="https://github.com/tgrall/redis-streams-101-java/blob/master/src/main/java/com/kanibl/redis/streams/simple/RedisStreams101Producer.java">here</a>.</p>

<h4>4- Consuming Messages</h4>

<p>Redis Streams offers various way to consume/read messages using the commands: <a href="https://redis.io/commands/xrange">XRANGE</a>, <a href="https://redis.io/commands/xrevrange">XREVRANGE</a>, <a href="https://redis.io/commands/xread">XREAD</a>, <a href="https://redis.io/commands/xreadgroup">XREADGROUP</a>.</p>

<p>I want to keep the article short and close to the way you would build an application with Apache Kafka, this is why I will use the <a href="https://redis.io/commands/xreadgroup">XREADGROUP</a> command from Lettuce.</p>

<p>The Consumer Groups allow developers to create a group of clients that will cooperate to consume messages from the streams (for scale and high availability); it is also a way to associate the client to specific applications roles; for example:</p>

<ul>
<li>a consumer group called &ldquo;data warehouse&rdquo; will consume messages and send them to a data warehouse</li>
<li>another consumer group called &ldquo;aggregator&rdquo; will consume the messages and aggregate the data and send them to another sink (another stream or storage)</li>
</ul>


<p>Each of this group will act independently, and each of this group could have multiple &ldquo;consumers&rdquo; (client).</p>

<p>Let&rsquo;s see how you use this in Java.</p>

<pre><code class="java">...

        try {
            // WARNING: Streams must exist before creating the group
            //          This will not be necessary in Lettuce 5.2, see https://github.com/lettuce-io/lettuce-core/issues/898
            syncCommands.xgroupCreate( XReadArgs.StreamOffset.from("weather_sensor:wind", "0-0"), "application_1"  );
        }
        catch (RedisBusyException redisBusyException) {
            System.out.println( String.format("\t Group '%s already' exists","application_1"));
        }


        System.out.println("Waiting for new messages");

        while(true) {

            List&lt;StreamMessage&lt;String, String&gt;&gt; messages = syncCommands.xreadgroup(
                    Consumer.from("application_1", "consumer_1"),
                    XReadArgs.StreamOffset.lastConsumed("weather_sensor:wind")
            );

            if (!messages.isEmpty()) {
                System.out.println( messages );
            }


        }

...
</code></pre>

<p>This code is a subset of the <code>main()</code> method I have removed the connection management part, to add readability. Let&rsquo;s take a look to the code:</p>

<ul>
<li>line 3 to 10, using the method <code>xgroupCreate()</code>, that matches the <a href="https://redis.io/commands/xgroup">XGROUP CREATE</a> command,

<ul>
<li>is used to create a new group called <code>application_1</code>,</li>
<li>consume messages from the stream <code>weather_sensor:wind</code></li>
<li>starting at the first message in the stream, this is indicated using the message ID <code>0-0</code>. <em>Note that it is also possible to indicate to the group to start to read at a specific message ID, or only the new messages that arrive after the creating of the consumer group using <code>$</code> special ID (or the helper method <code>XReadArgs.StreamOffset.latest()</code></em>.</li>
</ul>
</li>
<li>line 15 to 27, in this example we use an infinite loop (<code>while(true)</code>) to wait for any new messages published to the streams</li>
<li>line 17 to 20, the method <code>xreadgroup()</code> returns the messages based on the group configuration

<ul>
<li>line 18 define the consumer named <code>consumer_1</code> that is associated with the group <code>application_1</code>: you can create new group do distribute the read to multiple clients</li>
<li>line 19 indicates where to start, in this case, <code>StreamOffset.lastConsumed("weather_sensor:wind")</code> the consumer will consume messages that have not been read already. With the current configuration of the group (offset <code>0-0</code>), when the consumer will start for the first time, it will read all the existing messages.</li>
<li>the <a href="https://redis.io/commands/xreadgroup">XREADGROUP</a> command by default sends an acknowledgment for each consumed message.</li>
</ul>
</li>
</ul>


<p>The complete consumer code is available <a href="https://github.com/tgrall/redis-streams-101-java/blob/master/src/main/java/com/kanibl/redis/streams/simple/RedisStreams101Consumer.java">here</a>.</p>

<h3>Build &amp; Run the Simple Java Application</h3>

<p>Now that you have a better understanding of the code, let&rsquo;s run the producer and consumer. You can run this from your IDE, or using Maven.</p>

<p>Let&rsquo;s do it using Maven CLI, for this open 2 terminals:</p>

<ul>
<li>one to produce messages</li>
<li>one to consume them</li>
</ul>


<p><em>1- Clone and Build the project</em></p>

<pre><code class="bash">&gt; git clone https://github.com/tgrall/redis-streams-101-java.git

&gt; cd redis-streams-101-java

&gt; mvn clean verify
</code></pre>

<p><em>2- Post a new message</em></p>

<pre><code class="bash">
&gt; mvn exec:java -Dexec.mainClass="com.kanibl.redis.streams.simple.RedisStreams101Producer"
</code></pre>

<p><em>3- Consume messages</em></p>

<p>Open a new terminal and run the following command:</p>

<pre><code class="bash">
&gt; mvn exec:java -Dexec.mainClass="com.kanibl.redis.streams.simple.RedisStreams101Consumer"
</code></pre>

<p>The consumer will start and consume the message you just posted, and wait for any new messages.</p>

<p><em>4- In the first terminal post 100 new messages</em></p>

<pre><code class="bash">
&gt; mvn exec:java -Dexec.mainClass="com.kanibl.redis.streams.simple.RedisStreams101Producer" -Dexec.args="100"
</code></pre>

<p>The consumer will receive and print all the messages.</p>

<p><em>5- Kill the consumer and post more messages</em></p>

<p>Let&rsquo;s now do another test, stop the consumer using a simple <code>ctrl+C</code>.</p>

<p>Then post 5 new messages.</p>

<pre><code class="bash">
&gt; mvn exec:java -Dexec.mainClass="com.kanibl.redis.streams.simple.RedisStreams101Producer" -Dexec.args="5"
</code></pre>

<p>The messages are not yet consumed by any application, but still store in Redis Streams.</p>

<p>So when you start the consumer, it will consumes these new messages.</p>

<pre><code class="bash">
&gt; mvn exec:java -Dexec.mainClass="com.kanibl.redis.streams.simple.RedisStreams101Consumer"
</code></pre>

<p>This is a one of the differences between <a href="https://redis.io/topics/streams-intro">Redis Streams</a> and <a href="https://redis.io/topics/pubsub">Redis PubSub</a>. The producer application has publish many messages while the consumer application was not running. Since the consumer is ran with <code>StreamOffset.lastConsumed()</code>, when the consumer is starting, it looks to the last consumed ID, and start to read the streams from there. This method generate a XGROUPREAD command with the group</p>

<h3>Conclusion</h3>

<p>In this small project, you have learned, how to use Lettuce, a Java client for Redis to:</p>

<ul>
<li>publish messages to a stream</li>
<li>create a consumer group</li>
<li>consume messages using the consumer group.</li>
</ul>


<p>This is a very basic example, and in a next post I will show you how to work with multiple consumers, and to configure the Consumer Group and Consumers to control which messages you want to read</p>

<p>More to come!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With Kafka REST Proxy for MapR Streams]]></title>
    <link href="http://tgrall.github.io/blog/2017/01/20/getting-started-with-kafka-rest-proxy-for-mapr-streams/"/>
    <updated>2017-01-20T10:31:22+01:00</updated>
    <id>http://tgrall.github.io/blog/2017/01/20/getting-started-with-kafka-rest-proxy-for-mapr-streams</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p>MapR Ecosystem Package 2.0 (MEP) is coming with some new features related to MapR Streams:</p>

<ul>
<li><a href="http://maprdocs.mapr.com/home/Kafka/kafkaREST.html">Kafka REST Proxy for MapR Streams</a> provides a RESTful interface to MapR Streams and Kafka clusters to consume and product messages and to perform administrative operations.</li>
<li><a href="http://maprdocs.mapr.com/home/Kafka/kafkaConnect.html">Kafka Connect for MapR Streams</a> is a utility for streaming data between MapR Streams and Apache Kafka and other storage systems.</li>
</ul>


<p>MapR Ecosystem Packs (MEPs) are a way to deliver ecosystem upgrades decoupled from core upgrades - allowing you to upgrade your tooling independently of your Converged Data Platform. You can lean more about MEP 2.0 in <a href="https://www.mapr.com/blog/announcing-mapr-ecosystem-pack-mep-20">this article</a>.</p>

<p>In this blog we describe how to use the REST Proxy to publish and consume messages to/from MapR Streams. The REST Proxy is a great addition to the MapR Converged Data Platform allowing any programming language to use MapR Streams.</p>

<p>The Kafka REST Proxy provided with the MapR Streams tools, can be used with MapR Streams (default), but also used in a hybrid mode with Apache Kafka. In this article we will focus on MapR Streams.</p>

<!-- more -->


<h2>Prerequisites</h2>

<ul>
<li>MapR Converged Data Platform 5.2 with MEP 2.0

<ul>
<li>with MapR Streams Tools</li>
</ul>
</li>
<li>curl, wget or any HTTP/REST Client tool</li>
</ul>


<h2>Create the MapR Streams and Topic</h2>

<p>A stream is a collection of topics that you can manage as a group by:</p>

<ol>
<li>Setting security policies that apply to all topics in that stream</li>
<li>Setting a default number of partitions for each new topic that is created in the stream</li>
<li>Set a time-to-live for messages in every topic in the stream</li>
</ol>


<p>You can find more information about MapR Streams concepts in the <a href="http://maprdocs.mapr.com/home/MapR_Streams/mapr_streams.html">documentation</a>.</p>

<p>On your Mapr Cluster or Sandbox, run the following commands:</p>

<pre><code>$ maprcli stream create -path /apps/iot-stream -produceperm p -consumeperm p -topicperm p

$ maprcli stream topic create -path /apps/iot-stream -topic sensor-json -partitions 3

$ maprcli stream topic create -path /apps/iot-stream -topic sensor-binary -partitions 3
</code></pre>

<h2>Start Kafka Console Producers and Consumers</h2>

<p>Open two terminal windows and run the consumer Kafka utilities using the following commands:</p>

<h4>Consumer</h4>

<ul>
<li>Topic sensor-json</li>
</ul>


<pre><code>$ /opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-consumer.sh --new-consumer --bootstrap-server this.will.be.ignored:9092 --topic /apps/iot-stream:sensor-json
</code></pre>

<ul>
<li>Topic sensor-binary</li>
</ul>


<pre><code>$ /opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-consumer.sh --new-consumer --bootstrap-server this.will.be.ignored:9092 --topic /apps/iot-stream:sensor-binary
</code></pre>

<p>This two terminal windows will allow you to see the messages posted on the different topics</p>

<h2>Using Kafka REST Proxy</h2>

<h3>Inspect Topic Metadata</h3>

<p>The endpoint <code>/topics/[topic_name]</code> allows you to get some informations about the topic. In MapR Streams, topics are part of a <em>stream</em> identified by a path;
to use the topic using the REST API you have to use the full path, and encode it in the URL; for example:</p>

<ul>
<li><code>/apps/iot-stream:sensor-json</code> will be encoded with <code>%2Fapps%2Fiot-stream%3Asensor-json</code></li>
</ul>


<p>Run the following command, to get information about the <code>sensor-json</code> topic</p>

<pre><code>$ curl -X GET  http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-json
</code></pre>

<p>Note: For simplicity reason I am running the command from the node where the Kafka REST proxy is running, so it is possible to use <code>localhost</code>.</p>

<p>You can print JSON in a pretty way, by adding a Python command such as :</p>

<pre><code>$ curl -X GET  http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-json | python -m json.tool
</code></pre>

<p><strong>Default Stream</strong></p>

<p>As mentioned above, the Stream path is part of the topic name you have to use in the command;
however it is possible to configure the MapR Kafka REST Proxy to use a default stream.
For this you should add the following property in the <code>/opt/mapr/kafka-rest/kafka-rest-2.0.1/config/kafka-rest.properties</code> file:</p>

<ul>
<li><code>streams.default.stream=/apps/iot-stream</code></li>
</ul>


<p> When you change the Kafka REST proxy configuration, you must restart the service using maprcli or MCS.</p>

<p> The main reason to use the <code>streams.default.stream</code> properties is to simplify the URLs used by the application for example
 * with <code>streams.default.stream</code> you can use <code>curl -X GET  http://localhost:8082/topics/</code>
 * without this configuration, or if you want to use a specific stream you must specify it in the URL <code>http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-json</code></p>

<p> In this article, all the URLs contains the encoded stream name, like that you can start using the Kafka REST proxy without changind the configuration and also use it with different streams.</p>

<h3>Publishing Messages</h3>

<p>The Kafka REST Proxy for MapR Streams allows application to publish messages to MapR Streams. Messages could be send as JSON or Binary content (base64 encoding).</p>

<h4>To send a JSON Message:</h4>

<ul>
<li>the query should be a HTTP <code>POST</code></li>
<li>the Content-Type should be : <code>application/vnd.kafka.json.v1+json</code></li>
<li>the Body:</li>
</ul>


<pre><code class="JSON">{
  "records":
  [
    {
      "value":
      {
        "temp" : 10 ,
        "speed" : 40 ,
        "direction" : "NW"
        }  
      }
  ]
}
</code></pre>

<p>The complete request is:</p>

<pre><code>curl -X POST -H "Content-Type: application/vnd.kafka.json.v1+json" \
  --data '{"records":[{"value": {"temp" : 10 , "speed" : 40 , "direction" : "NW"}  }]}' \
  http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-json
</code></pre>

<p>You should see the message printed in the terminal window where the <code>/apps/iot-stream:sensor-json</code> consumer is running.</p>

<h4>To send a binary Message:</h4>

<ul>
<li>the query should be a HTTP <code>POST</code></li>
<li>the Content-Type should be : <code>application/vnd.kafka.binary.v1+json</code></li>
<li>the Body:</li>
</ul>


<pre><code class="JSON">{
  "records":
  [
    {
      "value":"SGVsbG8gV29ybGQ="
    }
  ]
}
</code></pre>

<p>Note that <code>SGVsbG8gV29ybGQ=</code> is the string &ldquo;Hello World&rdquo; encoded in Base64.</p>

<p>The complete request is:</p>

<pre><code>curl -X POST -H "Content-Type: application/vnd.kafka.binary.v1+json" \
  --data '{"records":[{"value":"SGVsbG8gV29ybGQ="}]}' \
  http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-binary
</code></pre>

<p>You should see the message printed in the terminal window where the <code>/apps/iot-stream:sensor-binary</code> consumer is running.</p>

<h4>Sending multiple messages</h4>

<p>The <code>records</code> field of the HTTP Body allows you to send multiple messages for example you can send:</p>

<pre><code>curl -X POST -H "Content-Type: application/vnd.kafka.json.v1+json" \
  --data '{"records":[{"value": {"temp" : 12 , "speed" : 42 , "direction" : "NW"}  }, {"value": {"temp" : 10 , "speed" : 37 , "direction" : "N"}  } ]}' \
  http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-json
</code></pre>

<p>This command will send 2 messages, and increment the offset by 2. You can do the same
with binary content, just add new element in the JSON array; for example:</p>

<pre><code>curl -X POST -H "Content-Type: application/vnd.kafka.binary.v1+json" \
  --data '{"records":[{"value":"SGVsbG8gV29ybGQ="}, {"value":"Qm9uam91cg=="}]}' \
  http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-binary
</code></pre>

<p>As you probably know, it is possible to set a key to a message to be sure that all the messages
with the same key will arrive in the same partition. For this, add the <code>key</code> attribute to the message as follow:</p>

<pre><code class="JSON">{
  "records":
  [
    {
      "key": "K001",
      "value":
      {
        "temp" : 10 ,
        "speed" : 40 ,
        "direction" : "NW"
        }  
      }
  ]
}
</code></pre>

<p>Now that you know how to post messages to MapR Stream topics usinf the REST Proxy, let&rsquo;s see how to consume the messages.</p>

<h3>Consuming Messages</h3>

<p>The REST proxy can also be used to consume messages from topics; for this you need to:</p>

<ol>
<li>Create a consumer instance.</li>
<li>Use this URL returned by the first call to read message.</li>
<li>Delete the consumer instanced if needed.</li>
</ol>


<h4>Creating the consumer instance</h4>

<p>The following request creates the consumer instance:</p>

<pre><code>curl -X POST -H "Content-Type: application/vnd.kafka.v1+json" \
      --data '{"name": "iot_json_consumer", "format": "json", "auto.offset.reset": "earliest"}' \
      http://localhost:8082/consumers/%2Fapps%2Fiot-stream%3Asensor-json
</code></pre>

<p>The response from the server looks like:</p>

<pre><code class="json">{
  "instance_id":"iot_json_consumer",
  "base_uri":"http://localhost:8082/consumers/%2Fapps%2Fiot-stream%3Asensor-json/instances/iot_json_consumer"
}
</code></pre>

<p>Note that we have used the <code>/consumers/[topic_name]</code> to create the consumer.</p>

<p>The <code>base_uri</code> will be used by the subsequent requests to get the messages from the topic. Like any MapR Streams/Kafka consumer the <code>auto.offset.reset</code> defines its behavior. In this example the value is set to <code>earliest</code>, this means that the consumer will read the messages from the beginning. You can find more information about the consumer configuration in the <a href="http://maprdocs.mapr.com/home/MapR_Streams/configuration_parameters_for_consumers.html">MapR Streams documentation</a>.</p>

<h4>Consuming the messages</h4>

<p>To consume the messages, just add the Mapr Streams topic to the URL of the consumer isntance.</p>

<p>The following request consumes the messages from the topic:</p>

<pre><code>curl -X GET -H "Accept: application/vnd.kafka.json.v1+json" \
http://localhost:8082/consumers/%2Fapps%2Fiot-stream%3Asensor-json/instances/iot_json_consumer/topics/%2Fapps%2Fiot-stream%3Asensor-json
</code></pre>

<p>This call returns the messages in a JSON document:</p>

<pre><code class="json">[
  {"key":null,"value":{"temp":10,"speed":40,"direction":"NW"},"topic":"/apps/iot-stream:sensor-json","partition":1,"offset":1},
  {"key":null,"value":{"temp":12,"speed":42,"direction":"NW"},"topic":"/apps/iot-stream:sensor-json","partition":1,"offset":2},
  {"key":null,"value":{"temp":10,"speed":37,"direction":"N"},"topic":"/apps/iot-stream:sensor-json","partition":1,"offset":3}
]
</code></pre>

<p>Each call to the API returns the new messages published, based on the offset of the last call.</p>

<p>Note that the Consumer will be destroyed:</p>

<ul>
<li>after some idle time set by the <code>consumer.instance.timeout.ms</code> (default value set to 300000ms / 5 minutes)</li>
<li>where it is destroyed using a REST API call (see below).</li>
</ul>


<h3>Consuming binary format messages</h3>

<p>The approach is the same if you need to consume binary messages, you need to change the format and accept header.</p>

<p>Call this URL to create a consumer instance for the binary topic:</p>

<pre><code>curl -X POST -H "Content-Type: application/vnd.kafka.v1+json" \
      --data '{"name": "iot_binary_consumer", "format": "binary", "auto.offset.reset": "earliest"}' \
      http://localhost:8082/consumers/%2Fapps%2Fiot-stream%3Asensor-binary
</code></pre>

<p>Then consume messages, the accept header is set to <code>application/vnd.kafka.binary.v1+json</code>:</p>

<pre><code>curl -X GET -H "Accept: application/vnd.kafka.binary.v1+json" \
http://localhost:8082/consumers/%2Fapps%2Fiot-stream%3Asensor-binary/instances/iot_binary_consumer/topics/%2Fapps%2Fiot-stream%3Asensor-binary
</code></pre>

<p>This call returns the messages in a JSON document, and the value is encoded in Base64</p>

<pre><code class="json">[
  {"key":null,"value":"SGVsbG8gV29ybGQ=","topic":"/apps/iot-stream:sensor-binary","partition":1,"offset":1},
  {"key":null,"value":"Qm9uam91cg==","topic":"/apps/iot-stream:sensor-binary","partition":1,"offset":2}
]
</code></pre>

<h3>Delete consumer instances</h3>

<p>As mentioned before the consumer will be destroyed automatically based on the <code>consumer.instance.timeout.ms</code> configuration of the REST Proxy;
it is also possible to destroyed the instance using the consumer instance URI and an HTTP DELETE call, as follow:</p>

<pre><code>curl -X DELETE http://localhost:8082/consumers/%2Fapps%2Fiot-stream%3Asensor-binary/instances/iot_binary_consumer
</code></pre>

<h2>Conclusion</h2>

<p>In this article you have learned how to use the Kafka REST Proxy for MapR Streams that allow any application to
use messages published in the MapR Converged Data Platform.</p>

<p>You can find more information about the Kafka REST Proxy in the <a href="http://maprdocs.mapr.com/home/Kafka/REST-proxy.html">MapR documentation</a> and the following resources:</p>

<ul>
<li><a href="https://www.mapr.com/blog/getting-started-sample-programs-mapr-streams">Getting Started with MapR Streams</a></li>
<li><a href="https://www.mapr.com/streaming-architecture-using-apache-kafka-mapr-streams">&ldquo;Streaming Architecture: New Designs Using Apache Kafka and MapR Streams&rdquo; ebook by Ted Dunning and Ellen Friedman</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With MQTT and Java]]></title>
    <link href="http://tgrall.github.io/blog/2017/01/02/getting-started-with-mqtt/"/>
    <updated>2017-01-02T16:03:09+01:00</updated>
    <id>http://tgrall.github.io/blog/2017/01/02/getting-started-with-mqtt</id>
    <content type="html"><![CDATA[<p>MQTT (MQ Telemetry Transport) is a lightweight publish/subscribe messaging protocol.
MQTT is used a lot in the Internet of Things applications, since it has been designed to
run on remote locations with system with small footprint.</p>

<p>The MQTT 3.1 is an OASIS standard, and you can find all the information at <a href="http://mqtt.org/">http://mqtt.org/</a></p>

<p>This article will guide you into the various steps to run your first MQTT application:</p>

<ol>
<li>Install and Start a MQTT Broker</li>
<li>Write an application that publishes messages</li>
<li>Write an application that consumes messages</li>
</ol>


<p>The source code of the sample application is available on <a href="https://github.com/tgrall/mqtt-sample-java">GitHub</a>.</p>

<!-- more -->


<h4>Prerequisites</h4>

<ul>
<li>Apache Maven 3.x</li>
<li>Git</li>
</ul>


<h3>Install and Start a MQTT Broker</h3>

<p>You can find many MQTT Brokers, for this example I will use one of the most common broker <a href="https://mosquitto.org">Mosquitto</a>.</p>

<p>You can download and install from the <a href="https://mosquitto.org/download/">binary package</a>. I have used <a href="http://brew.sh/">Homebrew</a> to install it on my Mac:</p>

<pre><code>$ brew install mosquitto
</code></pre>

<p>Start the MQTT Broker with the default configuration</p>

<pre><code>$ /usr/local/sbin/mosquitto
</code></pre>

<h3>Publish and Consume messages</h3>

<p>Open two terminal windows and run the following commands :</p>

<p>Consume</p>

<pre><code>$ mosquitto_sub -h 127.0.0.1 -t iot_data
</code></pre>

<p>Publish</p>

<pre><code>$ mosquitto_pub -h 127.0.0.1 -t iot_data -m "Hello world"
</code></pre>

<p>You should see the message <code>Hello world</code> in the consumer/subscriber window.</p>

<h3>Write your first MQTT Application</h3>

<p>For this example I will write a small Java application, since it is the language
that I am using in my global project.</p>

<h4>Maven Dependencies</h4>

<p>Add the <a href="https://eclipse.org/paho/">Eclipse Paho</a> dependency to your Maven project</p>

<pre><code class="xml">&lt;dependency&gt;
  &lt;groupId&gt;org.eclipse.paho&lt;/groupId&gt;
  &lt;artifactId&gt;org.eclipse.paho.client.mqttv3&lt;/artifactId&gt;
  &lt;version&gt;1.1.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<h4>Publishing a Message</h4>

<p>Publishing a message is quite easy, create a MqttClient and use it to post on a topic.</p>

<pre><code class="java">MqttClient client = new MqttClient("tcp://localhost:1883", MqttClient.generateClientId());
client.connect();
MqttMessage message = new MqttMessage();
message.setPayload("Hello world from Java".getBytes());
client.publish("iot_data", message);
client.disconnect();
</code></pre>

<p>You have many other options, configurations that you can use when posting a message
such as security, quality of service (QoS), and more; but in this post I want to simply
show how easy is to publish and consume MQTT messages.</p>

<h4>Consuming messages</h4>

<p>To consume messages you need to implement a <code>org.eclipse.paho.client.mqttv3.MqttCallback</code> that will receive the message and used this Callback class in the MqttClient of the Subscriber application.</p>

<p>The Callback class:</p>

<pre><code class="java">public class SimpleMqttCallBack implements MqttCallback {

  public void connectionLost(Throwable throwable) {
    System.out.println("Connection to MQTT broker lost!");
  }

  public void messageArrived(String s, MqttMessage mqttMessage) throws Exception {
    System.out.println("Message received:\n\t"+ new String(mqttMessage.getPayload()) );
  }

  public void deliveryComplete(IMqttDeliveryToken iMqttDeliveryToken) {
    // not used in this example
  }
}
</code></pre>

<p>This Callback class is used in the Subscriber application as follow:</p>

<pre><code class="java">MqttClient client=new MqttClient("tcp://localhost:1883", MqttClient.generateClientId());
client.setCallback( new SimpleMqttCallBack() );
client.connect();
</code></pre>

<p>Like for the publisher, I am using the broker and client without any option (QoS, security).</p>

<h2>Build and Run the Application</h2>

<p><strong>1- Get the Sample Code</strong></p>

<p>Clone the project from GitHub</p>

<pre><code>$ git clone https://github.com/tgrall/mqtt-sample-java.git
</code></pre>

<p><strong>2- Build the project with Apache Maven:</strong></p>

<p>This project is a simple Java application that runs a publisher and subscriber using the <a href="https://eclipse.org/paho/">Eclipse Paho library</a>.</p>

<pre><code>$ mvn clean package
</code></pre>

<p>For convenience, the example programs project is set up so that the maven package target produces a single executable,
<code>/mqtt-sample</code>, that includes all of the example programs and dependencies.</p>

<p><strong>3- Run the Subscriber</strong></p>

<p>The subscriber will receive and print all messages published on the <code>iot_data</code> topic.</p>

<pre><code>$ ./target/mqtt-sample subscriber
</code></pre>

<p><strong>4- Run the Publisher</strong></p>

<p>Run the publisher with the following command, the second parameter is the message to publish</p>

<pre><code>$ ./target/mqtt-sample publisher "My first MQTT message..."
</code></pre>

<h2>Conclusion</h2>

<p>In this article you have learned how to:</p>

<ul>
<li>Install and start a MQTT Broker, Mosquitto</li>
<li>Create a publisher and subscriber developed in Java</li>
</ul>


<p>This article is very simple by choice, to quickly run your first MQTT Application. I wrote this article as part of a global IoT project I am working on that will capture devices data, publish them into MapR Converged Data Platform using MQTT and MapR Streams; this is why I used Java for the application. You can use any MQTT client library to build the publishers and subscribers.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With Apache Flink and Mapr Streams]]></title>
    <link href="http://tgrall.github.io/blog/2016/10/17/getting-started-with-apache-flink-and-mapr-streams/"/>
    <updated>2016-10-17T10:12:10+02:00</updated>
    <id>http://tgrall.github.io/blog/2016/10/17/getting-started-with-apache-flink-and-mapr-streams</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p><a href="https://flink.apache.org/">Apache Flink</a> is an open source platform for distributed stream and batch data processing. Flink is a streaming data flow engine with several APIs to create data streams oriented application.</p>

<p>It is very common for Flink applications to use <a href="http://kafka.apache.org/">Apache Kafka</a> for data input and output.</p>

<p>This article will guide you into  the steps to use Apache Flink with <a href="https://www.mapr.com/products/mapr-streams">MapR Streams</a>. MapR Streams is a distributed messaging system for streaming event data at scale, and it’s integrated into the <a href="https://www.mapr.com/products/mapr-converged-data-platform">MapR Converged Data Platform</a>, based on the Apache Kafka API (0.9.0),
this article use the same code and approach than the <a href="http://tgrall.github.io/blog/2016/10/12/getting-started-with-apache-flink-and-kafka/">Flink and Kafka Getting Started</a>.</p>

<p><img src="http://tgrall.github.io/images/posts/flink-kafka/flink-mapr-streams.png" alt="MapR Streams and Flink" />.</p>

<!-- more -->


<h3>Prerequisites</h3>

<ul>
<li>MapR 5.2

<ul>
<li>You can use <a href="https://www.mapr.com/products/mapr-sandbox-hadoop">MapR Converged Data Platform Sandbox</a></li>
</ul>
</li>
<li>MapR Client installed on your development host

<ul>
<li><a href="http://maprdocs.mapr.com/home/AdvancedInstallation/SettingUptheClient-install-mapr-client.html">Installation and Configuration steps</a></li>
</ul>
</li>
<li>Git</li>
<li>Maven 3.x or later</li>
</ul>


<h2>Create your Flink Streaming Project</h2>

<p>The first step is to create an Java application, the easiest is to use the flink-quickstart-java archetype, that contains the core dependencies and packaging tasks. This article is similar with the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/quickstart/run_example_quickstart.html">Apache Flink Quick Start Example</a>, with a clear focus on data input and output with MapR Streams.</p>

<p>In this application we will create two jobs:</p>

<ul>
<li><code>WriteToKafka</code> : that generates random string and post them to a MapR Streams Topic using the Kafka Flink Connector and its Producer API.</li>
<li><code>ReadFromKafka</code> : that reads the same topic and print the messages in the standard output using the Kafka Flink Connector and its Consumer. API.</li>
</ul>


<p>The full project is available on GitHub:</p>

<ul>
<li><a href="https://github.com/mapr-demos/mapr-streams-flink-demo">MapR Streams Flink Demo</a></li>
</ul>


<p>Let’s create the project using Apache Maven:</p>

<pre><code>mvn archetype:generate \
      -DarchetypeGroupId=org.apache.flink\
      -DarchetypeArtifactId=flink-quickstart-java \
      -DarchetypeVersion=1.1.2 \
      -DgroupId=com.mapr.demos \
      -DartifactId=mapr-streams-flink-demo \
      -Dversion=1.0-SNAPSHOT \
      -DinteractiveMode=false 
</code></pre>

<p>Maven will create the following structure:</p>

<pre><code>$ tree mapr-streams-flink-demo/
mapr-streams-flink-demo/
├── pom.xml
└── src
    └── main
        ├── java
        │   └── com
        │       └── mapr
        │           └── demos
        │               ├── BatchJob.java
        │               ├── SocketTextStreamWordCount.java
        │               ├── StreamingJob.java
        │               └── WordCount.java
        └── resources
            └── log4j.properties
</code></pre>

<p>This project is configured to create a Jar file that contains your flink project code and also includes all dependencies needed to run it.</p>

<p>The project contains some other sample jobs, we do not need them for this article, you can either keep them to educational purposes or simply remove them from the project.</p>

<h2>Add Kafka &amp; MapR Streams Dependencies</h2>

<p>Open the <code>pom.xml</code> and add the following dependencies to your project:</p>

<p><strong>1- Add MapR Maven Repository</strong></p>

<p>In the <code>&lt;repositories&gt;</code> element add :</p>

<pre><code>   &lt;repository&gt;
     &lt;id&gt;mapr-releases&lt;/id&gt;
     &lt;url&gt;http://repository.mapr.com/maven/&lt;/url&gt;
     &lt;snapshots&gt;&lt;enabled&gt;false&lt;/enabled&gt;&lt;/snapshots&gt;
     &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt;
   &lt;/repository&gt;
</code></pre>

<p><strong>2- Add MapR Streams libraries</strong></p>

<p>In the <code>&lt;dependencies&gt;</code> element:</p>

<pre><code> &lt;dependency&gt;
   &lt;groupId&gt;com.mapr.streams&lt;/groupId&gt;
   &lt;artifactId&gt;mapr-streams&lt;/artifactId&gt;
   &lt;version&gt;5.2.0-mapr&lt;/version&gt;
 &lt;/dependency&gt;
 &lt;dependency&gt;
   &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
   &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
   &lt;version&gt;0.9.0.0-mapr-1602&lt;/version&gt;
 &lt;/dependency&gt;
</code></pre>

<p><strong>3- Add Flink Kafka Connector libraries</strong></p>

<p>As a first step, we have to add the Flink Kafka connector as a dependency so that we can use the Kafka sink. Add this to the pom.xml file in the dependencies section:</p>

<p>You must add now the Flink Kafka Connector dependency to use the Kafka sink. Add the following entry in the <code>&lt;dependencies&gt;</code> element:</p>

<pre><code> &lt;dependency&gt;
      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
      &lt;artifactId&gt;flink-connector-kafka-0.9_2.10&lt;/artifactId&gt;
      &lt;version&gt;${flink.version}&lt;/version&gt;
 &lt;/dependency&gt;
</code></pre>

<p><strong>4- Exclude Kafka Client to allow use of MapR Streams Client</strong></p>

<p>As you may know, MapR Streams uses the Kafka 0.9.0 API to produce and consume messages. So we need now to remove (exclude) tha Apache Kafka Client API to be sure that Flink use MapR Streams.</p>

<p>In the Flink Kafka Connector dependency add the following exclusion:</p>

<pre><code>  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
    &lt;artifactId&gt;flink-connector-kafka-0.9_2.10&lt;/artifactId&gt;
    &lt;version&gt;${flink.version}&lt;/version&gt;
      &lt;exclusions&gt;
        &lt;exclusion&gt;
          &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
          &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
        &lt;/exclusion&gt;
        &lt;exclusion&gt;
          &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
          &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt;
        &lt;/exclusion&gt;
      &lt;/exclusions&gt;
  &lt;/dependency&gt;
</code></pre>

<p>The Flink project is now ready to use the DataStream using the Kafka Connector so you can send and receive messages from MapR Streams.</p>

<p>Let’s now create a Stream in MapR and write some simple Flink code to use it.</p>

<h2>Create the MapR Streams and Topic</h2>

<p>A stream is a collection of topics that you can manage as a group by:</p>

<ol>
<li>Setting security policies that apply to all topics in that stream</li>
<li>Setting a default number of partitions for each new topic that is created in the stream</li>
<li>Set a time-to-live for messages in every topic in the stream</li>
</ol>


<p>You can find more information about MapR Streams concepts in the <a href="http://maprdocs.mapr.com/51/MapR_Streams/concepts.html">documentation</a>.</p>

<p>On your Mapr Cluster or Sandbox run the following commands:</p>

<pre><code>$ maprcli stream create -path /apps/application-stream -produceperm p -consumeperm p -topicperm p
$ maprcli stream topic create -path /apps/application-stream -topic flink-demo 
</code></pre>

<h3>Install and use MapR Kafka utilities</h3>

<p>Install <code>the mapr-kafka</code> package on your cluster :</p>

<pre><code>yum install mapr-kafka
</code></pre>

<p>Open two terminal windows and run the producer and consumer kafka utilities using the following commands:</p>

<p>Producer</p>

<pre><code>/opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-producer.sh --broker-list this.will.be.ignored:9092 --topic /apps/application-stream:flink-demo
</code></pre>

<p>Consumer</p>

<pre><code>/opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-consumer.sh --new-consumer --bootstrap-server this.will.be.ignored:9092 --topic /apps/application-stream:flink-demo
</code></pre>

<p>In the producer window, you can post some messages and see them in the consumer windows. We will use these tools to follow the interactions between MapR Streams and Flink.</p>

<h2>Write your Flink application</h2>

<p>Let’s now use the Flink Kafka Connector to send messages to MapR Streams and consume them.</p>

<h3>Producer</h3>

<p>The producer generates messages using the <code>SimpleStringGenerator()</code> class and send the string to the <code>/apps/application-stream:flink-demo</code> topic.</p>

<pre><code>  public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    Properties properties = new Properties();
    // properties.setProperty("bootstrap.servers", "&lt;kafka-broker&gt;:9092"); // not used by MapR Streams
    properties.setProperty("streams.buffer.max.time.ms", "200");

    DataStream&lt;String&gt; stream = env.addSource(new SimpleStringGenerator());
    stream.addSink(new FlinkKafkaProducer09&lt;&gt;("/apps/application-stream:flink-demo", new SimpleStringSchema(), properties));

    env.execute();
  }
</code></pre>

<p>The <code>SimpleStringGenerator()</code> method code is available <a href="https://github.com/mapr-demos/mapr-streams-flink-demo/blob/master/src/main/java/com/mapr/demos/WriteToKafka.java#L46-L61">here</a>.</p>

<p>The main steps are:</p>

<ul>
<li>create a new <code>StreamExecutionEnvironment</code> the basis of any Flink application</li>
<li>create a new <code>DataStream</code> in the application environment, the <code>SimpleStringGenerator</code> class implements the <code>[SourceFunction](https://ci.apache.org/projects/flink/flink-docs-release-1.1/api/java/)</code> the base interface for all streams data sources in Flink.</li>
<li>add the <code>FlinkKafkaProducer09</code> sink to the streams; since MapR Streams is based on Kafka API 0.9, it is possible to use the FlinkKafkaProducer09 class; with 2 small differences:

<ul>
<li>the broker list (first parameter) is not used since MapR Streams use the cluster location defined in the <code>/opt/mapr/conf/mapr-clusters.conf</code> class.</li>
<li>the topic name include the path and name of the MapR Stream stream in which the topic is located for example <code>/apps/application-stream:flink-demo</code></li>
</ul>
</li>
</ul>


<h3>Consumer</h3>

<p>The consumer simply reads the messages from the <code>/apps/application-stream:flink-demo</code> topic, and print them into the console.</p>

<pre><code>  public static void main(String[] args) throws Exception {
    // create execution environment
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    Properties properties = new Properties();
    // properties.setProperty("bootstrap.servers", "&lt;kafka-broker&gt;:9092"); // not used by MapR Streams
    properties.setProperty("group.id", "flink_consumer");

    DataStream&lt;String&gt; stream = env.addSource(new FlinkKafkaConsumer09&lt;&gt;(
        "/apps/application-stream:flink-demo", new SimpleStringSchema(), properties) );

    stream.map(new MapFunction&lt;String, String&gt;() {
      private static final long serialVersionUID = -6867736771747690202L;

      @Override
      public String map(String value) throws Exception {
        return "Stream Value: " + value;
      }
    }).print();

    env.execute();
  }
</code></pre>

<p>The main steps are:</p>

<ul>
<li>create a new <code>StreamExecutionEnvironment</code> the basis of any Flink application</li>
<li>create a set of properties with the consumer information, in this application we can only set the consumer <code>group.id</code>. Note that the <code>bootstrap.servers</code> property is not used by MapR Streams, so no need to set it.</li>
<li>use the <code>FlinkKafkaConsumer09</code> to get the message from the MapR Streams topic <code>/apps/application-stream:flink-demo</code></li>
</ul>


<h2>Build and Run the application</h2>

<p>Let’s run the application directly from Maven (or from your favorite IDE).</p>

<p>1- Build the project:</p>

<pre><code>$ mvn clean package
</code></pre>

<p>2- Run the Flink Producer Job</p>

<pre><code>$ mvn exec:java -Dexec.mainClass=com.mapr.demos.WriteToKafka
</code></pre>

<p>3- Run the Flink Consumer Job</p>

<pre><code>$ mvn exec:java -Dexec.mainClass=com.mapr.demos.ReadFromKafka
</code></pre>

<p>In the terminal, you should see the messages generated from the producer</p>

<p>You can now deploy and execute this job on your Flink cluster.</p>

<h2>Conclusion</h2>

<p>In this article you have learned how to use Flink with MapR Streams to write and read data streams. The key element is the configuration of the Maven Dependencies to configure the project to use MapR Streams libraries instead of Kafka ones.</p>

<p>This was originally published on the <a href="https://www.mapr.com/blog/getting-started-apache-flink-and-mapr-streams">MapR blog here</a>.</p>

<p>Learn about what Apache Flink can do and how it maintains consistency and provides flexibility in the &ldquo;<a href="https://www.mapr.com/introduction-to-apache-flink">Introduction to Apache Flink</a>&rdquo; ebook.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With Apache Flink and Kafka]]></title>
    <link href="http://tgrall.github.io/blog/2016/10/12/getting-started-with-apache-flink-and-kafka/"/>
    <updated>2016-10-12T04:54:17+02:00</updated>
    <id>http://tgrall.github.io/blog/2016/10/12/getting-started-with-apache-flink-and-kafka</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p><a href="https://flink.apache.org/">Apache Flink</a> is an open source platform for distributed stream and batch data processing. Flink is a streaming data flow engine with several APIs to create data streams oriented application.</p>

<p>It is very common for Flink applications to use <a href="http://kafka.apache.org/">Apache Kafka</a> for data input and output. This article will guide you into  the steps to use Apache Flink with Kafka.</p>

<p><img class="center" src="/images/posts/flink-kafka/flink-kafka.png" title="Flink-Kafka" ></p>

<!-- more -->


<h3>Prerequisites</h3>

<ul>
<li>Apache Kafka 0.9.x</li>
<li>Git</li>
<li>Maven 3.x or later</li>
</ul>


<h2>Create your Flink Streaming Project</h2>

<p>The first step is to create an Java application, the easiest is to use the flink-quickstart-java archetype, that contains the core dependencies and packaging tasks. This article is similar with the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/quickstart/run_example_quickstart.html">Apache Flink Quick Start Example</a>, with a clear focus on data input and output with MapR Streams.</p>

<p>In this application we will create two jobs:</p>

<ul>
<li><code>WriteToKafka</code> : that generates random string and post them to a MapR Streams Topic using the Kafka Flink Connector and its Producer API.</li>
<li><code>ReadFromKafka</code> : that reads the same topic and print the messages in the standard output using the Kafka Flink Connector and its Consumer. API.</li>
</ul>


<p>The full project is available on GitHub:</p>

<ul>
<li><a href="https://github.com/tgrall/kafka-flink-101">Flink and Kakfa Application</a></li>
</ul>


<p>Let’s create the project using Apache Maven:</p>

<pre><code class="bash">mvn archetype:generate \
      -DarchetypeGroupId=org.apache.flink\
      -DarchetypeArtifactId=flink-quickstart-java \
      -DarchetypeVersion=1.1.2 \
      -DgroupId=com.grallandco.demos \
      -DartifactId=kafka-flink-101 \
      -Dversion=1.0-SNAPSHOT \
      -DinteractiveMode=false 
</code></pre>

<p>Maven will create the following structure:</p>

<pre><code class="bash">tree kafka-flink-101/
kafka-flink-101/
├── pom.xml
└── src
    └── main
        ├── java
        │   └── com
        │       └── grallandco
        │           └── demos
        │               ├── BatchJob.java
        │               ├── SocketTextStreamWordCount.java
        │               ├── StreamingJob.java
        │               └── WordCount.java
        └── resources
            └── log4j.properties

7 directories, 6 files
</code></pre>

<p>This project is configured to create a Jar file that contains your flink project code and also includes all dependencies needed to run it.</p>

<p>The project contains some other sample jobs, we do not need them for this article, you can either keep them to educational purposes or simply remove them from the project.</p>

<h2>Add Kafka Connector</h2>

<p>Open the <code>pom.xml</code> and add the following dependencies to your project:</p>

<p>As a first step, we have to add the Flink Kafka connector as a dependency so that we can use the Kafka sink. Add this to the pom.xml file in the dependencies section:</p>

<p>You must add now the Flink Kafka Connector dependency to use the Kafka sink. Add the following entry in the <code>&lt;dependencies&gt;</code> element:</p>

<pre><code> &lt;dependency&gt;
      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
      &lt;artifactId&gt;flink-connector-kafka-0.9_2.10&lt;/artifactId&gt;
      &lt;version&gt;${flink.version}&lt;/version&gt;
 &lt;/dependency&gt;
</code></pre>

<p>The Flink project is now ready to use the DataStream using the Kafka Connector so you can send and receive messages from Apache Kafka.</p>

<h2>Install and Start Kafka</h2>

<p>Download Kafka, enter the following commands in your terminal:</p>

<pre><code class="bash">curl -O http://www.us.apache.org/dist/kafka/0.9.0.0/kafka_2.11-0.9.0.0.tgz
tar -xzf kafka_2.11-0.9.0.0.tgz
cd kafka_2.11-0.9.0.0
</code></pre>

<p>Kafka uses ZooKeeper, if you do not have Zookeeper running, you can start it using the following command:</p>

<pre><code class="bash">./bin/zookeeper-server-start.sh config/zookeeper.properties
</code></pre>

<p>Start a Kafka broker by running the following command in a new terminal:</p>

<pre><code class="bash">./bin/kafka-server-start.sh config/server.properties
</code></pre>

<p>In another terminal, run the following command to create a Kafka topic called <code>flink-demo</code>:</p>

<pre><code class="bash">./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic flink-demo
</code></pre>

<p>Use the Kafka tools to post and consume messages to the <code>flink-demo</code> topic.</p>

<p>Producer</p>

<pre><code class="bash">./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic flink-demo
</code></pre>

<p>Consumer</p>

<pre><code class="bash">./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic flink-demo --from-beginning
</code></pre>

<p>In the producer window, you can post some messages and see them in the consumer windows. We will use these tools to follow the interactions between Kafka and Flink.</p>

<h2>Write your Flink application</h2>

<p>Let’s now use the Flink Kafka Connector to send messages to Kafka and consume them.</p>

<h3>Producer</h3>

<p>The producer generates messages using the <code>SimpleStringGenerator()</code> class and send the string to the <code>flink-demo</code> topic.</p>

<pre><code>  public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    Properties properties = new Properties();
    properties.setProperty("bootstrap.servers", “localhost:9092"); 

    DataStream&lt;String&gt; stream = env.addSource(new SimpleStringGenerator());
    stream.addSink(new FlinkKafkaProducer09&lt;&gt;("flink-demo", new SimpleStringSchema(), properties));

    env.execute();
  }
</code></pre>

<p>The <code>SimpleStringGenerator()</code> method code is available <a href="https://github.com/tgrall/kafka-flink-101/blob/master/src/main/java/com/grallandco/demos/WriteToKafka.java#L45-L60">here</a>.</p>

<p>The main steps are:</p>

<ul>
<li>create a new <code>StreamExecutionEnvironment</code> the basis of any Flink application</li>
<li>create a new <code>DataStream</code> in the application environment, the <code>SimpleStringGenerator</code> class implements the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/api/java/">SourceFunction</a> the base interface for all streams data sources in Flink.</li>
<li>add the <code>FlinkKafkaProducer09</code> sink to the topic.</li>
</ul>


<h3>Consumer</h3>

<p>The consumer simply reads the messages from the <code>flink-demo</code> topic, and print them into the console.</p>

<pre><code>  public static void main(String[] args) throws Exception {
    // create execution environment
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    Properties properties = new Properties();
    properties.setProperty("bootstrap.servers", “localhost:9092");
    properties.setProperty("group.id", "flink_consumer");

    DataStream&lt;String&gt; stream = env.addSource(new FlinkKafkaConsumer09&lt;&gt;(
        "flink-demo", new SimpleStringSchema(), properties) );

    stream.map(new MapFunction&lt;String, String&gt;() {
      private static final long serialVersionUID = -6867736771747690202L;

      @Override
      public String map(String value) throws Exception {
        return "Stream Value: " + value;
      }
    }).print();

    env.execute();
  }
</code></pre>

<p>The main steps are:</p>

<ul>
<li>create a new <code>StreamExecutionEnvironment</code> the basis of any Flink application</li>
<li>create a set of properties with the consumer information, in this application we can only set the consumer <code>group.id</code>.</li>
<li>use the <code>FlinkKafkaConsumer09</code> to get the message from the topic <code>flink-demo</code></li>
</ul>


<h2>Build and Run the application</h2>

<p>Let’s run the application directly from Maven (or from your favorite IDE).</p>

<p>1- Build the project:</p>

<pre><code>$ mvn clean package
</code></pre>

<p>2- Run the Flink Producer Job</p>

<pre><code>$ mvn exec:java -Dexec.mainClass=com.mapr.demos.WriteToKafka
</code></pre>

<p>3- Run the Flink Consumer Job</p>

<pre><code>$ mvn exec:java -Dexec.mainClass=com.mapr.demos.ReadFromKafka
</code></pre>

<p>In the terminal, you should see the messages generated from the producer</p>

<p>You can now deploy and execute this job on your Flink cluster.</p>

<h2>Conclusion</h2>

<p>In this article you have learned how to use Flink with kafka to write and read data streams.</p>

<p>Learn about what Apache Flink can do and how it maintains consistency and provides flexibility in the &ldquo;<a href="https://www.mapr.com/introduction-to-apache-flink">Introduction to Apache Flink</a>&rdquo; ebook.</p>
]]></content>
  </entry>
  
</feed>
