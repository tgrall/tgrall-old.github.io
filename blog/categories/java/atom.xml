<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Java | Tug's Blog]]></title>
  <link href="http://tgrall.github.io/blog/categories/java/atom.xml" rel="self"/>
  <link href="http://tgrall.github.io/"/>
  <updated>2019-09-03T11:24:02+02:00</updated>
  <id>http://tgrall.github.io/</id>
  <author>
    <name><![CDATA[Tug Grall]]></name>
    <email><![CDATA[tugdual@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Getting Started With Redis Streams &amp; Java]]></title>
    <link href="http://tgrall.github.io/blog/2019/09/02/getting-with-redis-streams-and-java/"/>
    <updated>2019-09-02T09:24:24+02:00</updated>
    <id>http://tgrall.github.io/blog/2019/09/02/getting-with-redis-streams-and-java</id>
    <content type="html"><![CDATA[<p>As you may have seen, I have joined <a href="https://www.redislabs.com">Redis Labs</a> a month ago; one of the first task as a new hire is to learn more about Redis. So I learned, and I am still learning.</p>

<p>This is when I discovered <a href="https://redis.io/topics/streams-intro">Redis Streams</a>. I am a big fan of streaming-based applications so it is natural that I start with a small blog post explaining how to use Redis Streams and Java.</p>

<p><strong><em>What is Redis Streams?</em></strong></p>

<p>Redis Streams is a Redis Data Type, that represents a log so you can add new information/message in an append-only mode <em>(this is not 100% accurate since you can remove messages from the log)</em>. Using Redis Streams you can build &ldquo;Kafka Like&rdquo; applications, what I mean by that you can:</p>

<ul>
<li>create applications that publish and consume messages (nothing extraordinary here, you could already do that with Redi Pub/Sub)</li>
<li>consume messages that are published even when your client application (consumer) is not running. This is a big difference with Redis Pub/Sub</li>
<li>consume messages starting a specific offset, for example, read the whole history, or only new messages</li>
</ul>


<p>In addition to this, Redis Streams has the concept of <strong>Consumer Groups</strong>. Redis Streams Consumer Groups, like Apache Kafka ones, allows the client applications to consume messages in a distributed fashion (multiple clients), providing an easy way to scale and create highly available systems.</p>

<p><img class="center" src="/images/posts/getting-with-redis-streams-and-java/redis-streams-101-img-1.png"></p>

<p>Enroll in the <a href="https://university.redislabs.com/courses/course-v1:redislabs+RU202+2019_03/about">Redis University: Redis Streams</a> to learn more and get certified.</p>

<p><strong><em>Sample Application</em></strong></p>

<p>The <a href="https://github.com/tgrall/redis-streams-101-java">redis-streams-101-java GitHub Repository</a> contains sample code that shows how to</p>

<ul>
<li>post messages to a streams</li>
<li>consume messages using a consumer group</li>
</ul>


<!--more -->


<h4>Prerequisites</h4>

<ul>
<li>Redis 5.x, you have here multiple options:

<ul>
<li><a href="https://redis.io">Download</a> and install Redis Community</li>
<li>Install and Run a Docker image: <a href="https://hub.docker.com/_/redis">Community</a> or <a href="https://hub.docker.com/r/redislabs/redis">Redis Enterprise</a></li>
<li>Create a online instance on <a href="https://redislabs.com/redis-enterprise/essentials/">Redis Labs Cloud</a> (30mb for free)</li>
</ul>
</li>
<li>Java 8 or later</li>
<li>Apache Maven 3.5.x</li>
<li>Git</li>
</ul>


<h3>Java &amp; Redis Streams</h3>

<p>Redis has many Java clients developed by the community, as you can see on the <a href="https://redis.io/clients#java">Redis.io site</a>.</p>

<p>It looks, based on my short experience with Redis so far, that the most complete one around Redis Streams support is <a href="https://lettuce.io">Lettuce</a>, this is the one I will be using in the following code.</p>

<h4>1- Adding Lettuce to Your Maven Project</h4>

<p>Add the following dependency to your project file:</p>

<pre><code class="xml">        &lt;dependency&gt;
            &lt;groupId&gt;io.lettuce&lt;/groupId&gt;
            &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt;
            &lt;version&gt;5.1.8.RELEASE&lt;/version&gt;
        &lt;/dependency&gt;
</code></pre>

<h4>2- Connecting to Redis</h4>

<p>Import the following classes</p>

<pre><code class="java">import io.lettuce.core.*;
import io.lettuce.core.api.StatefulRedisConnection;
import io.lettuce.core.api.sync.RedisCommands;
</code></pre>

<p>Then connect with:</p>

<pre><code class="java">RedisClient redisClient = RedisClient.create("redis://password@host:port"); // change to reflect your environment
StatefulRedisConnection&lt;String, String&gt; connection = redisClient.connect();
RedisCommands&lt;String, String&gt; syncCommands = connection.sync();
</code></pre>

<p>When your application is done with the connection you should disconnect with the following code:</p>

<pre><code class="java">connection.close();
redisClient.shutdown();
</code></pre>

<h4>3- Sending Message to Streams</h4>

<p>Once you have a connection you can send a message. In this example, I will let Redis generate the message ID, which is time-based, and the body will be built using a Map representing IoT data, for example, a weather data capturing Wind speed and direction in real-time.</p>

<pre><code class="java">    public static void main(String[] args) {

        RedisClient redisClient = RedisClient.create("redis://localhost:6379"); // change to reflect your environment
        StatefulRedisConnection&lt;String, String&gt; connection = redisClient.connect();
        RedisCommands&lt;String, String&gt; syncCommands = connection.sync();

        Map&lt;String, String&gt; messageBody = new HashMap&lt;&gt;();
        messageBody.put( "speed", "15" );
        messageBody.put( "direction", "270" );
        messageBody.put( "sensor_ts", String.valueOf(System.currentTimeMillis()) );

        String messageId = syncCommands.xadd(
                "weather_sensor:wind",
                messageBody);

        System.out.println( String.format("Message %s : %s posted", messageId, messageBody) );

        connection.close();
        redisClient.shutdown();

    }
</code></pre>

<p>Let me explain the code:</p>

<ul>
<li>Lines 3-5 are used to connect to Redis</li>
<li>Lines 7-10 are used to create the message body, using a Map, since Redis Streams messages are string key/values.</li>
<li>Lines 12-14 call the <code>syncCommands.xadd()</code> method using the streams key &ldquo;weather_sensor:wind&rdquo; and the message body itself

<ul>
<li>this method returns the message ID.</li>
</ul>
</li>
<li>line 16 just print the message ID and content</li>
<li>the lines 18-19 close the connection and client.</li>
</ul>


<p>The complete producer code is available <a href="https://github.com/tgrall/redis-streams-101-java/blob/master/src/main/java/com/kanibl/redis/streams/simple/RedisStreams101Producer.java">here</a>.</p>

<h4>4- Consuming Messages</h4>

<p>Redis Streams offers various way to consume/read messages using the commands: <a href="https://redis.io/commands/xrange">XRANGE</a>, <a href="https://redis.io/commands/xrevrange">XREVRANGE</a>, <a href="https://redis.io/commands/xread">XREAD</a>, <a href="https://redis.io/commands/xreadgroup">XREADGROUP</a>.</p>

<p>I want to keep the article short and close to the way you would build an application with Apache Kafka, this is why I will use the <a href="https://redis.io/commands/xreadgroup">XREADGROUP</a> command from Lettuce.</p>

<p>The Consumer Groups allow developers to create a group of clients that will cooperate to consume messages from the streams (for scale and high availability); it is also a way to associate the client to specific applications roles; for example:</p>

<ul>
<li>a consumer group called &ldquo;data warehouse&rdquo; will consume messages and send them to a data warehouse</li>
<li>another consumer group called &ldquo;aggregator&rdquo; will consume the messages and aggregate the data and send them to another sink (another stream or storage)</li>
</ul>


<p>Each of this group will act independently, and each of this group could have multiple &ldquo;consumers&rdquo; (client).</p>

<p>Let&rsquo;s see how you use this in Java.</p>

<pre><code class="java">...

        try {
            // WARNING: Streams must exist before creating the group
            //          This will not be necessary in Lettuce 5.2, see https://github.com/lettuce-io/lettuce-core/issues/898
            syncCommands.xgroupCreate( XReadArgs.StreamOffset.from("weather_sensor:wind", "0-0"), "application_1"  );
        }
        catch (RedisBusyException redisBusyException) {
            System.out.println( String.format("\t Group '%s already' exists","application_1"));
        }


        System.out.println("Waiting for new messages");

        while(true) {

            List&lt;StreamMessage&lt;String, String&gt;&gt; messages = syncCommands.xreadgroup(
                    Consumer.from("application_1", "consumer_1"),
                    XReadArgs.StreamOffset.lastConsumed("weather_sensor:wind")
            );

            if (!messages.isEmpty()) {
                System.out.println( messages );
            }


        }

...
</code></pre>

<p>This code is a subset of the <code>main()</code> method I have removed the connection management part, to add readability. Let&rsquo;s take a look to the code:</p>

<ul>
<li>line 3 to 10, using the method <code>xgroupCreate()</code>, that matches the <a href="https://redis.io/commands/xgroup">XGROUP CREATE</a> command,

<ul>
<li>is used to create a new group called <code>application_1</code>,</li>
<li>consume messages from the stream <code>weather_sensor:wind</code></li>
<li>starting at the first message in the stream, this is indicated using the message ID <code>0-0</code>. <em>Note that it is also possible to indicate to the group to start to read at a specific message ID, or only the new messages that arrive after the creating of the consumer group using <code>$</code> special ID (or the helper method <code>XReadArgs.StreamOffset.latest()</code></em>.</li>
</ul>
</li>
<li>line 15 to 27, in this example we use an infinite loop (<code>while(true)</code>) to wait for any new messages published to the streams</li>
<li>line 17 to 20, the method <code>xreadgroup()</code> returns the messages based on the group configuration

<ul>
<li>line 18 define the consumer named <code>consumer_1</code> that is associated with the group <code>application_1</code>: you can create new group do distribute the read to multiple clients</li>
<li>line 19 indicates where to start, in this case, <code>StreamOffset.lastConsumed("weather_sensor:wind")</code> the consumer will consume messages that have not been read already. With the current configuration of the group (offset <code>0-0</code>), when the consumer will start for the first time, it will read all the existing messages.</li>
<li>the <a href="https://redis.io/commands/xreadgroup">XREADGROUP</a> command by default sends an acknowledgment for each consumed message.</li>
</ul>
</li>
</ul>


<p>The complete consumer code is available <a href="https://github.com/tgrall/redis-streams-101-java/blob/master/src/main/java/com/kanibl/redis/streams/simple/RedisStreams101Consumer.java">here</a>.</p>

<h3>Build &amp; Run the Simple Java Application</h3>

<p>Now that you have a better understanding of the code, let&rsquo;s run the producer and consumer. You can run this from your IDE, or using Maven.</p>

<p>Let&rsquo;s do it using Maven CLI, for this open 2 terminals:</p>

<ul>
<li>one to produce messages</li>
<li>one to consume them</li>
</ul>


<p><em>1- Clone and Build the project</em></p>

<pre><code class="bash">&gt; git clone https://github.com/tgrall/redis-streams-101-java.git

&gt; cd redis-streams-101-java

&gt; mvn clean verify
</code></pre>

<p><em>2- Post a new message</em></p>

<pre><code class="bash">
&gt; mvn exec:java -Dexec.mainClass="com.kanibl.redis.streams.simple.RedisStreams101Producer"
</code></pre>

<p><em>3- Consume messages</em></p>

<p>Open a new terminal and run the following command:</p>

<pre><code class="bash">
&gt; mvn exec:java -Dexec.mainClass="com.kanibl.redis.streams.simple.RedisStreams101Consumer"
</code></pre>

<p>The consumer will start and consume the message you just posted, and wait for any new messages.</p>

<p><em>4- In the first terminal post 100 new messages</em></p>

<pre><code class="bash">
&gt; mvn exec:java -Dexec.mainClass="com.kanibl.redis.streams.simple.RedisStreams101Producer" -Dexec.args="100"
</code></pre>

<p>The consumer will receive and print all the messages.</p>

<p><em>5- Kill the consumer and post more messages</em></p>

<p>Let&rsquo;s now do another test, stop the consumer using a simple <code>ctrl+C</code>.</p>

<p>Then post 5 new messages.</p>

<pre><code class="bash">
&gt; mvn exec:java -Dexec.mainClass="com.kanibl.redis.streams.simple.RedisStreams101Producer" -Dexec.args="5"
</code></pre>

<p>The messages are not yet consumed by any application, but still store in Redis Streams.</p>

<p>So when you start the consumer, it will consumes these new messages.</p>

<pre><code class="bash">
&gt; mvn exec:java -Dexec.mainClass="com.kanibl.redis.streams.simple.RedisStreams101Consumer"
</code></pre>

<p>This is a one of the differences between <a href="https://redis.io/topics/streams-intro">Redis Streams</a> and <a href="https://redis.io/topics/pubsub">Redis PubSub</a>. The producer application has publish many messages while the consumer application was not running. Since the consumer is ran with <code>StreamOffset.lastConsumed()</code>, when the consumer is starting, it looks to the last consumed ID, and start to read the streams from there. This method generate a XGROUPREAD command with the group</p>

<h3>Conclusion</h3>

<p>In this small project, you have learned, how to use Lettuce, a Java client for Redis to:</p>

<ul>
<li>publish messages to a stream</li>
<li>create a consumer group</li>
<li>consume messages using the consumer group.</li>
</ul>


<p>This is a very basic example, and in a next post I will show you how to work with multiple consumers, and to configure the Consumer Group and Consumers to control which messages you want to read</p>

<p>More to come!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With MQTT and Java]]></title>
    <link href="http://tgrall.github.io/blog/2017/01/02/getting-started-with-mqtt/"/>
    <updated>2017-01-02T16:03:09+01:00</updated>
    <id>http://tgrall.github.io/blog/2017/01/02/getting-started-with-mqtt</id>
    <content type="html"><![CDATA[<p>MQTT (MQ Telemetry Transport) is a lightweight publish/subscribe messaging protocol.
MQTT is used a lot in the Internet of Things applications, since it has been designed to
run on remote locations with system with small footprint.</p>

<p>The MQTT 3.1 is an OASIS standard, and you can find all the information at <a href="http://mqtt.org/">http://mqtt.org/</a></p>

<p>This article will guide you into the various steps to run your first MQTT application:</p>

<ol>
<li>Install and Start a MQTT Broker</li>
<li>Write an application that publishes messages</li>
<li>Write an application that consumes messages</li>
</ol>


<p>The source code of the sample application is available on <a href="https://github.com/tgrall/mqtt-sample-java">GitHub</a>.</p>

<!-- more -->


<h4>Prerequisites</h4>

<ul>
<li>Apache Maven 3.x</li>
<li>Git</li>
</ul>


<h3>Install and Start a MQTT Broker</h3>

<p>You can find many MQTT Brokers, for this example I will use one of the most common broker <a href="https://mosquitto.org">Mosquitto</a>.</p>

<p>You can download and install from the <a href="https://mosquitto.org/download/">binary package</a>. I have used <a href="http://brew.sh/">Homebrew</a> to install it on my Mac:</p>

<pre><code>$ brew install mosquitto
</code></pre>

<p>Start the MQTT Broker with the default configuration</p>

<pre><code>$ /usr/local/sbin/mosquitto
</code></pre>

<h3>Publish and Consume messages</h3>

<p>Open two terminal windows and run the following commands :</p>

<p>Consume</p>

<pre><code>$ mosquitto_sub -h 127.0.0.1 -t iot_data
</code></pre>

<p>Publish</p>

<pre><code>$ mosquitto_pub -h 127.0.0.1 -t iot_data -m "Hello world"
</code></pre>

<p>You should see the message <code>Hello world</code> in the consumer/subscriber window.</p>

<h3>Write your first MQTT Application</h3>

<p>For this example I will write a small Java application, since it is the language
that I am using in my global project.</p>

<h4>Maven Dependencies</h4>

<p>Add the <a href="https://eclipse.org/paho/">Eclipse Paho</a> dependency to your Maven project</p>

<pre><code class="xml">&lt;dependency&gt;
  &lt;groupId&gt;org.eclipse.paho&lt;/groupId&gt;
  &lt;artifactId&gt;org.eclipse.paho.client.mqttv3&lt;/artifactId&gt;
  &lt;version&gt;1.1.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<h4>Publishing a Message</h4>

<p>Publishing a message is quite easy, create a MqttClient and use it to post on a topic.</p>

<pre><code class="java">MqttClient client = new MqttClient("tcp://localhost:1883", MqttClient.generateClientId());
client.connect();
MqttMessage message = new MqttMessage();
message.setPayload("Hello world from Java".getBytes());
client.publish("iot_data", message);
client.disconnect();
</code></pre>

<p>You have many other options, configurations that you can use when posting a message
such as security, quality of service (QoS), and more; but in this post I want to simply
show how easy is to publish and consume MQTT messages.</p>

<h4>Consuming messages</h4>

<p>To consume messages you need to implement a <code>org.eclipse.paho.client.mqttv3.MqttCallback</code> that will receive the message and used this Callback class in the MqttClient of the Subscriber application.</p>

<p>The Callback class:</p>

<pre><code class="java">public class SimpleMqttCallBack implements MqttCallback {

  public void connectionLost(Throwable throwable) {
    System.out.println("Connection to MQTT broker lost!");
  }

  public void messageArrived(String s, MqttMessage mqttMessage) throws Exception {
    System.out.println("Message received:\n\t"+ new String(mqttMessage.getPayload()) );
  }

  public void deliveryComplete(IMqttDeliveryToken iMqttDeliveryToken) {
    // not used in this example
  }
}
</code></pre>

<p>This Callback class is used in the Subscriber application as follow:</p>

<pre><code class="java">MqttClient client=new MqttClient("tcp://localhost:1883", MqttClient.generateClientId());
client.setCallback( new SimpleMqttCallBack() );
client.connect();
</code></pre>

<p>Like for the publisher, I am using the broker and client without any option (QoS, security).</p>

<h2>Build and Run the Application</h2>

<p><strong>1- Get the Sample Code</strong></p>

<p>Clone the project from GitHub</p>

<pre><code>$ git clone https://github.com/tgrall/mqtt-sample-java.git
</code></pre>

<p><strong>2- Build the project with Apache Maven:</strong></p>

<p>This project is a simple Java application that runs a publisher and subscriber using the <a href="https://eclipse.org/paho/">Eclipse Paho library</a>.</p>

<pre><code>$ mvn clean package
</code></pre>

<p>For convenience, the example programs project is set up so that the maven package target produces a single executable,
<code>/mqtt-sample</code>, that includes all of the example programs and dependencies.</p>

<p><strong>3- Run the Subscriber</strong></p>

<p>The subscriber will receive and print all messages published on the <code>iot_data</code> topic.</p>

<pre><code>$ ./target/mqtt-sample subscriber
</code></pre>

<p><strong>4- Run the Publisher</strong></p>

<p>Run the publisher with the following command, the second parameter is the message to publish</p>

<pre><code>$ ./target/mqtt-sample publisher "My first MQTT message..."
</code></pre>

<h2>Conclusion</h2>

<p>In this article you have learned how to:</p>

<ul>
<li>Install and start a MQTT Broker, Mosquitto</li>
<li>Create a publisher and subscriber developed in Java</li>
</ul>


<p>This article is very simple by choice, to quickly run your first MQTT Application. I wrote this article as part of a global IoT project I am working on that will capture devices data, publish them into MapR Converged Data Platform using MQTT and MapR Streams; this is why I used Java for the application. You can use any MQTT client library to build the publishers and subscribers.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With MapR Streams]]></title>
    <link href="http://tgrall.github.io/blog/2016/03/10/getting-started-with-mapr-streams/"/>
    <updated>2016-03-10T10:09:32+01:00</updated>
    <id>http://tgrall.github.io/blog/2016/03/10/getting-started-with-mapr-streams</id>
    <content type="html"><![CDATA[<p>You can find a new tutorial that explains how to deploy an Apache Kafka application to MapR Streams, the tutorial is available here:</p>

<ul>
<li><a href="https://www.mapr.com/blog/getting-started-sample-programs-mapr-streams">Getting Started with MapR Streams</a></li>
</ul>


<p>MapR Streams is a new distributed messaging system for streaming event data at scale, and it’s integrated into the MapR converged platform.
MapR Streams uses the Apache Kafka API, so if you’re already familiar with Kafka, you’ll find it particularly easy to get started with MapR Streams.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Drill : How to Create a New Function?]]></title>
    <link href="http://tgrall.github.io/blog/2015/07/22/apache-drill-how-to-create-a-new-function/"/>
    <updated>2015-07-22T02:32:54+02:00</updated>
    <id>http://tgrall.github.io/blog/2015/07/22/apache-drill-how-to-create-a-new-function</id>
    <content type="html"><![CDATA[<p><a href="https://drill.apache.org/">Apache Drill</a> allows users to explore <em>any type of</em> data using ANSI SQL. This is great, but Drill goes even further than that and allows you to create custom functions to extend the query engine. These custom functions have all the performance of any of the Drill primitive operations, but allowing that performance makes writing these functions a little trickier than you might expect.</p>

<p>In this article, I&rsquo;ll explain step by step how to create and deploy a new function using a very basic example. Note that you can find lot of information about <a href="https://drill.apache.org/docs/develop-custom-functions-introduction/">Drill Custom Functions in the documentation</a>.</p>

<p>Let&rsquo;s create a new function that allows you to mask some characters in a string, and let&rsquo;s make it very simple. The new function will allow user to hide <em>x</em> number of characters from the start and replace then by any characters of their choice. This will look like:</p>

<pre><code>MASK( 'PASSWORD' , '#' , 4 ) =&gt; ####WORD
</code></pre>

<p>You can find the full project in the following <a href="https://github.com/tgrall/drill-simple-mask-function">Github Repository</a>.</p>

<p>As mentioned before, we could imagine many advanced features to this, but my goal is to focus on the steps to write a custom function, not
so much on what the function does.</p>

<!--more-->


<h2>Prerequisites</h2>

<p>For this you will need:</p>

<ul>
<li>Java Developer Kit 7 or later</li>
<li>Apache Drill 1.1 or later</li>
<li>Maven 3.0 or later</li>
</ul>


<h2>Dependencies</h2>

<p>The following Drill dependency should be added to your maven project</p>

<pre><code class="xml">&lt;dependency&gt;
      &lt;groupId&gt;org.apache.drill.exec&lt;/groupId&gt;
      &lt;artifactId&gt;drill-java-exec&lt;/artifactId&gt;
      &lt;version&gt;1.1.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<h2>Source</h2>

<p>The <code>Mask</code> function is an implementation of the <a href="https://github.com/apache/drill/blob/master/exec/java-exec/src/main/java/org/apache/drill/exec/expr/DrillSimpleFunc.java"><code>DrillSimpleFunc</code></a>.</p>

<p>Developers can create 2 types of custom functions:</p>

<ul>
<li>Simple Functions: these functions have a single row as input and produce a single value as output</li>
<li>Aggregation Functions: that will accept multiple rows as input and produce one value as output</li>
</ul>


<p>Simple functions are often referred to as UDF&rsquo;s which stands for user defined function.  Aggregation functions are referred to as UDAF which
stands for user defined aggregation function.</p>

<p>In this example, we just need to transform the value of a column on each row, so a simple function is enough.</p>

<h4>Create the function</h4>

<p>The first step is to implement the <a href="https://github.com/apache/drill/blob/master/exec/java-exec/src/main/java/org/apache/drill/exec/expr/DrillSimpleFunc.java"><code>DrillSimpleFunc</code></a> interface.</p>

<pre><code class="java">package org.apache.drill.contrib.function;

import org.apache.drill.exec.expr.DrillSimpleFunc;
import org.apache.drill.exec.expr.annotations.FunctionTemplate;

@FunctionTemplate(
        name="mask",
        scope= FunctionTemplate.FunctionScope.SIMPLE,
        nulls = FunctionTemplate.NullHandling.NULL_IF_NULL
)
public class SimpleMaskFunc implements DrillSimpleFunc{

    public void setup() {

    }

    public void eval() {

    }
}
</code></pre>

<p>The behavior of the function is driven by annotations (line 6-10)
  * <em>Name</em> of the function
  * <em>Scope</em> of the function, in our case Simple
  * What to do when the value is NULL, in this case Reverse will just returns NULL</p>

<p>Now we need to implement the logic of the function using <code>setup()</code> and <code>eval()</code> methods.</p>

<ul>
<li><code>setup</code> is self-explanatory, and in our case we do not need to setup anything.</li>
<li><code>eval</code> that is the core of the function. As you can see this method does not have any parameter, and return void. So how does it work?</li>
</ul>


<p>In fact the function will be generated dynamically (see <a href="https://github.com/apache/drill/blob/master/exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/DrillSimpleFuncHolder.java#L42">DrillSimpleFuncHolder</a>), and the input parameters and output holders are defined using holders by annotations. Let&rsquo;s look into this.</p>

<pre><code class="java">import io.netty.buffer.DrillBuf;
import org.apache.drill.exec.expr.DrillSimpleFunc;
import org.apache.drill.exec.expr.annotations.FunctionTemplate;
import org.apache.drill.exec.expr.annotations.Output;
import org.apache.drill.exec.expr.annotations.Param;
import org.apache.drill.exec.expr.holders.IntHolder;
import org.apache.drill.exec.expr.holders.NullableVarCharHolder;
import org.apache.drill.exec.expr.holders.VarCharHolder;

import javax.inject.Inject;


@FunctionTemplate(
        name = "mask",
        scope = FunctionTemplate.FunctionScope.SIMPLE,
        nulls = FunctionTemplate.NullHandling.NULL_IF_NULL
)
public class SimpleMaskFunc implements DrillSimpleFunc {

    @Param
    NullableVarCharHolder input;

    @Param(constant = true)
    VarCharHolder mask;

    @Param(constant = true)
    IntHolder toReplace;

    @Output
    VarCharHolder out;

    @Inject
    DrillBuf buffer;

    public void setup() {
    }

    public void eval() {

    }

}
</code></pre>

<p>We need to define the parameters of the function. In this case we have 3 parameters, each defined using the <code>@Param</code> annotation.  In addition, we also have to define the returned value using the <code>@Output</code> annotation.</p>

<p>The parameters of our mask function are:</p>

<ul>
<li>A nullable string</li>
<li>The mask char or string</li>
<li>The number of characters to replace starting from the first</li>
</ul>


<p>The function returns :</p>

<ul>
<li>A string</li>
</ul>


<p>For each of these parameters you have to use an holder class. For the <code>String</code>, this is managed by a <code>VarCharHolder</code> or <code>NullableVarCharHolder</code> -lines 21, 24,30- that provides a buffer to manage larger objects in a efficient way. Since we are manipulating a <code>VarChar</code> you also have to inject another buffer that will be used for the output -line 33-. Note that Drill doesn&rsquo;t actually use the Java heap for data being processed in a query but instead keeps this data off the heap and manages the life-cycle for us without using the Java
garbage collector.</p>

<p>We are almost done since we have the proper class, the input/output object, we just need to implement the <code>eval()</code> method itself, and use these objects.</p>

<pre><code class="java">public void eval() {

    // get the value and replace with
    String maskValue = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.getStringFromVarCharHolder(mask);
    String stringValue = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(input.start, input.end, input.buffer);

    int numberOfCharToReplace = Math.min(toReplace.value, stringValue.length());

    // build the mask substring
    String maskSubString = com.google.common.base.Strings.repeat(maskValue, numberOfCharToReplace);
    String outputValue = (new StringBuilder(maskSubString)).append(stringValue.substring(numberOfCharToReplace)).toString();

    // put the output value in the out buffer
    out.buffer = buffer;
    out.start = 0;
    out.end = outputValue.getBytes().length;
    buffer.setBytes(0, outputValue.getBytes());
}
</code></pre>

<p>The code is quite simple:</p>

<ul>
<li>Get the mask itself - line 4</li>
<li>Get the value - line 5</li>
<li>Get the number of character to replace - line 7</li>
<li>Generate a new string with masked values - lines 10/11</li>
<li>Create and populate the output buffer - lines 14 to 17</li>
</ul>


<p>This code does, however, look a bit strange to somebody used to reading Java code. This strangeness arises because the final code that is executed in a query will actually be generated on the fly. This allows Drill to leverage Java&rsquo;s just-in-time (JIT) compiler for maximum speed. To make this work, you have to respect some basic rules:</p>

<ul>
<li><strong>Do not use imports, but instead use the fully qualified class name</strong>, this is what is done on line 10 with the <code>Strings</code> class. (coming from the Google Guava API packaged in Apache Drill)</li>
<li>The <code>ValueHolders</code> classes, in our case <code>VarCharHolder</code> and <code>IntHolder</code> should be manipulated like structs, so you must call helper methods, for example <code>getStringFromVarCharHolder</code> and <code>toStringFromUTF8</code>. Calling methods like <code>toString</code> will result in very bad problems.</li>
</ul>


<p>Starting in Apache Drill 1.3.x, it is mandatory to specify the package name of your function in the <code>./resources/drill-module.conf</code> file as follow:</p>

<pre><code>drill {
  classpath.scanning {
    packages : ${?drill.classpath.scanning.packages} [
      org.apache.drill.contrib.function
    ]
  }
}
</code></pre>

<p>We are now ready to deploy and test this new function.</p>

<h3>Package</h3>

<p>Once again since, Drill will generate source, <em><strong>you must prepare your package in a way that classes and sources of the function are present in the classpath</strong></em>. This is different from the way that Java code is normally packaged but is necessary for Drill to be able to do the necessary code generation. Drill uses the compiled code to access the annotations and uses the source code to do code generation.</p>

<p>An easy way to do that is to use maven to build your project, and, in particular, use the <a href="https://maven.apache.org/plugins/maven-source-plugin/">maven-source-plugin</a> like this in your <code>pom.xml</code> file:</p>

<pre><code class="xml">&lt;plugin&gt;
    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
    &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt;
    &lt;version&gt;2.4&lt;/version&gt;
    &lt;executions&gt;
        &lt;execution&gt;
            &lt;id&gt;attach-sources&lt;/id&gt;
            &lt;phase&gt;package&lt;/phase&gt;
            &lt;goals&gt;
                &lt;goal&gt;jar-no-fork&lt;/goal&gt;
            &lt;/goals&gt;
        &lt;/execution&gt;
    &lt;/executions&gt;
&lt;/plugin&gt;
</code></pre>

<p>Now, when you build using <code>mvn package</code>, Maven will generate 2 jars:</p>

<ul>
<li>The default jar with the classes and resources (<em>drill-simple-mask-1.0.jar</em>)</li>
<li>A second jar with the sources (<em>drill-simple-mask-1.0-sources.jar</em>)</li>
</ul>


<p>Finally you must add a <code>drill-module.conf</code> file in the resources folder of your project, to tell Drill that your jar contains a custom function. If you have no specific configuration to set for your function you can keep this file empty.</p>

<p>We are all set, you can now package and deploy the new function, just package and copy the Jars into the Drill 3rd party folder; $DRILL_HOME/jars/3rdparty , where $DRILL_HOME being your Drill installation folder.</p>

<pre><code>mvn clean package

cp target/*.jar  $DRILL_HOME/jars/3rdparty
</code></pre>

<p>Restart drill.</p>

<h3>Run !</h3>

<p>You should now be able to use your function in your queries:</p>

<pre><code>SELECT MASK(first_name, '*' , 3) FIRST , MASK(last_name, '#', 7) LAST  FROM cp.`employee.json` LIMIT 5;
+----------+------------+
|  FIRST   |    LAST    |
+----------+------------+
| ***ri    | ######     |
| ***rick  | #######    |
| ***hael  | ######     |
| ***a     | #######ez  |
| ***erta  | #######    |
+----------+------------+
</code></pre>

<h2>Conclusion</h2>

<p>In this simple project you have learned how to write, deploy and use a custom Apache Drill Function. You can now extend this to create your own function.</p>

<p>One important thing to remember when extending Apache Drill (using a custom function, storage plugin or format), is that Drill runtime is generating dynamically lot of code. This means you may have to use a very specific pattern when writing and deploying your extensions. With our basic function this meant we had to:</p>

<ul>
<li>deploy <strong>classes AND sources</strong></li>
<li>use <strong>fully Qualified Class Names</strong></li>
<li>use value holder classes and helper methods to manipulate parameters
*</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Moving My Beers From Couchbase to MongoDB]]></title>
    <link href="http://tgrall.github.io/blog/2015/02/01/moving-my-beers-from-couchbase-to-mongodb/"/>
    <updated>2015-02-01T15:37:46+01:00</updated>
    <id>http://tgrall.github.io/blog/2015/02/01/moving-my-beers-from-couchbase-to-mongodb</id>
    <content type="html"><![CDATA[<p>Few days ago I have posted a <em>joke</em> on Twitter</p>

<blockquote class="twitter-tweet" lang="en"><p>Moving my Java from Couchbase to MongoDB <a href="http://t.co/Wnn3pXfMGi">pic.twitter.com/Wnn3pXfMGi</a></p>&mdash; Tugdual Grall (@tgrall) <a href="https://twitter.com/tgrall/status/559664540041117696">January 26, 2015</a></blockquote>


<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>So I decided to move it from a simple picture to a <em>real</em> project. Let&rsquo;s look at the two phases of this so called project:</p>

<ul>
<li>Moving the data from Couchbase to MongoDB</li>
<li>Updating the application code to use MongoDB</li>
</ul>


<p>Look at this screencast to see it in action:</p>

<p><iframe width="560" height="420" src="http://www.youtube.com/embed/Fpl74Z0HbC0?color=white&theme=light"></iframe></p>

<!-- more -->


<h2>Moving the data</h2>

<p>I have created a replication server, that uses the Couchbase XDCR protocol to get the document out and insert them into MongoDB. This server use the Couchbase CAPI Server project available <a href="https://github.com/couchbaselabs/couchbase-capi-server">here</a>.</p>

<p>This server will receive all the mutations made in the Couchbase:</p>

<ul>
<li>When a document is inserted or updated the full document is sent</li>
<li>When a document is deleted, only the medata are sent</li>
<li>The <code>replication server</code>, save the data into MongoDB (inserts and/or updates - no delete), and then return the list to Couchbase as part of the XDCR Protocol.</li>
</ul>


<p>One of the challenges is the fact Couchbase does not have the notion of &ldquo;types&rdquo; or &ldquo;collections&rdquo;. You put everything in a <em>bucket</em> and the application code knows how to deal with the data. Not necessary a problem, just a choice of implementation, but make it sometime harder than expected when you want to write tools. So here the logic that I apply in my replication server, to organize the data in multiple collections when it makes sense <em>(and when it is possible)</em>:</p>

<ul>
<li>If the JSON document does not contains a <em>type field</em>, all the documents will be saved in a single collection</li>
<li>If the JSON document contains a <em>type field</em> then a collection will be created for each type and documents will be inserted/updated in these collections</li>
<li>MongoDB does not allow attributes key to have . and $ signs, so it is necessary to change the name with alternative characters. This is done automatically during the copy of the data.</li>
</ul>


<p>All this, and more is configurable in the tool.</p>

<p>As you can see in the screencast this is straightforward.<em>(note that I have only tested very simple use cases and deployment)</em></p>

<p>You can download the tool and the source code here:</p>

<ul>
<li><a href="https://github.com/tgrall/mongodb-cb-replicator">https://github.com/tgrall/mongodb-cb-replicator</a></li>
<li>Download the <a href="http://goo.gl/WkuHBk">MongoCBReplicator.jar</a> file.</li>
</ul>


<h2>Updating the application code</h2>

<p>The next step is to use these data in an application. For this I simply use the Beer Sample Java application available on <a href="https://github.com/couchbaselabs/beersample-java">Couchbase repository</a>.</p>

<p>I just recreated the project and modified few things, to get the application up and running:</p>

<ul>
<li>Change the connection string</li>
<li>Remove the code that generate views</li>
<li>Replace set/get by MongoDB operations</li>
<li>Replace call to the views by simple queries</li>
</ul>


<p>The code of the MongoDBeer application is available here:</p>

<ul>
<li>[<a href="https://github.com/tgrall/mongodbeer">https://github.com/tgrall/mongodbeer</a>]</li>
</ul>


<p>I did not change any business logic, or added features, or even replaced the way navigation and page rendition is made. I just focused on the database access, for example :</p>

<pre><code class="java">
// Couchbase Query
View view = client.getView("beer", "by_name");
    Query query = new Query();
    query.setIncludeDocs(true).setLimit(20);
    ViewResponse result = client.query(view, query);

    ArrayList&lt;HashMap&lt;String, String&gt;&gt; beers =
      new ArrayList&lt;HashMap&lt;String, String&gt;&gt;();
    for(ViewRow row : result) {
      HashMap&lt;String, String&gt; parsedDoc = gson.fromJson(
        (String)row.getDocument(), HashMap.class);

      HashMap&lt;String, String&gt; beer = new HashMap&lt;String, String&gt;();
      beer.put("id", row.getId());
      beer.put("name", parsedDoc.get("name"));
      beer.put("brewery", parsedDoc.get("brewery_id"));
      beers.add(beer);
    }
    request.setAttribute("beers", beers);


// MongoDB Query
DBCursor cursor = db.getCollection("beer").find()
                                                   .sort( BasicDBObjectBuilder.start("name",1).get() )
                                                   .limit(20);
     ArrayList&lt;HashMap&lt;String, String&gt;&gt; beers =
             new ArrayList&lt;HashMap&lt;String, String&gt;&gt;();
     while (cursor.hasNext()) {
         DBObject row = cursor.next();
         HashMap&lt;String, String&gt; beer = new HashMap&lt;String, String&gt;();
         beer.put("id", (String)row.get("_id"));
         beer.put("name", (String)row.get("name"));
         beer.put("brewery", (String)row.get("brewery_id"));
         beers.add(beer);
     }



// Couchbase update
client.set(beerId, 0, gson.toJson(beer));

// MongoDB update
db.getCollection("beer").save(new BasicDBObject(beer));
</code></pre>

<p>I did not attend to optimize the MongoDB code,  but just to replace as few lines of code as possible.</p>

<p>Note: I have not created any index during the process. Obviously if your application have more and more data and you do intense work with it you must analyze your application/queries to see which indexes must be created.</p>

<h2>Adding new features</h2>

<p>Once you have the data into MongoDB you can do a lot more without anything more than MongoDB:</p>

<h4>Full Text Search</h4>

<p>You can create a Text index on various fields in the collection to provide advanced search capabilities to your users.</p>

<pre><code class="json">db.brewery.ensureIndex(
  {
    "name" : "text",
    "description" : "text"
  },
  {
    "weights" :
    {
      "name" : 10,
      "description" : 5
    },
    "name" : "TextIndex"
  }

);
</code></pre>

<p>Then you can query the database using the <code>$text</code> operation, for example all breweries with <em>Belgium</em> and without <em>Ale</em></p>

<pre><code class="json">db.brewery.find( { "$text" : { "$search" : "belgium -ale" }   }  , { "name" : 1  } );
{ "_id" : "daas", "name" : "Daas" }
{ "_id" : "chimay_abbaye_notre_dame_de_scourmont", "name" : "Chimay (Abbaye Notre Dame de Scourmont)" }
{ "_id" : "brasserie_de_cazeau", "name" : "Brasserie de Cazeau" }
{ "_id" : "inbev", "name" : "InBev" }
{ "_id" : "new_belgium_brewing", "name" : "New Belgium Brewing" }
{ "_id" : "palm_breweries", "name" : "Palm Breweries" }
</code></pre>

<h4>Some analytics</h4>

<p>Not sure these queries really make sense, but it is just to show that now you can leverage your documents without the need of any 3rd party tool.</p>

<p>Number of beer by category, from the most common to the less one:</p>

<pre><code class="json">db.beer.aggregate([
  {"$group" : { "_id" : "$category","count" : {"$sum" : 1 } } },
  {"$sort" : { "count" : -1 } },
  {"$project" : {   "category" : "$_id", "count" : 1, "_id" : 0 } }
]);

{ "count" : 1996, "category" : "North American Ale" }
{ "count" : 1468, "category" : null }
{ "count" : 564, "category" : "North American Lager" }
{ "count" : 441, "category" : "German Lager" }
...
...
</code></pre>

<p>Number of beer of a specific ABV by brewery, for example: top 3 breweries with the most beer with an abv greather or equals to a value, let&rsquo;s say 5:</p>

<pre><code class="json">db.beer.aggregate([
... { "$match" : { "abv" : { "$gte" : 5 }  } },
... { "$group" : { "_id" : "$brewery_id", "count" : { "$sum" : 1} }},
... { "$sort" : { "count" : -1 } },
... { "$limit" : 3 }
... ])

{ "_id" : "midnight_sun_brewing_co", "count" : 53 }
{ "_id" : "troegs_brewing", "count" : 33 }
{ "_id" : "rogue_ales", "count" : 31 }
</code></pre>

<h4>Geospatial queries</h4>

<p>The first thing to do with the data is to change the data structure to save the various data into a GeoJSON format, for this we can simply use a script into the MongoDB Shell:</p>

<pre><code class="json">&gt;mongo

use beers

db.brewery.find().forEach(
  function( doc ) {
    var loc = { type : "Point" };
    if (doc.geo &amp;&amp; doc.geo.lat &amp;&amp; doc.geo.lon) {
      loc.coordinates = [ doc.geo.lon , doc.geo.lat  ];
      db.brewery.update( { _id : doc._id } , {$set : { loc : loc } }  );
    }
  }
);

db.brewery.ensureIndex( { "loc" : "2dsphere" } );
</code></pre>

<p>This call take all the breweries and add a new attribute, name <code>loc</code> as a GeoJSON point. I could also chose to remove the old geo information using a &lsquo;$unset&rsquo;, but I did not; let&rsquo;s imagine that some API/applications are using it. This is a good example of flexible schema.</p>

<p>Now I can search for all the brewery that are at less than 30km from the Golden Gate in San Francisco: [-122.478255,37.819929]</p>

<pre><code class="json">db.brewery.find(
  { "loc" :
    { "$near" :
      { "$geometry" :
        {
          "type" : "Point",
          "coordinates" : [-122.478255,37.819929]
        },
        "$maxDistance" : 20000

      }
    }
  }
  , { name : 1 }  
)
</code></pre>

<p>You can also use Geospatial indexes and operators in the aggregation queries used above</p>

<h2>Conclusion</h2>

<p>As as said in the introduction, this week end project started as a joke on Twitter, and finished with a small blog post and Gitub repositories.</p>

<p>My goal here is not to compare the two solutions -I made my choice few months back-  but simply show how you can move from one to the other with almost no effort, not only the data but also the application code.</p>
]]></content>
  </entry>
  
</feed>
