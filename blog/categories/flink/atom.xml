<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Flink | Tug's Blog]]></title>
  <link href="http://tgrall.github.io/blog/categories/flink/atom.xml" rel="self"/>
  <link href="http://tgrall.github.io/"/>
  <updated>2019-09-03T11:24:02+02:00</updated>
  <id>http://tgrall.github.io/</id>
  <author>
    <name><![CDATA[Tug Grall]]></name>
    <email><![CDATA[tugdual@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Getting Started With Apache Flink and Mapr Streams]]></title>
    <link href="http://tgrall.github.io/blog/2016/10/17/getting-started-with-apache-flink-and-mapr-streams/"/>
    <updated>2016-10-17T10:12:10+02:00</updated>
    <id>http://tgrall.github.io/blog/2016/10/17/getting-started-with-apache-flink-and-mapr-streams</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p><a href="https://flink.apache.org/">Apache Flink</a> is an open source platform for distributed stream and batch data processing. Flink is a streaming data flow engine with several APIs to create data streams oriented application.</p>

<p>It is very common for Flink applications to use <a href="http://kafka.apache.org/">Apache Kafka</a> for data input and output.</p>

<p>This article will guide you into  the steps to use Apache Flink with <a href="https://www.mapr.com/products/mapr-streams">MapR Streams</a>. MapR Streams is a distributed messaging system for streaming event data at scale, and it’s integrated into the <a href="https://www.mapr.com/products/mapr-converged-data-platform">MapR Converged Data Platform</a>, based on the Apache Kafka API (0.9.0),
this article use the same code and approach than the <a href="http://tgrall.github.io/blog/2016/10/12/getting-started-with-apache-flink-and-kafka/">Flink and Kafka Getting Started</a>.</p>

<p><img src="http://tgrall.github.io/images/posts/flink-kafka/flink-mapr-streams.png" alt="MapR Streams and Flink" />.</p>

<!-- more -->


<h3>Prerequisites</h3>

<ul>
<li>MapR 5.2

<ul>
<li>You can use <a href="https://www.mapr.com/products/mapr-sandbox-hadoop">MapR Converged Data Platform Sandbox</a></li>
</ul>
</li>
<li>MapR Client installed on your development host

<ul>
<li><a href="http://maprdocs.mapr.com/home/AdvancedInstallation/SettingUptheClient-install-mapr-client.html">Installation and Configuration steps</a></li>
</ul>
</li>
<li>Git</li>
<li>Maven 3.x or later</li>
</ul>


<h2>Create your Flink Streaming Project</h2>

<p>The first step is to create an Java application, the easiest is to use the flink-quickstart-java archetype, that contains the core dependencies and packaging tasks. This article is similar with the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/quickstart/run_example_quickstart.html">Apache Flink Quick Start Example</a>, with a clear focus on data input and output with MapR Streams.</p>

<p>In this application we will create two jobs:</p>

<ul>
<li><code>WriteToKafka</code> : that generates random string and post them to a MapR Streams Topic using the Kafka Flink Connector and its Producer API.</li>
<li><code>ReadFromKafka</code> : that reads the same topic and print the messages in the standard output using the Kafka Flink Connector and its Consumer. API.</li>
</ul>


<p>The full project is available on GitHub:</p>

<ul>
<li><a href="https://github.com/mapr-demos/mapr-streams-flink-demo">MapR Streams Flink Demo</a></li>
</ul>


<p>Let’s create the project using Apache Maven:</p>

<pre><code>mvn archetype:generate \
      -DarchetypeGroupId=org.apache.flink\
      -DarchetypeArtifactId=flink-quickstart-java \
      -DarchetypeVersion=1.1.2 \
      -DgroupId=com.mapr.demos \
      -DartifactId=mapr-streams-flink-demo \
      -Dversion=1.0-SNAPSHOT \
      -DinteractiveMode=false 
</code></pre>

<p>Maven will create the following structure:</p>

<pre><code>$ tree mapr-streams-flink-demo/
mapr-streams-flink-demo/
├── pom.xml
└── src
    └── main
        ├── java
        │   └── com
        │       └── mapr
        │           └── demos
        │               ├── BatchJob.java
        │               ├── SocketTextStreamWordCount.java
        │               ├── StreamingJob.java
        │               └── WordCount.java
        └── resources
            └── log4j.properties
</code></pre>

<p>This project is configured to create a Jar file that contains your flink project code and also includes all dependencies needed to run it.</p>

<p>The project contains some other sample jobs, we do not need them for this article, you can either keep them to educational purposes or simply remove them from the project.</p>

<h2>Add Kafka &amp; MapR Streams Dependencies</h2>

<p>Open the <code>pom.xml</code> and add the following dependencies to your project:</p>

<p><strong>1- Add MapR Maven Repository</strong></p>

<p>In the <code>&lt;repositories&gt;</code> element add :</p>

<pre><code>   &lt;repository&gt;
     &lt;id&gt;mapr-releases&lt;/id&gt;
     &lt;url&gt;http://repository.mapr.com/maven/&lt;/url&gt;
     &lt;snapshots&gt;&lt;enabled&gt;false&lt;/enabled&gt;&lt;/snapshots&gt;
     &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt;
   &lt;/repository&gt;
</code></pre>

<p><strong>2- Add MapR Streams libraries</strong></p>

<p>In the <code>&lt;dependencies&gt;</code> element:</p>

<pre><code> &lt;dependency&gt;
   &lt;groupId&gt;com.mapr.streams&lt;/groupId&gt;
   &lt;artifactId&gt;mapr-streams&lt;/artifactId&gt;
   &lt;version&gt;5.2.0-mapr&lt;/version&gt;
 &lt;/dependency&gt;
 &lt;dependency&gt;
   &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
   &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
   &lt;version&gt;0.9.0.0-mapr-1602&lt;/version&gt;
 &lt;/dependency&gt;
</code></pre>

<p><strong>3- Add Flink Kafka Connector libraries</strong></p>

<p>As a first step, we have to add the Flink Kafka connector as a dependency so that we can use the Kafka sink. Add this to the pom.xml file in the dependencies section:</p>

<p>You must add now the Flink Kafka Connector dependency to use the Kafka sink. Add the following entry in the <code>&lt;dependencies&gt;</code> element:</p>

<pre><code> &lt;dependency&gt;
      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
      &lt;artifactId&gt;flink-connector-kafka-0.9_2.10&lt;/artifactId&gt;
      &lt;version&gt;${flink.version}&lt;/version&gt;
 &lt;/dependency&gt;
</code></pre>

<p><strong>4- Exclude Kafka Client to allow use of MapR Streams Client</strong></p>

<p>As you may know, MapR Streams uses the Kafka 0.9.0 API to produce and consume messages. So we need now to remove (exclude) tha Apache Kafka Client API to be sure that Flink use MapR Streams.</p>

<p>In the Flink Kafka Connector dependency add the following exclusion:</p>

<pre><code>  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
    &lt;artifactId&gt;flink-connector-kafka-0.9_2.10&lt;/artifactId&gt;
    &lt;version&gt;${flink.version}&lt;/version&gt;
      &lt;exclusions&gt;
        &lt;exclusion&gt;
          &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
          &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
        &lt;/exclusion&gt;
        &lt;exclusion&gt;
          &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
          &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt;
        &lt;/exclusion&gt;
      &lt;/exclusions&gt;
  &lt;/dependency&gt;
</code></pre>

<p>The Flink project is now ready to use the DataStream using the Kafka Connector so you can send and receive messages from MapR Streams.</p>

<p>Let’s now create a Stream in MapR and write some simple Flink code to use it.</p>

<h2>Create the MapR Streams and Topic</h2>

<p>A stream is a collection of topics that you can manage as a group by:</p>

<ol>
<li>Setting security policies that apply to all topics in that stream</li>
<li>Setting a default number of partitions for each new topic that is created in the stream</li>
<li>Set a time-to-live for messages in every topic in the stream</li>
</ol>


<p>You can find more information about MapR Streams concepts in the <a href="http://maprdocs.mapr.com/51/MapR_Streams/concepts.html">documentation</a>.</p>

<p>On your Mapr Cluster or Sandbox run the following commands:</p>

<pre><code>$ maprcli stream create -path /apps/application-stream -produceperm p -consumeperm p -topicperm p
$ maprcli stream topic create -path /apps/application-stream -topic flink-demo 
</code></pre>

<h3>Install and use MapR Kafka utilities</h3>

<p>Install <code>the mapr-kafka</code> package on your cluster :</p>

<pre><code>yum install mapr-kafka
</code></pre>

<p>Open two terminal windows and run the producer and consumer kafka utilities using the following commands:</p>

<p>Producer</p>

<pre><code>/opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-producer.sh --broker-list this.will.be.ignored:9092 --topic /apps/application-stream:flink-demo
</code></pre>

<p>Consumer</p>

<pre><code>/opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-consumer.sh --new-consumer --bootstrap-server this.will.be.ignored:9092 --topic /apps/application-stream:flink-demo
</code></pre>

<p>In the producer window, you can post some messages and see them in the consumer windows. We will use these tools to follow the interactions between MapR Streams and Flink.</p>

<h2>Write your Flink application</h2>

<p>Let’s now use the Flink Kafka Connector to send messages to MapR Streams and consume them.</p>

<h3>Producer</h3>

<p>The producer generates messages using the <code>SimpleStringGenerator()</code> class and send the string to the <code>/apps/application-stream:flink-demo</code> topic.</p>

<pre><code>  public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    Properties properties = new Properties();
    // properties.setProperty("bootstrap.servers", "&lt;kafka-broker&gt;:9092"); // not used by MapR Streams
    properties.setProperty("streams.buffer.max.time.ms", "200");

    DataStream&lt;String&gt; stream = env.addSource(new SimpleStringGenerator());
    stream.addSink(new FlinkKafkaProducer09&lt;&gt;("/apps/application-stream:flink-demo", new SimpleStringSchema(), properties));

    env.execute();
  }
</code></pre>

<p>The <code>SimpleStringGenerator()</code> method code is available <a href="https://github.com/mapr-demos/mapr-streams-flink-demo/blob/master/src/main/java/com/mapr/demos/WriteToKafka.java#L46-L61">here</a>.</p>

<p>The main steps are:</p>

<ul>
<li>create a new <code>StreamExecutionEnvironment</code> the basis of any Flink application</li>
<li>create a new <code>DataStream</code> in the application environment, the <code>SimpleStringGenerator</code> class implements the <code>[SourceFunction](https://ci.apache.org/projects/flink/flink-docs-release-1.1/api/java/)</code> the base interface for all streams data sources in Flink.</li>
<li>add the <code>FlinkKafkaProducer09</code> sink to the streams; since MapR Streams is based on Kafka API 0.9, it is possible to use the FlinkKafkaProducer09 class; with 2 small differences:

<ul>
<li>the broker list (first parameter) is not used since MapR Streams use the cluster location defined in the <code>/opt/mapr/conf/mapr-clusters.conf</code> class.</li>
<li>the topic name include the path and name of the MapR Stream stream in which the topic is located for example <code>/apps/application-stream:flink-demo</code></li>
</ul>
</li>
</ul>


<h3>Consumer</h3>

<p>The consumer simply reads the messages from the <code>/apps/application-stream:flink-demo</code> topic, and print them into the console.</p>

<pre><code>  public static void main(String[] args) throws Exception {
    // create execution environment
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    Properties properties = new Properties();
    // properties.setProperty("bootstrap.servers", "&lt;kafka-broker&gt;:9092"); // not used by MapR Streams
    properties.setProperty("group.id", "flink_consumer");

    DataStream&lt;String&gt; stream = env.addSource(new FlinkKafkaConsumer09&lt;&gt;(
        "/apps/application-stream:flink-demo", new SimpleStringSchema(), properties) );

    stream.map(new MapFunction&lt;String, String&gt;() {
      private static final long serialVersionUID = -6867736771747690202L;

      @Override
      public String map(String value) throws Exception {
        return "Stream Value: " + value;
      }
    }).print();

    env.execute();
  }
</code></pre>

<p>The main steps are:</p>

<ul>
<li>create a new <code>StreamExecutionEnvironment</code> the basis of any Flink application</li>
<li>create a set of properties with the consumer information, in this application we can only set the consumer <code>group.id</code>. Note that the <code>bootstrap.servers</code> property is not used by MapR Streams, so no need to set it.</li>
<li>use the <code>FlinkKafkaConsumer09</code> to get the message from the MapR Streams topic <code>/apps/application-stream:flink-demo</code></li>
</ul>


<h2>Build and Run the application</h2>

<p>Let’s run the application directly from Maven (or from your favorite IDE).</p>

<p>1- Build the project:</p>

<pre><code>$ mvn clean package
</code></pre>

<p>2- Run the Flink Producer Job</p>

<pre><code>$ mvn exec:java -Dexec.mainClass=com.mapr.demos.WriteToKafka
</code></pre>

<p>3- Run the Flink Consumer Job</p>

<pre><code>$ mvn exec:java -Dexec.mainClass=com.mapr.demos.ReadFromKafka
</code></pre>

<p>In the terminal, you should see the messages generated from the producer</p>

<p>You can now deploy and execute this job on your Flink cluster.</p>

<h2>Conclusion</h2>

<p>In this article you have learned how to use Flink with MapR Streams to write and read data streams. The key element is the configuration of the Maven Dependencies to configure the project to use MapR Streams libraries instead of Kafka ones.</p>

<p>This was originally published on the <a href="https://www.mapr.com/blog/getting-started-apache-flink-and-mapr-streams">MapR blog here</a>.</p>

<p>Learn about what Apache Flink can do and how it maintains consistency and provides flexibility in the &ldquo;<a href="https://www.mapr.com/introduction-to-apache-flink">Introduction to Apache Flink</a>&rdquo; ebook.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With Apache Flink and Kafka]]></title>
    <link href="http://tgrall.github.io/blog/2016/10/12/getting-started-with-apache-flink-and-kafka/"/>
    <updated>2016-10-12T04:54:17+02:00</updated>
    <id>http://tgrall.github.io/blog/2016/10/12/getting-started-with-apache-flink-and-kafka</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p><a href="https://flink.apache.org/">Apache Flink</a> is an open source platform for distributed stream and batch data processing. Flink is a streaming data flow engine with several APIs to create data streams oriented application.</p>

<p>It is very common for Flink applications to use <a href="http://kafka.apache.org/">Apache Kafka</a> for data input and output. This article will guide you into  the steps to use Apache Flink with Kafka.</p>

<p><img class="center" src="/images/posts/flink-kafka/flink-kafka.png" title="Flink-Kafka" ></p>

<!-- more -->


<h3>Prerequisites</h3>

<ul>
<li>Apache Kafka 0.9.x</li>
<li>Git</li>
<li>Maven 3.x or later</li>
</ul>


<h2>Create your Flink Streaming Project</h2>

<p>The first step is to create an Java application, the easiest is to use the flink-quickstart-java archetype, that contains the core dependencies and packaging tasks. This article is similar with the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/quickstart/run_example_quickstart.html">Apache Flink Quick Start Example</a>, with a clear focus on data input and output with MapR Streams.</p>

<p>In this application we will create two jobs:</p>

<ul>
<li><code>WriteToKafka</code> : that generates random string and post them to a MapR Streams Topic using the Kafka Flink Connector and its Producer API.</li>
<li><code>ReadFromKafka</code> : that reads the same topic and print the messages in the standard output using the Kafka Flink Connector and its Consumer. API.</li>
</ul>


<p>The full project is available on GitHub:</p>

<ul>
<li><a href="https://github.com/tgrall/kafka-flink-101">Flink and Kakfa Application</a></li>
</ul>


<p>Let’s create the project using Apache Maven:</p>

<pre><code class="bash">mvn archetype:generate \
      -DarchetypeGroupId=org.apache.flink\
      -DarchetypeArtifactId=flink-quickstart-java \
      -DarchetypeVersion=1.1.2 \
      -DgroupId=com.grallandco.demos \
      -DartifactId=kafka-flink-101 \
      -Dversion=1.0-SNAPSHOT \
      -DinteractiveMode=false 
</code></pre>

<p>Maven will create the following structure:</p>

<pre><code class="bash">tree kafka-flink-101/
kafka-flink-101/
├── pom.xml
└── src
    └── main
        ├── java
        │   └── com
        │       └── grallandco
        │           └── demos
        │               ├── BatchJob.java
        │               ├── SocketTextStreamWordCount.java
        │               ├── StreamingJob.java
        │               └── WordCount.java
        └── resources
            └── log4j.properties

7 directories, 6 files
</code></pre>

<p>This project is configured to create a Jar file that contains your flink project code and also includes all dependencies needed to run it.</p>

<p>The project contains some other sample jobs, we do not need them for this article, you can either keep them to educational purposes or simply remove them from the project.</p>

<h2>Add Kafka Connector</h2>

<p>Open the <code>pom.xml</code> and add the following dependencies to your project:</p>

<p>As a first step, we have to add the Flink Kafka connector as a dependency so that we can use the Kafka sink. Add this to the pom.xml file in the dependencies section:</p>

<p>You must add now the Flink Kafka Connector dependency to use the Kafka sink. Add the following entry in the <code>&lt;dependencies&gt;</code> element:</p>

<pre><code> &lt;dependency&gt;
      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
      &lt;artifactId&gt;flink-connector-kafka-0.9_2.10&lt;/artifactId&gt;
      &lt;version&gt;${flink.version}&lt;/version&gt;
 &lt;/dependency&gt;
</code></pre>

<p>The Flink project is now ready to use the DataStream using the Kafka Connector so you can send and receive messages from Apache Kafka.</p>

<h2>Install and Start Kafka</h2>

<p>Download Kafka, enter the following commands in your terminal:</p>

<pre><code class="bash">curl -O http://www.us.apache.org/dist/kafka/0.9.0.0/kafka_2.11-0.9.0.0.tgz
tar -xzf kafka_2.11-0.9.0.0.tgz
cd kafka_2.11-0.9.0.0
</code></pre>

<p>Kafka uses ZooKeeper, if you do not have Zookeeper running, you can start it using the following command:</p>

<pre><code class="bash">./bin/zookeeper-server-start.sh config/zookeeper.properties
</code></pre>

<p>Start a Kafka broker by running the following command in a new terminal:</p>

<pre><code class="bash">./bin/kafka-server-start.sh config/server.properties
</code></pre>

<p>In another terminal, run the following command to create a Kafka topic called <code>flink-demo</code>:</p>

<pre><code class="bash">./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic flink-demo
</code></pre>

<p>Use the Kafka tools to post and consume messages to the <code>flink-demo</code> topic.</p>

<p>Producer</p>

<pre><code class="bash">./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic flink-demo
</code></pre>

<p>Consumer</p>

<pre><code class="bash">./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic flink-demo --from-beginning
</code></pre>

<p>In the producer window, you can post some messages and see them in the consumer windows. We will use these tools to follow the interactions between Kafka and Flink.</p>

<h2>Write your Flink application</h2>

<p>Let’s now use the Flink Kafka Connector to send messages to Kafka and consume them.</p>

<h3>Producer</h3>

<p>The producer generates messages using the <code>SimpleStringGenerator()</code> class and send the string to the <code>flink-demo</code> topic.</p>

<pre><code>  public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    Properties properties = new Properties();
    properties.setProperty("bootstrap.servers", “localhost:9092"); 

    DataStream&lt;String&gt; stream = env.addSource(new SimpleStringGenerator());
    stream.addSink(new FlinkKafkaProducer09&lt;&gt;("flink-demo", new SimpleStringSchema(), properties));

    env.execute();
  }
</code></pre>

<p>The <code>SimpleStringGenerator()</code> method code is available <a href="https://github.com/tgrall/kafka-flink-101/blob/master/src/main/java/com/grallandco/demos/WriteToKafka.java#L45-L60">here</a>.</p>

<p>The main steps are:</p>

<ul>
<li>create a new <code>StreamExecutionEnvironment</code> the basis of any Flink application</li>
<li>create a new <code>DataStream</code> in the application environment, the <code>SimpleStringGenerator</code> class implements the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/api/java/">SourceFunction</a> the base interface for all streams data sources in Flink.</li>
<li>add the <code>FlinkKafkaProducer09</code> sink to the topic.</li>
</ul>


<h3>Consumer</h3>

<p>The consumer simply reads the messages from the <code>flink-demo</code> topic, and print them into the console.</p>

<pre><code>  public static void main(String[] args) throws Exception {
    // create execution environment
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    Properties properties = new Properties();
    properties.setProperty("bootstrap.servers", “localhost:9092");
    properties.setProperty("group.id", "flink_consumer");

    DataStream&lt;String&gt; stream = env.addSource(new FlinkKafkaConsumer09&lt;&gt;(
        "flink-demo", new SimpleStringSchema(), properties) );

    stream.map(new MapFunction&lt;String, String&gt;() {
      private static final long serialVersionUID = -6867736771747690202L;

      @Override
      public String map(String value) throws Exception {
        return "Stream Value: " + value;
      }
    }).print();

    env.execute();
  }
</code></pre>

<p>The main steps are:</p>

<ul>
<li>create a new <code>StreamExecutionEnvironment</code> the basis of any Flink application</li>
<li>create a set of properties with the consumer information, in this application we can only set the consumer <code>group.id</code>.</li>
<li>use the <code>FlinkKafkaConsumer09</code> to get the message from the topic <code>flink-demo</code></li>
</ul>


<h2>Build and Run the application</h2>

<p>Let’s run the application directly from Maven (or from your favorite IDE).</p>

<p>1- Build the project:</p>

<pre><code>$ mvn clean package
</code></pre>

<p>2- Run the Flink Producer Job</p>

<pre><code>$ mvn exec:java -Dexec.mainClass=com.mapr.demos.WriteToKafka
</code></pre>

<p>3- Run the Flink Consumer Job</p>

<pre><code>$ mvn exec:java -Dexec.mainClass=com.mapr.demos.ReadFromKafka
</code></pre>

<p>In the terminal, you should see the messages generated from the producer</p>

<p>You can now deploy and execute this job on your Flink cluster.</p>

<h2>Conclusion</h2>

<p>In this article you have learned how to use Flink with kafka to write and read data streams.</p>

<p>Learn about what Apache Flink can do and how it maintains consistency and provides flexibility in the &ldquo;<a href="https://www.mapr.com/introduction-to-apache-flink">Introduction to Apache Flink</a>&rdquo; ebook.</p>
]]></content>
  </entry>
  
</feed>
