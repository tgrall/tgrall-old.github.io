<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Bigdata | Tug's Blog]]></title>
  <link href="http://tgrall.github.io/blog/categories/bigdata/atom.xml" rel="self"/>
  <link href="http://tgrall.github.io/"/>
  <updated>2019-09-19T05:47:46+02:00</updated>
  <id>http://tgrall.github.io/</id>
  <author>
    <name><![CDATA[Tug Grall]]></name>
    <email><![CDATA[tugdual@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Streaming Analytics in a Digitally Industrialized World]]></title>
    <link href="http://tgrall.github.io/blog/2016/09/26/streaming-analytics-in-a-digitally-industrialized-world/"/>
    <updated>2016-09-26T15:30:20+02:00</updated>
    <id>http://tgrall.github.io/blog/2016/09/26/streaming-analytics-in-a-digitally-industrialized-world</id>
    <content type="html"><![CDATA[<p>Get an introduction to streaming analytics, which allows you real-time insight from captured events and big data. There are applications across industries, from finance to wine making, though there are two primary challenges to be addressed.</p>

<p>Did you know that a plane flying from Texas to London can generate 30 million data points per flight? As Jim Daily of GE Aviation notes, that equals <a href="https://www.ge.com/digital/blog/industrial-iot-improving-airline-economics">10 billion data points</a> in one year. And we’re talking about one plane alone. So you can understand why <a href="http://cloudblog.ericsson.com/cloud-scalability-combined-with-speed-inside-ges-cloud-transformation">another top GE executive recently told Ericsson Business Review</a> that &ldquo;Cloud is the future of IT,&rdquo; with a focus on supporting challenging applications in industries such as aviation and energy.</p>

<!-- more -->


<h3>The benefits of big data</h3>

<p>Today, thanks to modern big data platforms, many companies are able to take advantage of the same methods as industry giant GE to store, process, and analyze massive amounts of data. This means that you can also capture core business data, such as that coming from a CRM system, or traffic sensors, or say jet engines, and associate it to other data such as social, application, blog or industrial data. Ultimately, this will result in greater data insights and can enable things like better customer segmentation and prediction.</p>

<p>Sounds great, right? There’s a catch. The challenge is that these data are processed in batch mode, meaning that you have to wait a few hours or even days to access relevant KPI’s and insights. Not only is there a delay, but analysis is based on data that’s out of date – even if only by a few hours.</p>

<h3>Analysis of big event streams</h3>

<p>That’s where streaming analytics comes in. Streaming analytics is the <a href="http://searchcloudapplications.techtarget.com/opinion/Streaming-analytics-lets-you-view-the-past-to-see-the-future?utm_medium=EM&amp;asrc=EM_NLN_58517129&amp;utm_campaign=20160606_Seeing%20the%20future...with%20the%20past?_fchurchville&amp;utm_source=NLN&amp;track=NL-1839&amp;ad=908120&amp;src=908120">analysis of large event streams</a>, which is data that is in constant movement. These streams can include actions that can be incredibly small, say one click, yet result in an explosion of data. The benefit is that you can capture events and data as they happen, delivering value to the enterprise in near real time.</p>

<p>What does streaming analytics look like regarding the previously mentioned CRM system? It means that an enterprise can get immediate feedback on something like a specific marketing campaign, website update, or product alteration. When a user clicks within one of these realms and an order is immediately processed, that information is pushed out in real time to various tools, allowing the enterprise to adjust its customer interaction.</p>

<h3>Streaming analytics in practice</h3>

<p>Where else is streaming analytics applicable? Think about the benefits of fraud detection in the financial sector, or the growth of sensors in manufacturing, or collection of data from large scale machines. One example is where Ericsson has collaborated with <a href="http://cloudblog.ericsson.com/can-the-networked-society-and-iot-make-better-wine">vintners in Germany, using IoT to improve traditional harvesting methods</a> through greater precision and speed of response to outside factors. In these streaming analytics scenarios, the key is “<a href="https://www.mapr.com/blog/lets-get-real-acting-data-real-time">High-Frequency Decisioning,</a>” where you move from knowing to doing in real-time synergy. Time to action is compressed, resulting in dramatic results like achieving greater user satisfaction, higher revenue, or reduced risk.</p>

<h3>Two primary challenges with streaming analytics</h3>

<p>Streaming analytics is not without challenges. Systems must capture events in near real time, at scale, and in a distributed fashion. And of course, events must be stored and processed in real time. Since big data is generated one event at a time, you need to have an incredibly powerful storage and processing layer, which can provide deep analytics and rich features like machine learning systems.</p>

<p>To take on the first challenge, new messaging systems like <a href="http://kafka.apache.org">Apache Kafka</a> and <a href="https://www.mapr.com/products/mapr-streams">MapR Streams</a> provide a common API for developers to publish and subscribe to any event. And to process and store data in real-time, one of the most efficient methods would be to use a <a href="https://www.mapr.com/products/mapr-fs">distributed file system</a> and <a href="https://www.mapr.com/products/mapr-db-in-hadoop-nosql">NoSQL databases</a>. This provides horizontal scalability and flexibility. A storage system must also process and do analytics calculations in real time in a distributed manner.</p>

<p>Tools such as <a href="https://www.mapr.com/products/apache-spark">Apache Spark</a> and <a href="http://flink.apache.org">Flink</a> come to mind, which provide rich analytical functions that can be integrated with any tool, including real time alerting systems. It is also interesting to mention that all the events, data, that are now stored in real time continue to be accessible with traditional analytics tools; thanks to the distributed, and scalable distributed SQL Engines provided in modern big data platforms.</p>

<p>This was originally published on the <a href="http://cloudblog.ericsson.com/streaming-analytics-of-big-data-in-real-time">Ericsson Cloud blog here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Convert a CSV File to Apache Parquet With Drill]]></title>
    <link href="http://tgrall.github.io/blog/2015/08/17/convert-csv-file-to-apache-parquet-dot-dot-dot-with-drill/"/>
    <updated>2015-08-17T14:07:00+02:00</updated>
    <id>http://tgrall.github.io/blog/2015/08/17/convert-csv-file-to-apache-parquet-dot-dot-dot-with-drill</id>
    <content type="html"><![CDATA[<p>A very common use case when working with Hadoop is to store and query simple files (CSV, TSV, &hellip;); then to get better performance and efficient storage convert these files into more efficient format, for example <a href="https://parquet.apache.org/">Apache Parquet</a>.</p>

<p><a href="https://parquet.apache.org/">Apache Parquet</a> is a <a href="https://en.wikipedia.org/wiki/Column-oriented_DBMS">columnar storage format</a> available to any project in the Hadoop ecosystem. Apache Parquet has the following characteristics:</p>

<ul>
<li>Self-describing</li>
<li>Columnar format</li>
<li>Language-independent</li>
</ul>


<p>Let&rsquo;s take a concrete example, you can find many interesting Open Data sources that distribute data as CSV files- or equivalent format-. So you can store them into your distributed file system and use them in your applications/jobs/analytics queries. This is not the most efficient way especially when we know that these data won&rsquo;t move that often. So instead of simply storing the CSV let&rsquo;s copy this information into Parquet.</p>

<h3>How to convert CSV files into Parquet files?</h3>

<p>You can use code to achieve this, as you can see in the <a href="https://github.com/Parquet/parquet-compatibility/blob/master/parquet-compat/src/test/java/parquet/compat/test/ConvertUtils.java">ConvertUtils</a> sample/test class. You can use a simpler way with Apache Drill. Drill allows you save the result of a query as Parquet files.</p>

<p>The following steps will show you how to do convert a simple CSV into a Parquet file using Drill.</p>

<!-- more -->


<h4>Prerequisites</h4>

<ul>
<li>Apache Drill : Standalone <a href="https://drill.apache.org/">Apache Drill</a> or using <a href="https://www.mapr.com/products/mapr-sandbox-hadoop/download-sandbox-drill">Apache Drill Sandbox from MapR</a></li>
<li>Some CSV Files: for example <a href="http://www.flysfo.com/media/facts-statistics/air-traffic-statistics">Passenger Dataset from SFO Air Traffic Statistics</a></li>
</ul>


<h4>Querying the CSV file</h4>

<p>Let&rsquo;s execute a basic query:</p>

<pre><code class="sql">SELECT *
FROM dfs.`/opendata/Passenger/SFO_Passenger_Data/MonthlyPassengerData_200507_to_201503.csv`
LIMIT 5;

["200507","ATA Airlines","TZ","ATA Airlines","TZ","Domestic","US","Deplaned","Low Fare","Terminal 1","B","27271\r"]
...
...
</code></pre>

<p>As you can see, by default Drill processes each line as an array of columns, all values being simple String. So if you need to do some operations with these values (projection or where clause) you must use the column index, and cast the value to the proper type. You can see a simple example below:</p>

<pre><code>SELECT
columns[0] as `DATE`,
columns[1] as `AIRLINE`,
CAST(columns[11] AS DOUBLE) as `PASSENGER_COUNT`
FROM dfs.`/opendata/Passenger/SFO_Passenger_Data/*.csv`
WHERE CAST(columns[11] AS DOUBLE) &lt; 5
;

+---------+-----------------------------------+------------------+
|  DATE   |              AIRLINE              | PASSENGER_COUNT  |
+---------+-----------------------------------+------------------+
| 200610  | United Airlines - Pre 07/01/2013  | 2.0              |
...
...
</code></pre>

<p>We are now ready to create our Parquet files using the &ldquo;Create Table As Select&rdquo; (aka <a href="http://drill.apache.org/docs/create-table-as-ctas-command/">CTAS</a>)</p>

<pre><code class="sql">alter session set `store.format`='parquet';


CREATE TABLE dfs.tmp.`/stats/airport_data/` AS
SELECT
CAST(SUBSTR(columns[0],1,4) AS INT)  `YEAR`,
CAST(SUBSTR(columns[0],5,2) AS INT) `MONTH`,
columns[1] as `AIRLINE`,
columns[2] as `IATA_CODE`,
columns[3] as `AIRLINE_2`,
columns[4] as `IATA_CODE_2`,
columns[5] as `GEO_SUMMARY`,
columns[6] as `GEO_REGION`,
columns[7] as `ACTIVITY_CODE`,
columns[8] as `PRICE_CODE`,
columns[9] as `TERMINAL`,
columns[10] as `BOARDING_AREA`,
CAST(columns[11] AS DOUBLE) as `PASSENGER_COUNT`
FROM dfs.`/opendata/Passenger/SFO_Passenger_Data/*.csv`
</code></pre>

<p>That&rsquo;s it! You have now a Parquet file, a single file in our case since our dataset is really small. Apache Drill will create multiples files for the tables depending of the size and configuration your environment.</p>

<p>I invite you to read this Chapter in the Apache Drill documentation to learn more about <a href="https://drill.apache.org/docs/parquet-format/">Drill and Parquet</a>.</p>

<h3>Query Parquet Files</h3>

<p>Now that you have created your Parquet files you can use them in any of your Hadoop processes, but you can also use them in Drill, as follow:</p>

<pre><code>
SELECT *
FROM dfs.tmp.`/stats/airport_data/*`
</code></pre>

<h2>Conclusion</h2>

<p>In this article you have learned how to convert a CSV file using an Apache Drill query.</p>

<p>You can do that with any source supported by Drill, for example from JSON to Parquet, or even a complex join query between multiple data sources. You can also chose a different output format for example JSON, or a CSV.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Big Data... Is Hadoop the Good Way to Start?]]></title>
    <link href="http://tgrall.github.io/blog/2014/11/25/big-data-dot-dot-dot-is-hadoop-the-good-way-to-start/"/>
    <updated>2014-11-25T07:27:36+01:00</updated>
    <id>http://tgrall.github.io/blog/2014/11/25/big-data-dot-dot-dot-is-hadoop-the-good-way-to-start</id>
    <content type="html"><![CDATA[<p>In the past 2 years, I have met many developers, architects that are working on “big data” projects. This sounds amazing, but quite often the truth is not that amazing.</p>

<h4>TL;TR</h4>

<p>You believe that you have a big data project?</p>

<ul>
<li>Do not start with the installation of an Hadoop Cluster &ndash; the &ldquo;<em>how</em>&rdquo;</li>
<li>Start to talk to business people to understand their problem &ndash; the &ldquo;<em>why</em>&rdquo;</li>
<li>Understand the data you must process</li>
<li>Look at the volume &ndash; very often it is not &ldquo;that&rdquo; big</li>
<li>Then implement it, and take a simple approach, for example start with MongoDB + Apache Spark</li>
</ul>


<p><img class="center center <a" src="href="http://cdn.meme.am/instances/500x/47510205.jpg">http://cdn.meme.am/instances/500x/47510205.jpg</a>&#8221; width=&#8221;320&#8221; title=&#8221;&lsquo;Big Data&rsquo;&#8221; ></p>

<!-- more -->


<h2>The infamous &ldquo;big data project&rdquo;</h2>

<p>A typical discussion would look like:</p>

<p>Me: <em>“Can you tell me more about this project, what do you do with your data?”</em></p>

<p>Mr. Big Bytes: <em>“Sure, we have a 40 nodes Hadoop cluster&hellip;&ldquo;</em></p>

<p>Me: <em>“This is cool but which type of data do you store, and what is the use case, business value?&ldquo;</em></p>

<p>Mr. Big Bytes: <em>“We store, all the logs of our applications, we have hundreds of gigabits…&#8221;</em></p>

<p>After a long blank: <em>“We have not yet started to analyze these data. For now it is jut  &lsquo;us, the IT team,&rsquo; we store the data, like that soon we will be able to do interesting things with them&#8221;</em></p>

<p>You can meet the same person few months later; the cluster is still sitting here, with no activity on it. I even met some consultants telling me they received calls from their customer asking the following:</p>

<p><em>“Hmmm, we have an Hadoop cluster installed, can you help us to find what to do with it?&ldquo;</em></p>

<p><em>Wrong! That is wrong!!!!!</em> This means that the IT Team has spent lot of time for nothing, at least for the business; and I am not even sure the team has learned something technically.</p>

<h3>Start with the &ldquo;Why&rdquo; not with the &ldquo;How&rdquo;!</h3>

<p>The solution to this, could be obvious, start your “big data project” answering the “why/what” questions first! The “how”, the implementation, will come later.</p>

<p>I am sure that most of the enterprises will benefit of a so called “big data project”, but it is really important to understand the problems first. <em>And these problems are not technical…</em> at least at the beginning. So you must spend time with the business persons to understand what could help them. Let&rsquo;s take some examples.</p>

<p>You are working in a bank or insurance, business people will be more than happy to predict when/why customer will be leaving the company by doing some <em>churn analysis</em>; or it will be nice to be able to see when it makes lot of sense <em>to sell new contracts</em>, service to existing customers.</p>

<p>You are working in retail/commerce, your business will be happy to see if they can <u>adjust the price</u> to the market, or <u>provide precise recommendations</u> to a user from an analysis of other customer behavior.</p>

<p>We can find many other examples. But as you can see we are not talking about technology, just business and possible benefits. In fact nothing new, compare with the applications you are building, you need first to have some requirements/ideas to build a product. Here we just need to have some &ldquo;data input&rdquo; to see <u>how we can enrich the data</u> with some business value.</p>

<p>Once you have started to ask all these questions you will start to see some input, and possible processing around them:</p>

<ul>
<li>You are an insurance, you customers has no contact with your representative, or the customer satisfaction is medium/bad; you start to see some customer name in quotes coming from price comparison website…. hmm you can guess that they are looking for a new insurance.</li>
<li>Still in the insurance, when your customer are close to the requirement age, or has teenagers learning how to drive, moving to college, you know that you have some opportunity to sell new contract, or adapt existing ones to the new needs</li>
<li>In retail, you may want to look to all customers and what they have ordered, and based on this be able to recommend some products to a customer that &ldquo;looks&rdquo; the same.</li>
<li>Another very common use case these days, you want to do some sentiment analysis of social networks to see how your brand is perceived by your community</li>
</ul>


<p>As you can see now, we can start to think about the data we have to use and the type of processing we have to do on them.</p>

<h3>Let&rsquo;s now talk about the &ldquo;How&rdquo;</h3>

<p>Now that you have a better idea about what you want to do, it does not mean that you should dive into a large cluster installation.</p>

<p>Before that, you should continue to analyze the data:</p>

<ul>
<li>What is the structure of the data that I have to analyze?</li>
<li>How big is my dataset?</li>
<li>How much data I have to ingest on a period of time (minute, hour, day, &hellip;)</li>
</ul>


<p>All these questions will help you to understand better your application. This is where it is often interesting too, and we realize that for most of us the &ldquo;big data&rdquo; is not that big!</p>

<p>I was working the other day with a Telco company in Belgium, and I was talking about possible new project. I simply said:</p>

<ul>
<li>Belgium is what, 11+ millions of people</li>
<li>If you store a 50kb object for each person this represent:</li>
<li>Your full dataset will be 524Gb, yes not even a Terabyte!</li>
</ul>


<p>Do you need a large Hadoop cluster to store and process this? You can use it, but you do not need to! You can find something smaller, and easier to start with.</p>

<p>Any database will do the job, starting with MongoDB. I think it is really interesting to start this project with a MongoDB cluster, not only because it will allow you to scale out as much as you need, but also because you will leverage the flexibility of the document model. This will allow you to store any type of data, and easily adapt the structure to the new data, or requirements.</p>

<p>Storing the data is only one part of the equation. The other part is how you achieve the data processing. Lately I am playing a lot with <a href="https://spark.apache.org/">Apache Spark</a>. Spark provides a very powerful engine for large scale data processing, and it is a lot simpler than Map Reduce jobs. In addition to this, you can run Spark without Hadoop! This means you can connect you Spark to your MongoDB, with the <a href="http://docs.mongodb.org/ecosystem/tools/hadoop/">MongoDB Hadoop Connector</a> and other data sources and directly execute job on your main database.</p>

<p>What I like also about this approach, you can when you dataset starts to grow, and it become harder to process all the data on your operational database, you can easily add Hadoop; and keep most of your data processing layer intact, and only change the data sources information. In this case you will connect MongoDB and Hadoop to get/push the data into HDFS, once again using the MongoDB Hadoop Connector.</p>

<h3>Conclusion</h3>

<p>Too many times, projects are driven by technology instead of focusing on the business value. This is particularly true around big data projects. So be sure you start by understanding the business problem, and find the data that could help to solve it.</p>

<p>Once you have the business problem and the data, select the good technology, that could be very simple, simple files and python scripts, or more often a database like MongoDB with a data processing layer like Spark. And start to move to Hadoop when it is really mandatory&hellip; a very, very, very, large dataset.</p>
]]></content>
  </entry>
  
</feed>
