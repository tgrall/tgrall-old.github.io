<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Spark | Tug's Blog]]></title>
  <link href="http://tgrall.github.io/blog/categories/spark/atom.xml" rel="self"/>
  <link href="http://tgrall.github.io/"/>
  <updated>2020-05-16T18:24:59+02:00</updated>
  <id>http://tgrall.github.io/</id>
  <author>
    <name><![CDATA[Tug Grall]]></name>
    <email><![CDATA[tugdual@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setting Up Spark Dynamic Allocation on MapR]]></title>
    <link href="http://tgrall.github.io/blog/2016/09/01/setting-up-spark-dynamic-allocation-on-mapr/"/>
    <updated>2016-09-01T11:15:57+02:00</updated>
    <id>http://tgrall.github.io/blog/2016/09/01/setting-up-spark-dynamic-allocation-on-mapr</id>
    <content type="html"><![CDATA[<p>Apache Spark can use various cluster manager to execute application (Stand Alone, YARN, Apache Mesos). When you install Apache Spark on MapR you can submit application in a Stand Alone mode or using YARN.</p>

<p>This article focuses on YARN and Dynamic Allocation, a feature that lets Spark add or remove executors dynamically based on the workload. You can find more information about this feature in this presentation from Databricks:</p>

<ul>
<li><a href="http://www.slideshare.net/databricks/dynamic-allocation-in-spark">Dynamic Allocation in Spark</a></li>
</ul>


<p>Let’s see how to configure Spark and YARN to use dynamic allocation (that is disabled by default).</p>

<!-- more -->


<h4>Prerequisites</h4>

<ul>
<li>MapR Converged Data Platform Cluster</li>
<li>Apache Spark for MapR installed</li>
</ul>


<p>This example has been described for MapR 5.2 with Apache Spark 1.6.1, you just need to adapt the version to your environment.</p>

<h3>Enabling Dynamic Allocation in Apache Spark</h3>

<p>The first thing to do is to enable Dynamic Allocation in Spark, for this you need to edit the spark configuration file on each Spark node.</p>

<pre><code>/opt/mapr/spark/spark-1.6.1/conf/spark-defaults.conf
</code></pre>

<p>and add the following entries:</p>

<pre><code>spark.dynamicAllocation.enabled = true
spark.shuffle.service.enabled = true
spark.dynamicAllocation.minExecutors = 5 
spark.executor.instances = 0
</code></pre>

<p>You can find additional configuration options in the <a href="http://spark.apache.org/docs/1.6.1/configuration.html#dynamic-allocation">Apache Spark Documentation</a>.</p>

<h3>Enabling Spark External Shuffle for YARN</h3>

<p>You have now to edit YARN configuration to add information about Spark Shuffle Service, edit the following file, on each YARN node:</p>

<pre><code>/opt/mapr/hadoop/hadoop-2.7.0/etc/hadoop/yarn-site.xml
</code></pre>

<p>add these properties:</p>

<pre><code>  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce_shuffle,mapr_direct_shuffle,spark_shuffle&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt;
    &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;
  &lt;/property&gt;
</code></pre>

<h4>Add Spark Shuffle to YARN classpath</h4>

<p>Spark Shuffle service must be added to the YARN classpath. The jar is located in the spark distribution:</p>

<pre><code>/opt/mapr/spark/spark-1.6.1/lib/spark-1.6.1-mapr-1605-yarn-shuffle.jar
</code></pre>

<p>To achieve this add the jar in the following folder on each node:</p>

<pre><code>/opt/mapr/hadoop/hadoop-2.7.0/share/hadoop/yarn/lib
</code></pre>

<p>You can either copyy the file or create a symlink:</p>

<pre><code>$ ln -s /opt/mapr/spark/spark-1.6.1/lib/spark-1.6.1-mapr-1605-yarn-shuffle.jar /opt/mapr/hadoop/hadoop-2.7.0/share/hadoop/yarn/lib
</code></pre>

<h4>Restart YARN</h4>

<p>Since you have changed the YARN configuration <em>you must restart your node managers</em> using the following command:</p>

<pre><code>$ maprcli node services -name nodemanager -action restart -nodes [list of nodes]
</code></pre>

<h3>Submitting a Spark Job</h3>

<p>Your MapR Cluster is not ready to use Spark dynamic allocation, this means that when you submit a job you do not need to specify any resource configuration, for example:</p>

<pre><code>/opt/mapr/spark/spark-1.6.1/bin/spark-submit \
  --class com.mapr.demo.WordCountSorted \
  --master yarn \
  ~/spark-examples-1.0-SNAPSHOT.jar \
  /mapr/my.cluster.com/input/4gb_txt_file.txt \
  /mapr/my.cluster.com/user/mapr/output/
</code></pre>

<p>note that you can still specify the resources, but in this case the dynamic allocation will not be used for this specific job, for example:</p>

<pre><code>/opt/mapr/spark/spark-1.6.1/bin/spark-submit \
  --class com.mapr.demo.WordCountSorted \
  --master yarn \
  --num-executors 3
  --executor-memory 1G \
  ~/spark-examples-1.0-SNAPSHOT.jar \
  /mapr/my.cluster.com/input/4gb_txt_file.txt \
  /mapr/my.cluster.com/user/mapr/output/
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Big Data... Is Hadoop the Good Way to Start?]]></title>
    <link href="http://tgrall.github.io/blog/2014/11/25/big-data-dot-dot-dot-is-hadoop-the-good-way-to-start/"/>
    <updated>2014-11-25T07:27:36+01:00</updated>
    <id>http://tgrall.github.io/blog/2014/11/25/big-data-dot-dot-dot-is-hadoop-the-good-way-to-start</id>
    <content type="html"><![CDATA[<p>In the past 2 years, I have met many developers, architects that are working on “big data” projects. This sounds amazing, but quite often the truth is not that amazing.</p>

<h4>TL;TR</h4>

<p>You believe that you have a big data project?</p>

<ul>
<li>Do not start with the installation of an Hadoop Cluster &ndash; the &ldquo;<em>how</em>&rdquo;</li>
<li>Start to talk to business people to understand their problem &ndash; the &ldquo;<em>why</em>&rdquo;</li>
<li>Understand the data you must process</li>
<li>Look at the volume &ndash; very often it is not &ldquo;that&rdquo; big</li>
<li>Then implement it, and take a simple approach, for example start with MongoDB + Apache Spark</li>
</ul>


<p><img class="center center <a" src="href="http://cdn.meme.am/instances/500x/47510205.jpg">http://cdn.meme.am/instances/500x/47510205.jpg</a>&#8221; width=&#8221;320&#8221; title=&#8221;&lsquo;Big Data&rsquo;&#8221; ></p>

<!-- more -->


<h2>The infamous &ldquo;big data project&rdquo;</h2>

<p>A typical discussion would look like:</p>

<p>Me: <em>“Can you tell me more about this project, what do you do with your data?”</em></p>

<p>Mr. Big Bytes: <em>“Sure, we have a 40 nodes Hadoop cluster&hellip;&ldquo;</em></p>

<p>Me: <em>“This is cool but which type of data do you store, and what is the use case, business value?&ldquo;</em></p>

<p>Mr. Big Bytes: <em>“We store, all the logs of our applications, we have hundreds of gigabits…&#8221;</em></p>

<p>After a long blank: <em>“We have not yet started to analyze these data. For now it is jut  &lsquo;us, the IT team,&rsquo; we store the data, like that soon we will be able to do interesting things with them&#8221;</em></p>

<p>You can meet the same person few months later; the cluster is still sitting here, with no activity on it. I even met some consultants telling me they received calls from their customer asking the following:</p>

<p><em>“Hmmm, we have an Hadoop cluster installed, can you help us to find what to do with it?&ldquo;</em></p>

<p><em>Wrong! That is wrong!!!!!</em> This means that the IT Team has spent lot of time for nothing, at least for the business; and I am not even sure the team has learned something technically.</p>

<h3>Start with the &ldquo;Why&rdquo; not with the &ldquo;How&rdquo;!</h3>

<p>The solution to this, could be obvious, start your “big data project” answering the “why/what” questions first! The “how”, the implementation, will come later.</p>

<p>I am sure that most of the enterprises will benefit of a so called “big data project”, but it is really important to understand the problems first. <em>And these problems are not technical…</em> at least at the beginning. So you must spend time with the business persons to understand what could help them. Let&rsquo;s take some examples.</p>

<p>You are working in a bank or insurance, business people will be more than happy to predict when/why customer will be leaving the company by doing some <em>churn analysis</em>; or it will be nice to be able to see when it makes lot of sense <em>to sell new contracts</em>, service to existing customers.</p>

<p>You are working in retail/commerce, your business will be happy to see if they can <u>adjust the price</u> to the market, or <u>provide precise recommendations</u> to a user from an analysis of other customer behavior.</p>

<p>We can find many other examples. But as you can see we are not talking about technology, just business and possible benefits. In fact nothing new, compare with the applications you are building, you need first to have some requirements/ideas to build a product. Here we just need to have some &ldquo;data input&rdquo; to see <u>how we can enrich the data</u> with some business value.</p>

<p>Once you have started to ask all these questions you will start to see some input, and possible processing around them:</p>

<ul>
<li>You are an insurance, you customers has no contact with your representative, or the customer satisfaction is medium/bad; you start to see some customer name in quotes coming from price comparison website…. hmm you can guess that they are looking for a new insurance.</li>
<li>Still in the insurance, when your customer are close to the requirement age, or has teenagers learning how to drive, moving to college, you know that you have some opportunity to sell new contract, or adapt existing ones to the new needs</li>
<li>In retail, you may want to look to all customers and what they have ordered, and based on this be able to recommend some products to a customer that &ldquo;looks&rdquo; the same.</li>
<li>Another very common use case these days, you want to do some sentiment analysis of social networks to see how your brand is perceived by your community</li>
</ul>


<p>As you can see now, we can start to think about the data we have to use and the type of processing we have to do on them.</p>

<h3>Let&rsquo;s now talk about the &ldquo;How&rdquo;</h3>

<p>Now that you have a better idea about what you want to do, it does not mean that you should dive into a large cluster installation.</p>

<p>Before that, you should continue to analyze the data:</p>

<ul>
<li>What is the structure of the data that I have to analyze?</li>
<li>How big is my dataset?</li>
<li>How much data I have to ingest on a period of time (minute, hour, day, &hellip;)</li>
</ul>


<p>All these questions will help you to understand better your application. This is where it is often interesting too, and we realize that for most of us the &ldquo;big data&rdquo; is not that big!</p>

<p>I was working the other day with a Telco company in Belgium, and I was talking about possible new project. I simply said:</p>

<ul>
<li>Belgium is what, 11+ millions of people</li>
<li>If you store a 50kb object for each person this represent:</li>
<li>Your full dataset will be 524Gb, yes not even a Terabyte!</li>
</ul>


<p>Do you need a large Hadoop cluster to store and process this? You can use it, but you do not need to! You can find something smaller, and easier to start with.</p>

<p>Any database will do the job, starting with MongoDB. I think it is really interesting to start this project with a MongoDB cluster, not only because it will allow you to scale out as much as you need, but also because you will leverage the flexibility of the document model. This will allow you to store any type of data, and easily adapt the structure to the new data, or requirements.</p>

<p>Storing the data is only one part of the equation. The other part is how you achieve the data processing. Lately I am playing a lot with <a href="https://spark.apache.org/">Apache Spark</a>. Spark provides a very powerful engine for large scale data processing, and it is a lot simpler than Map Reduce jobs. In addition to this, you can run Spark without Hadoop! This means you can connect you Spark to your MongoDB, with the <a href="http://docs.mongodb.org/ecosystem/tools/hadoop/">MongoDB Hadoop Connector</a> and other data sources and directly execute job on your main database.</p>

<p>What I like also about this approach, you can when you dataset starts to grow, and it become harder to process all the data on your operational database, you can easily add Hadoop; and keep most of your data processing layer intact, and only change the data sources information. In this case you will connect MongoDB and Hadoop to get/push the data into HDFS, once again using the MongoDB Hadoop Connector.</p>

<h3>Conclusion</h3>

<p>Too many times, projects are driven by technology instead of focusing on the business value. This is particularly true around big data projects. So be sure you start by understanding the business problem, and find the data that could help to solve it.</p>

<p>Once you have the business problem and the data, select the good technology, that could be very simple, simple files and python scripts, or more often a database like MongoDB with a data processing layer like Spark. And start to move to Hadoop when it is really mandatory&hellip; a very, very, very, large dataset.</p>
]]></content>
  </entry>
  
</feed>
