<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Cloud | Tug's Blog]]></title>
  <link href="http://tgrall.github.io/blog/categories/cloud/atom.xml" rel="self"/>
  <link href="http://tgrall.github.io/"/>
  <updated>2020-05-16T18:24:59+02:00</updated>
  <id>http://tgrall.github.io/</id>
  <author>
    <name><![CDATA[Tug Grall]]></name>
    <email><![CDATA[tugdual@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Redis Rolling Upgrade on Pivotal Cloud Foundry (PCF)]]></title>
    <link href="http://tgrall.github.io/blog/2019/09/19/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/"/>
    <updated>2019-09-19T05:05:23+02:00</updated>
    <id>http://tgrall.github.io/blog/2019/09/19/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf</id>
    <content type="html"><![CDATA[<h3>Introduction</h3>

<p>In this article, I will show you how to update Redis Enterprise on PCF and see how Redis Enterprise cluster will guarantee a service continuity using out of the box failover.</p>

<p>If you need a Cloud Foundry application that calls Redis automatically you can use this <a href="https://github.com/tgrall/simple-redis-spring-demo-pcf">project simple-redis-spring-demo-pcf</a>.</p>

<p>For this article, I will upgrade <a href="https://docs.pivotal.io/partners/redis-labs-enterprise-pack/index.html">Redis Enterprise for PCF</a> from the version v5.4.2400147 to the latest version, currently v5.4.40700169.</p>

<!--more -->


<p><strong>Prerequisites</strong></p>

<ul>
<li>Pivotal Cloud Foundry up &amp; running

<ul>
<li>Administrator access to Ops Manager and Apps Manager</li>
</ul>
</li>
<li>One of more Redis databases running on PCF

<ul>
<li>My environment has2 databases in version v5.4.2400147</li>
<li>One wit replication (<code>db:4</code>) another one without replication (<code>db:5</code>)</li>
</ul>
</li>
</ul>


<h3>Initial Environment</h3>

<p>Let&rsquo;s take a look to the environment before the update; for this you can access the Redis Enterprise Cluster Management Console:</p>

<ul>
<li><a href="https://">https://</a>[Cluster Management Console Subdomain].[System Domain]</li>
<li>for example <a href="https://console-redis.sys.my-domain.cf-app.com">https://console-redis.sys.my-domain.cf-app.com</a> .</li>
</ul>


<blockquote><p>Do not use this to create/delete a database, you must use Cloud Foundry to do it. (<code>cf</code> command or UI)</p></blockquote>

<p>In the Web console, go to &ldquo;Cluster&rdquo; then &ldquo;Configuration&rdquo;, you can see the version of Redis Labs Enterprise Cluster (5.4.0-24), and Redis (5.0.2) versions.</p>

<p><img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/img-001-webui-cluster-version.png"></p>

<p>You can also use the <code>rladmin</code> command line to achieve this.</p>

<p><strong>Checking Redis cluster using the command line</strong></p>

<p>SSH to your Ops Manager and, <code>bosh ssh</code> to one of the Redis cluster VMs.</p>

<p>When I run the <code>bosh vms</code> command on my environment I can see the following VMs related to my Redis deployment:</p>

<p><img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/img-002-redis-vms-list.png"></p>

<p>The deployment is made of 5 VMs:</p>

<ul>
<li>the 3 first VMs are the Redis Nodes</li>
<li>the 2 others are related to the PCF integration (Registrar and Service Broker)</li>
</ul>


<p>We can look in more details into the role of each VMS in the cluster, for this I will <code>bosh ssh</code> into one of the nodes:</p>

<pre><code>$ bosh -d redis-enterprise-[your-deployment-id] ssh redis-enterprise-node/[your-vm-id]
</code></pre>

<p>Once connected use the <code>sudo rladmin status</code> to look at the Redis cluster deployed on PCF.</p>

<p><img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/img-003-rladmin-view.png"></p>

<p>In this cluster you see:</p>

<ul>
<li>in the <strong><em>Cluster Nodes</em></strong> section that we have 3 nodes in version 5.4.0-24</li>
<li>in the <strong><em>Databases</em></strong> section that we have 2 database instances, the name is generated by Cloud Foundry. In this environment, the <code>db:4</code> is replicated with shards on <code>node:1</code> (master) and <code>node:2</code> (slave/replica), while <code>db:5</code> is not replicated.</li>
</ul>


<p>Let&rsquo;s now see the Redis version of the databases using:</p>

<ul>
<li><code>sudo rladmin status databases extra redis_version</code></li>
</ul>


<p>As expected the version if 5.0.2, the same value that you have seen in the Web console.</p>

<h2>Installing the latest version of Redis Enterprise for PCF</h2>

<p>Once the latest release of Redis Enterprise on PCF is imported, the upgrade is easy to do:</p>

<ol>
<li>Click on &ldquo;Redis Enterprise on PCF&rdquo; in the left menu.</li>
<li>Click on the &ldquo;<strong>+</strong>&rdquo; link.

<ul>
<li>The tiles is updated to the new version, you can review the configuration, not needed in this tutorial.</li>
</ul>
</li>
<li><p>Click on &ldquo;Review Pending Changes&rdquo; button.
 <img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/img-004-tile-update.png"></p></li>
<li><p>Unselect all product except Redis</p></li>
<li>Click Apply Changes</li>
</ol>


<p>Once you have clicked the update process will start, and you can follow the progress using the log information.</p>

<p>Nevertheless, it is interesting to see what is happening behind the scene using the command line on the VMS.</p>

<p>The update process using PCF will do the following:</p>

<ul>
<li>Update and restart each node one by one (the 5 nodes of the Redis Enterprise deployment)</li>
<li>during these steps, Redis Cluster will fail over moving the master and endpoint to another node to provide service continuity to the applications.</li>
</ul>


<p>Let&rsquo;s look at the following screenshots to see how the rolling upgrade was done by PCF.</p>

<h4>Starting Point</h4>

<p>The cluster is up and running with 3 nodes with the version 5.4.0-24, and the <code>node:1</code> is the master of the cluster</p>

<p><strong><em>Cluster Nodes:</em></strong></p>

<p><img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/001-rladmin-cluster-version.png"></p>

<p><strong><em>Endpoints:</em></strong></p>

<p><img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/001-rladmin-cluster-version-endpoint.png"></p>

<p>The <code>node:1</code> is also the endpoint for the <code>db:4</code></p>

<p><strong><em>Shards:</em></strong></p>

<p><img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/001-rladmin-cluster-db.png"></p>

<p>You can see 3 shards in this deployment:</p>

<ul>
<li><code>db:4</code> is replicated and has 2 shards the master on <code>node:1</code> and a replica on <code>node:2</code>, the failover will automatically happen with no data loss.</li>
<li><code>db:5</code> is not replicated and has a single shard, so the database will be recreated fresh on a new node during the update.</li>
</ul>


<p>So if you want to have a full service continuity with no data loss it is mandatory to use replication.</p>

<h4>PCF Updating Node 1</h4>

<p>PCF has now started the process and stopped the <code>node:1</code>.</p>

<p><strong><em>Cluster Nodes:</em></strong></p>

<p><img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/002-rladmin-cluster-status.png"></p>

<p>All the nodes are still on the &ldquo;old version&rdquo;, but the cluster master has been moved now to <code>node:2</code>; so applications will continue to work.</p>

<p>The errors are here to indicate that the <code>node:1</code> is not accessible, and the <code>node:3</code> also raised an error since the replication link is not available.</p>

<p><strong><em>Endpoints:</em></strong></p>

<p><img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/002-rladmin-cluster-version-endpoint.png"></p>

<p>Here we see that the <code>db:4</code> endpoint, now on <code>node:2</code>, Redis Enterprises cluster manager has moved the endpoint to this node automatically.</p>

<p><strong><em>Shards:</em></strong></p>

<p><img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/002-rladmin-cluster-db.png"></p>

<ul>
<li><code>db:4</code> is up and the master shard has been moved from <code>node:1</code> to <code>node:2</code></li>
<li><code>db:5</code> is not present anymore, a new master will be created automatically on <code>node:3</code>, but empty.</li>
</ul>


<p>The fail over is done transparently with no impact for the application.</p>

<h4>PCF is restarting the updated Node 1</h4>

<p>Once the node:1 VM is restarted with the updated version of Redis Enterprise you can see the new version number and status.</p>

<p><strong><em>Cluster Nodes:</em></strong></p>

<p><img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/003-rladmin-cluster-version.png"></p>

<p>The 3 nodes of the cluster are up and running, and you can see that the <code>node:1</code> has been updated to the new version 5.4.4-7.</p>

<p>The master is still the <code>node:2</code></p>

<p>For a short time the cluster will have heterogeneous nodes, this is not an issue.</p>

<p><strong><em>Shards:</em></strong></p>

<p><img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/003-rladmin-cluster-db.png"></p>

<p>You can see that the <code>db:4</code> shards have the status <code>OK, OLD VERSION</code> that indicates that:</p>

<ul>
<li>the database is up and running</li>
<li>but the database itself has not yet been updated to the latest Redis version</li>
</ul>


<p>The update of the database is done automatically, so after a while, if you run the command <code>sudo rladmin status databases extra redis_version</code> you will see something like:</p>

<p><img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/003-rladmin-cluster-db-version.png"></p>

<h4>Updating all the nodes</h4>

<p>The PCF update will continue and upgrade:</p>

<ul>
<li><code>node:2</code>, Redis Cluster will move the masters (cluster, shard, endpoint) to another node, in our case <code>node:1</code> for the replicated database (<code>db:4</code>)</li>
<li>once the <code>node:2</code> is done the same work will be done on node 3.</li>
</ul>


<p><strong><em>Cluster Nodes:</em></strong></p>

<p>All the nodes of the clusters are now updated to the latest version of Redis Enterprise (5.4.4-7) supported on PCF.</p>

<p><img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/004-rladmin-cluster-version.png"></p>

<p><strong><em>Shards:</em></strong></p>

<p><img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/004-rladmin-cluster-db.png"></p>

<p>The update of the database is done automatically, so after a while if your run the command <code>sudo rladmin status databases extra redis_version</code> you will see something like:</p>

<p><img class="center" src="/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/004-rladmin-cluster-db-version.png"></p>

<blockquote><p>In this example I am doing a “minor upgrade”, from Redis Cluster 5.4.0/Redis 5.0.2 to Redis Cluster 5.4.4/Redis 5.0.4, and everything is done automatically.</p>

<p>If you are doing a major upgrade for example from 4.x to 5.x, the cluster will automatically be updated to the proper release, but you will have to manually update the existing databases as documented here.</p></blockquote>

<h4>Updating Redis on PCF Services</h4>

<p>During the update, you will see other VMs stopped and started in the process. These VMs are used for:</p>

<ul>
<li>Redis Registrar</li>
<li>ResisLabs Service Broker</li>
</ul>


<p>These services and nodes are not part of the &ldquo;Redis Enterprise&rdquo; per se, but are part of the integration with PCF.</p>

<h2>Conclusion</h2>

<p>The update of the Redis Cluster is now complete:</p>

<ul>
<li>All the nodes are on 5.4.4-7 (from 5.4.0-024)</li>
<li>All the databases have been updated to the new Redis 5.0.4 (from 5.0.2)</li>
</ul>


<p>The upgrade has been done automatically without any interruptions of service:</p>

<ul>
<li>PCF scripts have been responsible for upgrading, stoping and starting each part of the installation in the correct order</li>
<li>while Redis Enterprise Cluster has been responsible for keeping the databases available for the applications, during the process.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Multi-Nodes Redis Cluster With Docker]]></title>
    <link href="http://tgrall.github.io/blog/2019/09/05/multi-nodes-redis-cluster-with-docker/"/>
    <updated>2019-09-05T11:33:56+02:00</updated>
    <id>http://tgrall.github.io/blog/2019/09/05/multi-nodes-redis-cluster-with-docker</id>
    <content type="html"><![CDATA[<p>As part of my on-boarding/training at RedisLabs I continue to play with the product, and I have decided today to install a local 3 nodes cluster of Redis Enterprise Software (RS); and show how easy is to move from a single node/shard database to a multi nodes highly available one.</p>

<p>Once your cluster is up &amp; running, you will kill some containers to see how the system automatically fail-over to guarantee service continuity.</p>

<p>The deployment will look more or less like the schema below, (<em><a href="https://docs.redislabs.com/latest/rs/getting-started/docker/">coming from RedisLabs documentation</a></em>)</p>

<p><img class="center" src="/images/posts/multi-nodes-redis-cluster-with-docker/docker-deployment.png"></p>

<p>This is a perfect environment for learning, developing and testing your applications, but it is not supported in production; for production, you can use:</p>

<ul>
<li><a href="https://redislabs.com/redis-enterprise/pro/">Redis Cloud</a></li>
<li><a href="https://docs.redislabs.com/latest/platforms/openshift/">Redis Enterprise Software with Kubernetes and Red Hat OpenShift</a></li>
<li><a href="https://docs.redislabs.com/latest/platforms/pks/">Redis Enterprise Software with Kubernetes Operator on PKS (Pivotal Container Service)</a></li>
<li><a href="https://docs.redislabs.com/latest/platforms/pcf/">Redis Enterprise for Pivotal Cloud Foundry (PCF)</a>.</li>
</ul>


<!--more-->


<p><strong>Prerequisites:</strong></p>

<ul>
<li>Docker Desktop (<em>I am running Docker on Mac</em>)</li>
</ul>


<h3>Installing and Running your First Redis Node</h3>

<p>As usual, installing a new product with Docker is very simple just run the following command:</p>

<pre><code class="bash">docker run -d --cap-add sys_resource \
--name  redis-node1 \
-p 8443:8443 \
-p 9443:9443 \
-p 12000:12000 \
redislabs/redis
</code></pre>

<p>Let&rsquo;s look at the parameters used here:</p>

<ul>
<li><code>-d</code>: run the container in the background</li>
<li><code>--cap-add sys_resource</code>: add Linux  <code>sys_resource</code>capabilities to set proper privileges</li>
<li><code>--name  redis-node1</code>: naming the container</li>
<li><code>-p 8443:8443</code>: to access the management web UI (HTTPS)</li>
<li><code>-p 9443:9443</code>: to access the REST API (HTTPS)</li>
<li><code>-p 12000:12000</code>: the TCP port that we will use for the database endpoint on this node</li>
<li><code>redislabs/redis</code>: use the RedisLabs image (the enterprise version of Redis)</li>
</ul>


<h4>Creating a new Cluster</h4>

<p>Once the container is started you can configure the &ldquo;cluster&rdquo;.</p>

<ol>
<li>Go top <a href="https://localhost:8443/">https://localhost:8443/</a> (accept the connect using the temporary certificate)</li>
<li>Click &ldquo;Setup&rdquo;</li>
<li>Change the Cluster Name to &ldquo;my-redis-cluster.tug-demo.com&rdquo;</li>
<li>Click &ldquo;Next&rdquo;</li>
<li>On the &ldquo;cluster authentication&rdquo; click &ldquo;Next&rdquo;  <em>(we will be using the free version)</em></li>
<li>Enter the user admin credentials and click &ldquo;Next&rdquo;.</li>
</ol>


<p>Once it is configured, connect to the console to the console using the credentials you have created.</p>

<h4>Adding a new database</h4>

<p>Now you have to create a new database.</p>

<ol>
<li>Select &ldquo;Redis Database&rdquo; and &ldquo;Single Region&rdquo;</li>
<li>Enter the name &ldquo;test-db-001&rdquo;, and &ldquo;0.5&rdquo; for the memory limit</li>
<li>Click &ldquo;Show Advanced Options&rdquo;</li>
<li>Enter 12000 in the &ldquo;Endpoint port number&rdquo; field</li>
<li>Click &ldquo;Activate&rdquo;.</li>
</ol>


<p>After  fewseconds, the database is created and available.</p>

<p>Note: we have not set anything special around clustering and replication; we will do that later.</p>

<h4>Using the Single Node Database</h4>

<p>You can now connect to the database. You can use  <code>redis-cli</code>from your host, or you can connect to the container and do it from there:</p>

<pre><code class="bash">&gt; docker  exec-it redis-node1 /bin/bash

redislabs@0a174e819a6b:/opt$ redis-cli -p 12000

127.0.0.1:12000&gt; SET foo bar
OK

127.0.0.1:12000&gt; GET foo
"bar"

127.0.0.1:12000&gt;  exit
</code></pre>

<p><strong><em>Checkpoint</em></strong></p>

<p>So far you have:</p>

<ol>
<li>Install a single node cluster of Redis Enterprise using Docker</li>
<li>Create a new cluster</li>
<li>Created a database that listens on port 12000.</li>
</ol>


<p>In the container, run the  <code>rladminstatus</code>command, to get information about your deployment.</p>

<p><img class="center" src="/images/posts/multi-nodes-redis-cluster-with-docker/rladmin-status.png"></p>

<p>In the cluster node section, you can see the address of the node, 172.17.0.2 in my case. This is the IP address of the container, that will be used to create the multi-node cluster.</p>

<p>It is time to add new nodes to the cluster and enable replication and sharding</p>

<h3>Adding new nodes</h3>

<p>To add new nodes to the cluster, you start new containers. Since the 3 containers will be running on the same host, it is necessary, to avoid conflicts, to use different mapping to the Web UI, REST API, and database endpoint ports.</p>

<p><strong>Start node 2:</strong></p>

<pre><code class="bash">docker run -d --cap-add sys_resource \
--name redis-node2 \
-p 8444:8443 \
-p 9444:9443 \
-p 12001:12000 \
redislabs/redis
</code></pre>

<p><strong>Start node 3:</strong></p>

<pre><code class="bash">docker run -d --cap-add sys_resource \
--name redis-node3 \
-p 8445:8443 \
-p 9445:9443 \
-p 12002:12000 \
redislabs/redis
</code></pre>

<p>So to configure each node you need to use the URLs:</p>

<ul>
<li>node 2: <a href="https://localhost:8444/">https://localhost:8444/</a></li>
<li>node 3: <a href="https://localhost:8445/">https://localhost:8445/</a></li>
</ul>


<p>I have just increase the port number of the Web UI (8443: node 1, 8444: node 2, 8445 node 3).</p>

<p>For these 2 new nodes, do the following steps to add them to the cluster:</p>

<ol>
<li>Click &ldquo;Setup&rdquo;</li>
<li>In  clusterconfiguration, select &ldquo;Join Cluster&rdquo;,

<ul>
<li>Enter the IP address of the first node, 172.17.0.2 in my environment</li>
<li>Enter the credentials you have used during the installation of the first node.</li>
</ul>
</li>
<li>Click &ldquo;Next&rdquo;</li>
</ol>


<p>After a few seconds, you will be redirected to the home page and see the list of nodes of your cluster.</p>

<p>Repeat the same steps for the third node.</p>

<p>Your environment should look like this after the installation and configuration of the 3 nodes.</p>

<p><img class="center" src="/images/posts/multi-nodes-redis-cluster-with-docker/cluster-view-3-nodes.png"></p>

<p>You can also reuse the  <code>rladminstatus</code>command on one of the containers and see the new configuration.</p>

<p>If you look carefully, you can see that you have only 1 shard in your cluster. Let&rsquo;s now add a new shard to the database.</p>

<p><strong>Enabling Clustering and Replication to the DB</strong></p>

<p>In the Redis Enterprise Admin Web UI, (<em>you can use any of the nodes</em>):</p>

<ol>
<li>Click on the &ldquo;databases&rdquo; tab</li>
<li>Click on &ldquo;test-db-001&rdquo; database</li>
<li>Click on the &ldquo;configuration&rdquo;</li>
<li>Go to the bottom of the page and click &ldquo;Edit&rdquo;</li>
<li>Check the &ldquo;Replication&rdquo; checkbox, to create new shard that will be a replica, to provide high availability</li>
<li>Check &ldquo;Database Clustering&rdquo; and increase the number of shards to 2. This will  <em>distribute</em>the data in your database into 2 shards, this for better scalability.
 <em>You can see that the UI indicated that you have  </em>4 shards with replication*. Yes because you have a database that you have &ldquo;divided in 2&rdquo;, and each of the portions of the database is replicated.
(Also with the free version of Redis Enterprise you are limited to 4 shards, so do not be surprised if you can not increase the number of shards to more than 4)</li>
<li>Click &ldquo;Update&rdquo; at the bottom of the page.</li>
</ol>


<p>Go back to the &ldquo;nodes&rdquo; tab, and you will see that you have now 4 shards distributed on your 3 nodes.</p>

<p><strong>Discovering the cluster topology</strong></p>

<p>Run  <code>rladminstatus</code>to inspect your cluster and see how the various components are installed:</p>

<p><img class="center" src="/images/posts/multi-nodes-redis-cluster-with-docker/rladmin-status-with-db.png"></p>

<p>For example, you can see, that in my environment:</p>

<p><em>in the &ldquo;CLUSTER NODES&rdquo; section that the &ldquo;node:1&rdquo; is the master of the cluster
</em>in the &ldquo;DATABASES&rdquo; section that replication is enabled, and the database uses a &ldquo;<a href="https://docs.redislabs.com/latest/rs/concepts/rebalancing-shard-placement/#dense-shard-placement-policy">dense placement</a>&rdquo;
<em>in the &ldquo;SHARDS&rdquo; section you can see the various shards and their placement (</em>node:1|2|3<em>), their role (</em>master|slave*) and their slots.</p>

<p>Using Redis Enterprise Enterprise Software (RS), all the clustering is managed transparently for you, and your applications. This means that you just have to connect your application to RS Cluster.</p>

<p><strong>Clustering in Action</strong></p>

<p>First of all, you have already seen a lot, just using the Web UI (and you could have done it using CLI and REST API), you have moved an existing database from a single instance to a distributed and highly available instance.</p>

<p>So now if something happens to the system, for example, if one of the masters disappears RS will automatically get another  oneelected.</p>

<p>Let me kill for example the node 3 that contains the 2 masters for my database.</p>

<pre><code class="bash">&gt; docker  killredis-node3
</code></pre>

<p>After a few seconds, you should see that the master shards are now on another node, in my case node:1.</p>

<p><img class="center" src="/images/posts/multi-nodes-redis-cluster-with-docker/rladmin-status-with-db-002.png"></p>

<p>So if an application, is using this cluster it would be almost transparent as the election of the new master is happening in the  backgroud.</p>

<p>If you restart the node 3 it will rejoin the cluster, and the replicas will be updated on node 3 with any changes that happened to the masters.</p>

<pre><code class="bash">&gt; docker start  redis-node3
</code></pre>

<p>The same automatic fail-over will happen if you kill a node with the cluster  manager,or the endpoint.</p>

<h4>Conclusion</h4>

<p>In this small article you have learned how to:</p>

<ul>
<li>deploy a 3 nodes Redis Enterprise Server (RS) on Docker (on a single host)</li>
<li>create a database, and make it highly available and distributed easily using the Admin UI</li>
<li>look at the deployment using  <code>rladminstatus</code>command.</li>
</ul>


<p>You have also seen, by killing some nodes, how the cluster fail-over will various master services (shards, endpoint, master cluster node) to another node automatically. This to ensure a continuity of service for your application.</p>

<p>In another  postI will show what is the exact behavior of client applications during the fail-over.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deploy Your Node/Couchbase Application to the Cloud With Clever Cloud]]></title>
    <link href="http://tgrall.github.io/blog/2013/07/11/deploy-your-node-slash-couchbase-application-to-the-cloud-with-clever-cloud/"/>
    <updated>2013-07-11T06:47:51+02:00</updated>
    <id>http://tgrall.github.io/blog/2013/07/11/deploy-your-node-slash-couchbase-application-to-the-cloud-with-clever-cloud</id>
    <content type="html"><![CDATA[<h3>Introduction</h3>

<p><a href="http://www.clever-cloud.com/en/">Clever Cloud</a> is the first PaaS to provide Couchbase as a service allowing developers to run applications in a fully managed environment. This article shows how to deploy an existing application to Clever Cloud.</p>

<p><img class="<a" src="href="http://f.cl.ly/items/2L2M2k2O000e3g2N1z3z/couchbase_gradient_clever.png">http://f.cl.ly/items/2L2M2k2O000e3g2N1z3z/couchbase_gradient_clever.png</a>&#8221;></p>

<p>I am using a very simple Node application that I have documented in a previous article: “<a href="http://tugdualgrall.blogspot.fr/2013/03/easy-application-development-with.html">Easy application development with Couchbase, Angular and Node</a>”.</p>

<p>Clever Cloud provides support for various databases MySQL, PostgreSQL, but also and this is most important for me <a href="http://www.clever-cloud.com/en/services/couchbase.html">Couchbase</a>. No only Clever Cloud allows you to use database services but also you can deploy and host your application that could be developed in the language/technology of your choice : Java, Node, Scala, Python, PHP, … and all this in a secure, scalable and managed environment.</p>

<!-- more -->


<h3>Setting up your Clever Cloud environment</h3>

<h4>Create your account</h4>

<ol>
<li>Go to the Clever Cloud site :<a href="http://www.clever-cloud.com/"> http://www.clever-cloud.com/</a></li>
<li>Click on “Login” link and follow the steps to create your account.</li>
<li>After few seconds you will received an email and be redirected to the Clever Cloud Console.</li>
</ol>


<h4>Create a Couchbase instance</h4>

<p>The <a href="https://console.clever-cloud.com/">Clever Cloud Console</a> allows you to create your Couchbase Bucket in few clicks:</p>

<p>1-  Cick on “Services” in the left menu</p>

<p>2-  Click on “Add a Service” in the left menu </span></p>

<p><img class="<a" src="href="http://4.bp.blogspot.com/-He8scPOrH5I/Uac5B_O2k3I/AAAAAAAAAcE/OZyn8jW-bV8/s320/clever-cloud-add-couchbase.png">http://4.bp.blogspot.com/-He8scPOrH5I/Uac5B_O2k3I/AAAAAAAAAcE/OZyn8jW-bV8/s320/clever-cloud-add-couchbase.png</a>&#8221;></p>

<p>3- Click on “Couchbase” button.</p>

<ol>
<li>Select the size of the RAM quota for your bucket</li>
</ol>


<p><img class="<a" src="href="http://4.bp.blogspot.com/-V_GMolXLClI/Uac5CMdKQoI/AAAAAAAAAcM/vvGKJUXW-xQ/s320/Screen+Shot+2013-05-30+at+9.19.59+AM.png">http://4.bp.blogspot.com/-V_GMolXLClI/Uac5CMdKQoI/AAAAAAAAAcM/vvGKJUXW-xQ/s320/Screen+Shot+2013-05-30+at+9.19.59+AM.png</a>&#8221;></p>

<p>The size of the RAM Quota for your bucket will have an impact on performance but also on the pricing.</p>

<p>5- Click “Add this Service”</p>

<p>You are done, you should receive an email with all the information to access your newly created bucket.</p>

<p>The mail from Clever Cloud contains the following information:</p>

<pre><code>db_host = xxxxxxxx.couchbase.clvrcld.net    Location of the database, this is where the endpoint is located.
db_name = yyyyyyyy  Name of the Couchbase bucket
db_username = xxxxxxxx  Not used in Couchbase context
db_password = zzzzzzzz  Password to connect to the Couchbase Bucket
</code></pre>

<p>So you are now ready to use your bucket.</p>

<p>Note: In the current version of the Clever Cloud Couchbase Service you do not have access to a management console. If you want to get some information about the database or create views you need to do it from you application code.</p>

<h4>Connect your Application to Couchbase@Clever-Cloud</h4>

<p>The first step is to get some code, so let’s clone the “Couchbase Ideas Sample Application”, and install the dependencies, using the following commands:</p>

<pre><code>git clone -b 03-vote-with-value https://github.com/tgrall/couchbase-node-ideas.git

cd couchbase-node-ideas

git branch mybranch

git checkout mybranch

npm install
</code></pre>

<p>Open the app.js and edit the connection info to point your application to the Couchbase instance and modify the HTTP port of your application to 8080 - this is a mandatory step documented <a href="http://doc.clever-cloud.com/nodejs/nodejs/#requirements">here</a> :</p>

<pre><code class="js">dbConfiguration = {
  "hosts": ["xxxxxxxxxxx.couchbase.clvrcld.net:8091"],
  "bucket": "xxxxxxxxxxx",
  "user": "xxxxxxxxxx",
  "password": "yyyyyyyyyyyyyyyyyyyyyyyyy"
};
...
...

appServer = app.listen(8080, function() {
  console.log("Express server listening on port %d in %s mode", appServer.address().port, app.settings.env);
});
</code></pre>

<p>Launch your application using</p>

<pre><code>node app.js
</code></pre>

<p>Go to <a href="http://localhost:8080">http://localhost:8080</a></p>

<p>Your application is now using Couchbase on the cloud powered by Clever Cloud. Let’s now deploy the application itself on Clever Cloud</p>

<h3>Deploy your application on Clever Cloud</h3>

<p>The easiest way to deploy an application to Clever Cloud is using git. The first thing to do is to add your SSH public key into Clever Cloud Console. If you do not have any SSH yet, follow the steps described on Github : “<a href="https://help.github.com/articles/generating-ssh-keys">Generating SSH Keys</a>”.</p>

<h4>Add your SSH key</h4>

<p>Note: As you can guess this should be done only once</p>

<p>Open the id_rsa.pub file with a text editor. This is your SSH key. Select all and copy to your clipboard.</p>

<ol>
<li>Go to the Clever Cloud Console</li>
<li>Click on “Profile” entry in the left menu</li>
<li>Click on “SSH Keys”</li>
<li>Click on “Add a SSH Key”</li>
<li>Enter a name (anything you want) and paste your key</li>
<li>Click “Add” button</li>
</ol>


<p>You are now ready to deploy applications to Clever Cloud. The next thing to do, is to create a new node application in Clever Cloud.</p>

<h4>Create your Application</h4>

<ol>
<li>Click “Add an app” in the Application menu in the top menu.</li>
<li>Give a name and description to this application</li>
<li>Select the Instance type, in this case “Node.js”</li>
<li>Configure your instances, you can keep the default values for now, click “Next”</li>
<li>Check the configuration, and click “Create”</li>
</ol>


<p>Your application is created, you are redirected to the generic information page, where you can find a Git URL that we will use to deploy the application.</p>

<p>You can navigate into the entries in the left menu to see more information about your application. In addition to the Information page, you can look at the following entries:</p>

<ol>
<li>“Domain Names” to configure the URL to access your application</li>
<li>“Logs” to view the application logs</li>
</ol>


<h4>Deploy the Application</h4>

<p>So we are almost there!</p>

<p>The deployment to Clever Cloud is done using a Git push command, so you need to add the deployment URL as a remote repository to your application, using the following command:</p>

<pre><code>git remote add clever git+ssh://git@push.clever-cloud.com/app_[your_app_id].git

git commit -a -m “Couchbase on Clever Cloud connection”

git push clever mybranch:master
</code></pre>

<p>Once you have added the application as remote repository you can commit and push your application.</p>

<p>The last command pushes the application  to Clever Cloud. It is important to note that Clever Cloud will always deploy the application on the “master” branch on the remote repository. The notation mybranch:master is used to mention it. If you work locally on your master branch just use “master”.</p>

<p>You can now go to the Clever Cloud console and look in the log and click on the URL in the “Domain Names” section to test your application.</p>

<p>You should be able to see your application, that is running on the Clever Cloud PaaS.</p>

<p>When you update your application, you just need to do a  git push and git commit.</p>

<h3>Conclusion</h3>

<p>In this tutorial you have learned how to:</p>

<ul>
<li>Create your Clever Cloud account</li>
<li>Create a Couchbase instance</li>
<li>Create and deploye a Node.js application</li>
</ul>


<p>Feel free to test this yourself, with Node or other technology, as you can see it is quite easy to setup.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SAP Cloud Inside : Build and Run Applications in the Cloud]]></title>
    <link href="http://tgrall.github.io/blog/2012/02/19/sap-cloud-inside-build-and-run-applications-in-the-cloud/"/>
    <updated>2012-02-19T23:56:56+01:00</updated>
    <id>http://tgrall.github.io/blog/2012/02/19/sap-cloud-inside-build-and-run-applications-in-the-cloud</id>
    <content type="html"><![CDATA[<p>Last week I was invited to present eXo Cloud IDE during the SAP Cloud Inside. This SAP Community event was a great opportunity to discuss about the cloud with an interesting point of view: the impact of the cloud for SAP customers (especially administrators and developers).</p>

<p>During this presentation I have introduced the <a href="http://www.cloud-ide.com/">eXo Cloud IDE</a>, and I did a demonstration in which O have built and deployed applications : Open Social Gadgets, Ruby on Rails and Java/Spring, and explain how it could be extended to SAP business services.</p>

<p>Here the slides that I have used during this presentation:</p>

<p><iframe src="http://www.slideshare.net/slideshow/embed_code/95672 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen></iframe> </p>

<p><strong><a href="http://www.slideshare.net/tgrall/sap-cloud-inside-develop-and-deploy-on-the-cloud" title="SAP Cloud Inside : Develop and Run on the Cloud">SAP Cloud Inside : Develop and Run on the Cloud</a></strong></p>

<p>Remember that you can register yourself to the <a href="http://www.cloud-ide.com/">eXo Cloud IDE</a> Service and start develop application from your browser.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[eXo Cloud IDE : Develop for the Cloud on the Cloud]]></title>
    <link href="http://tgrall.github.io/blog/2011/03/16/exo-cloud-ide-develop-for-the-cloud-on-the-cloud/"/>
    <updated>2011-03-16T12:55:44+01:00</updated>
    <id>http://tgrall.github.io/blog/2011/03/16/exo-cloud-ide-develop-for-the-cloud-on-the-cloud</id>
    <content type="html"><![CDATA[<p><img class="<a" src="href="http://1.bp.blogspot.com/-IgPlseainto/TYBq0-Sr6nI/AAAAAAAAAOw/YL3TECcXJts/s400/logoexoplatform.png">http://1.bp.blogspot.com/-IgPlseainto/TYBq0-Sr6nI/AAAAAAAAAOw/YL3TECcXJts/s400/logoexoplatform.png</a>&#8221;></p>

<p>Yesterday, eXo has launched a new cloud based service: the <a href="http://www.cloud-ide.com/">eXo Cloud IDE</a>. This IDE is an online service that facilitates the development of gadgets and mashups that could be deployed directly to a PaaS.</p>

<p>Before launching this service on the Cloud we, eXo team and customers, have used the IDE embedded in the <a href="http://www.exoplatform.com/company/en/platform/exo-platform-35">eXo Platform</a> to extend our intranet and customer deployments (some of the modules that we have developed live on our intranet are available as plugins on the <a href="http://www.exoplatform.com/company/en/Content-types/Plugins">eXo Resource Center</a>).</p>

<p>This IDE is the last mile between the users and the developers. It provides a way to add new services asked by business users at a lower cost with a good time to market. And all this based on standards that corporate and Web developers like : REST Services using <a href="http://jcp.org/en/jsr/detail?id=311">JAX-RS</a> and UI based on <a href="http://www.opensocial.org/">OpenSocial</a> gadgets in which you can leverage all the cool and powerful features of HTML5.</p>

<p>What eXo has launched yesterday is a big step for developers since you can now develop, test and deploy your gadgets and services online, and this in your &ldquo;personal environment&rdquo; using eXo Cloud IDE supports multi-tenancy. I won&rsquo;t go in all the features of the IDE since you can test it yourself by joining the <a href="http://cloud-ide.com/">beta program</a> and look at this video:</p>

<p><iframe src="http://player.vimeo.com/video/20815141?portrait=0"  width="550" height="265" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen></iframe></p>

<p>eXo Cloud IDE resources:</p>

<ul>
<li>Overview of <a href="http://www.exoplatform.com/company/en/platform/exo-platform-35">eXo Platform 3.5</a></li>
<li>Overview of <a href="http://cloud-ide.com/">eXo Cloud IDE</a>, including how-to videos and sample applications</li>
<li>Join the <a href="http://cloud-ide.com/">eXo Cloud IDE private beta</a></li>
<li>Benjamin Mestrallet’s <a href="http://blog.exoplatform.org/2011/03/15/history-of-exo-cloud-ide/">blog post</a> about eXo Cloud IDE</li>
<li><a href="http://www.exoplatform.com/exo-platform-3-trial/eXoPlatform-3.zip">Evaluation download</a> of eXo Platform 3.0</li>
</ul>

]]></content>
  </entry>
  
</feed>
