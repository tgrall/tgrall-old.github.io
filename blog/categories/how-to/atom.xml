<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: How-to | Tug's Blog]]></title>
  <link href="http://tgrall.github.io/blog/categories/how-to/atom.xml" rel="self"/>
  <link href="http://tgrall.github.io/"/>
  <updated>2020-01-02T17:37:30+01:00</updated>
  <id>http://tgrall.github.io/</id>
  <author>
    <name><![CDATA[Tug Grall]]></name>
    <email><![CDATA[tugdual@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Getting Started With Apache Flink and Kafka]]></title>
    <link href="http://tgrall.github.io/blog/2016/10/12/getting-started-with-apache-flink-and-kafka/"/>
    <updated>2016-10-12T04:54:17+02:00</updated>
    <id>http://tgrall.github.io/blog/2016/10/12/getting-started-with-apache-flink-and-kafka</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p><a href="https://flink.apache.org/">Apache Flink</a> is an open source platform for distributed stream and batch data processing. Flink is a streaming data flow engine with several APIs to create data streams oriented application.</p>

<p>It is very common for Flink applications to use <a href="http://kafka.apache.org/">Apache Kafka</a> for data input and output. This article will guide you into  the steps to use Apache Flink with Kafka.</p>

<p><img class="center" src="/images/posts/flink-kafka/flink-kafka.png" title="Flink-Kafka" ></p>

<!-- more -->


<h3>Prerequisites</h3>

<ul>
<li>Apache Kafka 0.9.x</li>
<li>Git</li>
<li>Maven 3.x or later</li>
</ul>


<h2>Create your Flink Streaming Project</h2>

<p>The first step is to create an Java application, the easiest is to use the flink-quickstart-java archetype, that contains the core dependencies and packaging tasks. This article is similar with the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/quickstart/run_example_quickstart.html">Apache Flink Quick Start Example</a>, with a clear focus on data input and output with MapR Streams.</p>

<p>In this application we will create two jobs:</p>

<ul>
<li><code>WriteToKafka</code> : that generates random string and post them to a MapR Streams Topic using the Kafka Flink Connector and its Producer API.</li>
<li><code>ReadFromKafka</code> : that reads the same topic and print the messages in the standard output using the Kafka Flink Connector and its Consumer. API.</li>
</ul>


<p>The full project is available on GitHub:</p>

<ul>
<li><a href="https://github.com/tgrall/kafka-flink-101">Flink and Kakfa Application</a></li>
</ul>


<p>Let’s create the project using Apache Maven:</p>

<pre><code class="bash">mvn archetype:generate \
      -DarchetypeGroupId=org.apache.flink\
      -DarchetypeArtifactId=flink-quickstart-java \
      -DarchetypeVersion=1.1.2 \
      -DgroupId=com.grallandco.demos \
      -DartifactId=kafka-flink-101 \
      -Dversion=1.0-SNAPSHOT \
      -DinteractiveMode=false 
</code></pre>

<p>Maven will create the following structure:</p>

<pre><code class="bash">tree kafka-flink-101/
kafka-flink-101/
├── pom.xml
└── src
    └── main
        ├── java
        │   └── com
        │       └── grallandco
        │           └── demos
        │               ├── BatchJob.java
        │               ├── SocketTextStreamWordCount.java
        │               ├── StreamingJob.java
        │               └── WordCount.java
        └── resources
            └── log4j.properties

7 directories, 6 files
</code></pre>

<p>This project is configured to create a Jar file that contains your flink project code and also includes all dependencies needed to run it.</p>

<p>The project contains some other sample jobs, we do not need them for this article, you can either keep them to educational purposes or simply remove them from the project.</p>

<h2>Add Kafka Connector</h2>

<p>Open the <code>pom.xml</code> and add the following dependencies to your project:</p>

<p>As a first step, we have to add the Flink Kafka connector as a dependency so that we can use the Kafka sink. Add this to the pom.xml file in the dependencies section:</p>

<p>You must add now the Flink Kafka Connector dependency to use the Kafka sink. Add the following entry in the <code>&lt;dependencies&gt;</code> element:</p>

<pre><code> &lt;dependency&gt;
      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
      &lt;artifactId&gt;flink-connector-kafka-0.9_2.10&lt;/artifactId&gt;
      &lt;version&gt;${flink.version}&lt;/version&gt;
 &lt;/dependency&gt;
</code></pre>

<p>The Flink project is now ready to use the DataStream using the Kafka Connector so you can send and receive messages from Apache Kafka.</p>

<h2>Install and Start Kafka</h2>

<p>Download Kafka, enter the following commands in your terminal:</p>

<pre><code class="bash">curl -O http://www.us.apache.org/dist/kafka/0.9.0.0/kafka_2.11-0.9.0.0.tgz
tar -xzf kafka_2.11-0.9.0.0.tgz
cd kafka_2.11-0.9.0.0
</code></pre>

<p>Kafka uses ZooKeeper, if you do not have Zookeeper running, you can start it using the following command:</p>

<pre><code class="bash">./bin/zookeeper-server-start.sh config/zookeeper.properties
</code></pre>

<p>Start a Kafka broker by running the following command in a new terminal:</p>

<pre><code class="bash">./bin/kafka-server-start.sh config/server.properties
</code></pre>

<p>In another terminal, run the following command to create a Kafka topic called <code>flink-demo</code>:</p>

<pre><code class="bash">./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic flink-demo
</code></pre>

<p>Use the Kafka tools to post and consume messages to the <code>flink-demo</code> topic.</p>

<p>Producer</p>

<pre><code class="bash">./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic flink-demo
</code></pre>

<p>Consumer</p>

<pre><code class="bash">./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic flink-demo --from-beginning
</code></pre>

<p>In the producer window, you can post some messages and see them in the consumer windows. We will use these tools to follow the interactions between Kafka and Flink.</p>

<h2>Write your Flink application</h2>

<p>Let’s now use the Flink Kafka Connector to send messages to Kafka and consume them.</p>

<h3>Producer</h3>

<p>The producer generates messages using the <code>SimpleStringGenerator()</code> class and send the string to the <code>flink-demo</code> topic.</p>

<pre><code>  public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    Properties properties = new Properties();
    properties.setProperty("bootstrap.servers", “localhost:9092"); 

    DataStream&lt;String&gt; stream = env.addSource(new SimpleStringGenerator());
    stream.addSink(new FlinkKafkaProducer09&lt;&gt;("flink-demo", new SimpleStringSchema(), properties));

    env.execute();
  }
</code></pre>

<p>The <code>SimpleStringGenerator()</code> method code is available <a href="https://github.com/tgrall/kafka-flink-101/blob/master/src/main/java/com/grallandco/demos/WriteToKafka.java#L45-L60">here</a>.</p>

<p>The main steps are:</p>

<ul>
<li>create a new <code>StreamExecutionEnvironment</code> the basis of any Flink application</li>
<li>create a new <code>DataStream</code> in the application environment, the <code>SimpleStringGenerator</code> class implements the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/api/java/">SourceFunction</a> the base interface for all streams data sources in Flink.</li>
<li>add the <code>FlinkKafkaProducer09</code> sink to the topic.</li>
</ul>


<h3>Consumer</h3>

<p>The consumer simply reads the messages from the <code>flink-demo</code> topic, and print them into the console.</p>

<pre><code>  public static void main(String[] args) throws Exception {
    // create execution environment
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    Properties properties = new Properties();
    properties.setProperty("bootstrap.servers", “localhost:9092");
    properties.setProperty("group.id", "flink_consumer");

    DataStream&lt;String&gt; stream = env.addSource(new FlinkKafkaConsumer09&lt;&gt;(
        "flink-demo", new SimpleStringSchema(), properties) );

    stream.map(new MapFunction&lt;String, String&gt;() {
      private static final long serialVersionUID = -6867736771747690202L;

      @Override
      public String map(String value) throws Exception {
        return "Stream Value: " + value;
      }
    }).print();

    env.execute();
  }
</code></pre>

<p>The main steps are:</p>

<ul>
<li>create a new <code>StreamExecutionEnvironment</code> the basis of any Flink application</li>
<li>create a set of properties with the consumer information, in this application we can only set the consumer <code>group.id</code>.</li>
<li>use the <code>FlinkKafkaConsumer09</code> to get the message from the topic <code>flink-demo</code></li>
</ul>


<h2>Build and Run the application</h2>

<p>Let’s run the application directly from Maven (or from your favorite IDE).</p>

<p>1- Build the project:</p>

<pre><code>$ mvn clean package
</code></pre>

<p>2- Run the Flink Producer Job</p>

<pre><code>$ mvn exec:java -Dexec.mainClass=com.mapr.demos.WriteToKafka
</code></pre>

<p>3- Run the Flink Consumer Job</p>

<pre><code>$ mvn exec:java -Dexec.mainClass=com.mapr.demos.ReadFromKafka
</code></pre>

<p>In the terminal, you should see the messages generated from the producer</p>

<p>You can now deploy and execute this job on your Flink cluster.</p>

<h2>Conclusion</h2>

<p>In this article you have learned how to use Flink with kafka to write and read data streams.</p>

<p>Learn about what Apache Flink can do and how it maintains consistency and provides flexibility in the &ldquo;<a href="https://www.mapr.com/introduction-to-apache-flink">Introduction to Apache Flink</a>&rdquo; ebook.</p>
]]></content>
  </entry>
  
</feed>
