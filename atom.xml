<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tug's Blog]]></title>
  <link href="http://tgrall.github.io/atom.xml" rel="self"/>
  <link href="http://tgrall.github.io/"/>
  <updated>2020-01-29T06:08:06+01:00</updated>
  <id>http://tgrall.github.io/</id>
  <author>
    <name><![CDATA[Tug Grall]]></name>
    <email><![CDATA[tugdual@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to Use SSL/TLS With Redis Enterprise]]></title>
    <link href="http://tgrall.github.io/blog/2020/01/02/how-to-use-ssl-slash-tls-with-redis-enterprise/"/>
    <updated>2020-01-02T10:47:13+01:00</updated>
    <id>http://tgrall.github.io/blog/2020/01/02/how-to-use-ssl-slash-tls-with-redis-enterprise</id>
    <content type="html"><![CDATA[<p><img class="center" src="http://tgrall.github.io/images/posts/how-to-use-ssl-slash-tls-with-redis-enterprise/000_header.jpeg"></p>

<p>In this article, I will explain how to secure your Redis databases using SSL (Secure Sockets Layer). In production, it is a good practice to use SSL to protect the data that are moving between various computers (client applications and Redis servers). Transport Level Security (TLS) guarantees that only allowed applications/computers are connected to the database, and also that data is not viewed or altered by a middle man process.</p>

<p>You can secure the connections between your client applications and Redis cluster using:</p>

<ul>
<li>One-Way SSL: the client (your application) get the certificate from the server (Redis cluster), validate it, and then all communications are encrypted</li>
<li>Two-Way SSL: (aka mutual SSL) here both the client and the server authenticate each other and validate that both ends are trusted.</li>
</ul>


<p>In this article, I will focus on the Two-Way SSL, and using Redis Enterprise.</p>

<!-- more -->


<p>Prerequisites:</p>

<ul>
<li>A Redis Enterprise 5.4.x database, (my database is protected by the password <code>secretdb01</code>, and listening on port <code>12000</code>)</li>
<li><code>redis-cli</code> to run basic commands</li>
<li>Python, Node, and Java installed if you want to test various languages.</li>
</ul>


<p><strong>Simple  Test</strong></p>

<p>Let&rsquo;s make sure that the database is available:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>redis-cli -p 12000 -a secretdb01 INFO SERVER</span></code></pre></td></tr></table></div></figure>


<p>This should print the Server information.</p>

<h4>1- Get the Certificate from Redis Cluster</h4>

<p>You have access to the Redis Enterprise Cluster, you go to one of the nodes to retrieve the certificate (that is a self-generated one by default).</p>

<p>The cluster certificate is located at: <code>/etc/opt/redislabs/proxy_cert.pem</code>.</p>

<p>You have to copy it on each client machine; note that once it is done you can use this certificate to connect using &ldquo;One-Way SSL&rdquo;, but not the purpose of this article.</p>

<p>In my demonstration I am using Docker and copy the certificate using this command from my host:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker cp redis-node1:/etc/opt/redislabs/proxy_cert.pem ./certificates</span></code></pre></td></tr></table></div></figure>


<h4>2- Generate a New Client Certificate</h4>

<p>Using the Two-Way SSL you need to have a certificate for the client that will be used by Redis database proxy to trust the client.</p>

<p>In this article I will use a self-signed certificate using OpenSSL, in this example, we are creating a certificate for an application named <code>app_001</code>.</p>

<p>You can create as many certificates as you want, or reuse this one for all servers/applications.</p>

<p>Open a terminal and run the following commands:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>openssl req <span class="se">\</span>
</span><span class='line'>  -nodes <span class="se">\</span>
</span><span class='line'> -newkey rsa:2048 <span class="se">\</span>
</span><span class='line'> -keyout client_key_app_001.pem <span class="se">\</span>
</span><span class='line'> -x509 <span class="se">\</span>
</span><span class='line'> -days <span class="m">36500</span> <span class="se">\</span>
</span><span class='line'> -out client_cert_app_001.pem
</span></code></pre></td></tr></table></div></figure>


<p>This command generate a new client key (<code>client_key_001.pem</code>) and certificate (<code>client_cert_001.pem</code>) with no passphrase.</p>

<h4>3- Configure the Redis Database</h4>

<p>The next step is to take the certificate and add it to the database you want to protect.</p>

<p>Let&rsquo;s copy the certificate and paste it into the Redis Enterprise Web Console.</p>

<p>Copy the certificate in your clipboard:</p>

<p>Mac:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pbcopy &lt; client_cert_app_001.pem
</span></code></pre></td></tr></table></div></figure>


<p>Linux:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> xclip -sel clip &lt; client_cert_app_001.pem
</span></code></pre></td></tr></table></div></figure>


<p>Windows:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>clip &lt; client_cert_app_001.pem
</span></code></pre></td></tr></table></div></figure>


<p>Go to the Redis Enterprise Admin Web Console and enable TLS on your database:</p>

<ol>
<li>Edit the database configuration</li>
<li>Check TLS</li>
<li>Select &ldquo;Require TLS for All communications&rdquo;</li>
<li>Check &ldquo;Enforce client authentication&rdquo;</li>
<li>Paste the certificate in the text area</li>
<li>Click the Save button to save the certificate</li>
<li>Click the Update button to save the configuration.</li>
</ol>


<p><img class="center" src="http://tgrall.github.io/images/posts/how-to-use-ssl-slash-tls-with-redis-enterprise/001-tls-configuration.png"></p>

<p>The database is now protected, and it is mandatory to use the SSL certificate to connect to it.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>redis-cli -p <span class="m">12000</span> -a secretdb01 INFO SERVER
</span><span class='line'><span class="o">(</span>error<span class="o">)</span> ERR unencrypted connection is prohibited
</span></code></pre></td></tr></table></div></figure>


<h4>4- Connect to the Database using the Certificate</h4>

<p>In all following examples, I am using a &ldquo;self-signed&rdquo; certificate, so I do not check the validity of the hostname.
You should adapt the connections/TLS information based on your certificate configuration.</p>

<h4>4.1 Using Redis-CLI</h4>

<p>To connect to a SSL protected database using <code>redis-cli</code> you have to use <a href="https://www.stunnel.org/index.html"><code>stunnel</code></a>.</p>

<p>Create a <code>stunnel.conf</code> file with the following content:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">cert</span> <span class="o">=</span> /path_to/certificates/client_cert_app_001.pem
</span><span class='line'><span class="nv">key</span> <span class="o">=</span> /path_to/certificates/client_key_app_001.pem
</span><span class='line'><span class="nv">cafile</span> <span class="o">=</span> /path_to/certificates/proxy_cert.pem
</span><span class='line'><span class="nv">client</span> <span class="o">=</span> yes
</span><span class='line'>
</span><span class='line'><span class="o">[</span>redislabs<span class="o">]</span>
</span><span class='line'><span class="nv">accept</span> <span class="o">=</span> 127.0.0.1:6380
</span><span class='line'><span class="nv">connect</span> <span class="o">=</span> 127.0.0.1:12000
</span></code></pre></td></tr></table></div></figure>


<p>Start stunnel using the command</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>stunnel ./stunnel.conf
</span></code></pre></td></tr></table></div></figure>


<p>This will start a process that listen to port <code>6380</code> and used as a proxy to the Redis Enterprise database on port <code>12000</code>.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>redis-cli -p <span class="m">6380</span> -a secretdb01 INFO SERVER
</span></code></pre></td></tr></table></div></figure>


<h5>4.2 Using Python</h5>

<p>Using Python, you have to set the SSL connection parameters:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c">#!/usr/local/bin/python3</span>
</span><span class='line'>
</span><span class='line'><span class="kn">import</span> <span class="nn">redis</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">pprint</span>
</span><span class='line'>
</span><span class='line'><span class="k">try</span><span class="p">:</span>
</span><span class='line'>  <span class="n">r</span> <span class="o">=</span> <span class="n">redis</span><span class="o">.</span><span class="n">StrictRedis</span><span class="p">(</span>
</span><span class='line'>    <span class="n">password</span><span class="o">=</span><span class="s">&#39;secretdb01&#39;</span><span class="p">,</span>
</span><span class='line'>    <span class="n">decode_responses</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
</span><span class='line'>    <span class="n">host</span><span class="o">=</span><span class="s">&#39;localhost&#39;</span><span class="p">,</span>
</span><span class='line'>    <span class="n">port</span><span class="o">=</span><span class="mi">12000</span><span class="p">,</span>
</span><span class='line'>    <span class="n">ssl</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
</span><span class='line'>    <span class="n">ssl_keyfile</span><span class="o">=</span><span class="s">&#39;./client_key_app_001.pem&#39;</span><span class="p">,</span>
</span><span class='line'>    <span class="n">ssl_certfile</span><span class="o">=</span><span class="s">&#39;./client_cert_app_001.pem&#39;</span><span class="p">,</span>
</span><span class='line'>    <span class="n">ssl_cert_reqs</span><span class="o">=</span><span class="s">&#39;required&#39;</span><span class="p">,</span>
</span><span class='line'>    <span class="n">ssl_ca_certs</span><span class="o">=</span><span class="s">&#39;./proxy_cert.pem&#39;</span><span class="p">,</span>
</span><span class='line'>    <span class="p">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">info</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</span><span class='line'>  <span class="n">pprint</span><span class="o">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">info</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
</span><span class='line'>  <span class="k">print</span><span class="p">(</span><span class="s">&quot;Error connecting to Redis: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">err</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>More information in the documentation &ldquo;<a href="https://redislabs.com/lp/python-redis/">Using Redis with Python</a>&rdquo;.</p>

<h5>4.3 Using Node.JS</h5>

<p>For <a href="http://redis.js.org/">Node Redis</a>, use the <a href="https://nodejs.org/api/tls.html">TLS</a> library to configure the client connection:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="kd">var</span> <span class="nx">redis</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s2">&quot;redis&quot;</span><span class="p">);</span>
</span><span class='line'><span class="kd">var</span> <span class="nx">tls</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s1">&#39;tls&#39;</span><span class="p">);</span>
</span><span class='line'><span class="kd">var</span> <span class="nx">fs</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s1">&#39;fs&#39;</span><span class="p">);</span>
</span><span class='line'>
</span><span class='line'><span class="kd">var</span> <span class="nx">ssl</span> <span class="o">=</span> <span class="p">{</span>
</span><span class='line'>    <span class="nx">key</span><span class="o">:</span> <span class="nx">fs</span><span class="p">.</span><span class="nx">readFileSync</span><span class="p">(</span><span class="s1">&#39;../certificates/client_key_app_001.pem&#39;</span><span class="p">,</span><span class="nx">encoding</span><span class="o">=</span><span class="s1">&#39;ascii&#39;</span><span class="p">),</span>
</span><span class='line'>    <span class="nx">cert</span><span class="o">:</span> <span class="nx">fs</span><span class="p">.</span><span class="nx">readFileSync</span><span class="p">(</span><span class="s1">&#39;../certificates/client_cert_app_001.pem&#39;</span><span class="p">,</span><span class="nx">encoding</span><span class="o">=</span><span class="s1">&#39;ascii&#39;</span><span class="p">),</span>
</span><span class='line'>    <span class="nx">ca</span><span class="o">:</span> <span class="p">[</span> <span class="nx">fs</span><span class="p">.</span><span class="nx">readFileSync</span><span class="p">(</span><span class="s1">&#39;../certificates/proxy_cert.pem&#39;</span><span class="p">,</span><span class="nx">encoding</span><span class="o">=</span><span class="s1">&#39;ascii&#39;</span><span class="p">)</span> <span class="p">],</span>
</span><span class='line'>    <span class="nx">checkServerIdentity</span><span class="o">:</span> <span class="p">()</span> <span class="o">=&gt;</span> <span class="p">{</span> <span class="k">return</span> <span class="kc">null</span><span class="p">;</span> <span class="p">},</span>
</span><span class='line'><span class="p">};</span>
</span><span class='line'>
</span><span class='line'><span class="kd">var</span> <span class="nx">client</span> <span class="o">=</span> <span class="nx">redis</span><span class="p">.</span><span class="nx">createClient</span><span class="p">(</span><span class="mi">12000</span><span class="p">,</span><span class="s1">&#39;127.0.0.1&#39;</span><span class="p">,</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>      <span class="nx">password</span> <span class="o">:</span> <span class="s1">&#39;secretdb01&#39;</span><span class="p">,</span>
</span><span class='line'>      <span class="nx">tls</span><span class="o">:</span> <span class="nx">ssl</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'><span class="p">);</span>
</span><span class='line'>
</span><span class='line'><span class="nx">client</span><span class="p">.</span><span class="nx">info</span><span class="p">(</span> <span class="s2">&quot;SERVER&quot;</span><span class="p">,</span> <span class="kd">function</span> <span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">reply</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>    <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">reply</span><span class="p">);</span>
</span><span class='line'><span class="p">}</span> <span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>More information in the documentation &ldquo;<a href="https://redislabs.com/lp/node-js-redis/">Using Redis with Node.js</a>&rdquo;.</p>

<h5>4.4 Using Java</h5>

<p>In Java, to be able to connect using SSL, you have to install all the certificates in the Java environment using the <a href="https://docs.oracle.com/en/java/javase/11/tools/keytool.html">keytool</a> utility.</p>

<p>Create a <strong>keystore</strong> file that stores the key and certificate you have created earlier:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">openssl</span> <span class="nx">pkcs12</span> <span class="o">-</span><span class="kr">export</span> <span class="o">\</span>
</span><span class='line'>  <span class="o">-</span><span class="k">in</span> <span class="p">.</span><span class="o">/</span><span class="nx">client_cert_app_001</span><span class="p">.</span><span class="nx">pem</span> <span class="o">\</span>
</span><span class='line'>  <span class="o">-</span><span class="nx">inkey</span> <span class="p">.</span><span class="o">/</span><span class="nx">client_key_app_001</span><span class="p">.</span><span class="nx">pem</span> <span class="o">\</span>
</span><span class='line'>  <span class="o">-</span><span class="nx">out</span> <span class="nx">client</span><span class="o">-</span><span class="nx">keystore</span><span class="p">.</span><span class="nx">p12</span> <span class="o">\</span>
</span><span class='line'>  <span class="o">-</span><span class="nx">name</span> <span class="s2">&quot;APP_01_P12&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you can see the keystore is used to store the credentials associated with you client; it will be used later with the <code>-javax.net.ssl.keyStore</code> system property in the Java application.</p>

<p>In addition to the keys tore, you also have to create a trust store, that is used to store other credentials for example in our case the redis cluster certificate.</p>

<p>Create a <strong>trust store</strong> file and add the Redis cluster certificate to it</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">keytool</span> <span class="o">-</span><span class="nx">genkey</span> <span class="o">\</span>
</span><span class='line'>  <span class="o">-</span><span class="nx">dname</span> <span class="s2">&quot;cn=CLIENT_APP_01&quot;</span> <span class="o">\</span>
</span><span class='line'>  <span class="o">-</span><span class="nx">alias</span> <span class="nx">truststorekey</span> <span class="o">\</span>
</span><span class='line'>  <span class="o">-</span><span class="nx">keyalg</span> <span class="nx">RSA</span> <span class="o">\</span>
</span><span class='line'>  <span class="o">-</span><span class="nx">keystore</span> <span class="p">.</span><span class="o">/</span><span class="nx">client</span><span class="o">-</span><span class="nx">truststore</span><span class="p">.</span><span class="nx">p12</span> <span class="o">\</span>
</span><span class='line'>  <span class="o">-</span><span class="nx">keypass</span> <span class="nx">secret</span>
</span><span class='line'>  <span class="o">-</span><span class="nx">storepass</span> <span class="nx">secret</span>
</span><span class='line'>  <span class="o">-</span><span class="nx">storetype</span> <span class="nx">pkcs12</span>
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">keytool</span> <span class="o">-</span><span class="kr">import</span> <span class="o">\</span>
</span><span class='line'>  <span class="o">-</span><span class="nx">keystore</span> <span class="p">.</span><span class="o">/</span><span class="nx">client</span><span class="o">-</span><span class="nx">truststore</span><span class="p">.</span><span class="nx">p12</span> <span class="o">\</span>
</span><span class='line'>  <span class="o">-</span><span class="nx">file</span> <span class="p">.</span><span class="o">/</span><span class="nx">proxy_cert</span><span class="p">.</span><span class="nx">pem</span> <span class="o">\</span>
</span><span class='line'>  <span class="o">-</span><span class="nx">alias</span> <span class="nx">redis</span><span class="o">-</span><span class="nx">cluster</span><span class="o">-</span><span class="nx">crt</span>
</span></code></pre></td></tr></table></div></figure>


<p>The trustore will be used later with the <code>-javax.net.ssl.trustStore</code> system property in the Java application.</p>

<p>You can now run the Java application with the following environment variables:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">java</span> <span class="o">-</span><span class="nx">Djavax</span><span class="p">.</span><span class="nx">net</span><span class="p">.</span><span class="nx">ssl</span><span class="p">.</span><span class="nx">keyStore</span><span class="o">=</span><span class="err">/path_to/certificates/java/client-keystore.p12 \</span>
</span><span class='line'><span class="o">-</span><span class="nx">Djavax</span><span class="p">.</span><span class="nx">net</span><span class="p">.</span><span class="nx">ssl</span><span class="p">.</span><span class="nx">keyStorePassword</span><span class="o">=</span><span class="nx">secret</span> <span class="o">\</span>
</span><span class='line'><span class="o">-</span><span class="nx">Djavax</span><span class="p">.</span><span class="nx">net</span><span class="p">.</span><span class="nx">ssl</span><span class="p">.</span><span class="nx">trustStore</span><span class="o">=</span><span class="err">/path_to/certificates/java/client-truststore.p12 \</span>
</span><span class='line'><span class="o">-</span><span class="nx">Djavax</span><span class="p">.</span><span class="nx">net</span><span class="p">.</span><span class="nx">ssl</span><span class="p">.</span><span class="nx">trustStorePassword</span><span class="o">=</span><span class="nx">secret</span> <span class="o">\</span>
</span><span class='line'><span class="o">-</span><span class="nx">jar</span> <span class="nx">MyApp</span><span class="p">.</span><span class="nx">jar</span>
</span></code></pre></td></tr></table></div></figure>


<p>For this example and simplicity, I will hard code these property in the Java code itself:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kn">import</span> <span class="nn">redis.clients.jedis.Jedis</span><span class="o">;</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">java.net.URI</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">SSLTest</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">System</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">&quot;javax.net.ssl.keyStore&quot;</span><span class="o">,</span> <span class="s">&quot;/path_to/certificates/client-keystore.p12&quot;</span><span class="o">);</span>
</span><span class='line'>        <span class="n">System</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">&quot;javax.net.ssl.keyStorePassword&quot;</span><span class="o">,</span> <span class="s">&quot;secret&quot;</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">System</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">&quot;javax.net.ssl.trustStore&quot;</span><span class="o">,</span><span class="s">&quot;/path_to/certificates/client-truststore.p12&quot;</span><span class="o">);</span>
</span><span class='line'>        <span class="n">System</span><span class="o">.</span><span class="na">setProperty</span><span class="o">(</span><span class="s">&quot;javax.net.ssl.trustStorePassword&quot;</span><span class="o">,</span><span class="s">&quot;secret&quot;</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">URI</span> <span class="n">uri</span> <span class="o">=</span> <span class="n">URI</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="s">&quot;rediss://127.0.0.1:12000&quot;</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">Jedis</span> <span class="n">jedis</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">Jedis</span><span class="o">(</span><span class="n">uri</span><span class="o">);</span>
</span><span class='line'>        <span class="n">jedis</span><span class="o">.</span><span class="na">auth</span><span class="o">(</span><span class="s">&quot;secretdb01&quot;</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>        <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="n">jedis</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">&quot;SERVER&quot;</span><span class="o">));</span>
</span><span class='line'>        <span class="n">jedis</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>line 8-12, the system environment variables are set to point to the keystore and trust store (this should be externalized)</li>
<li>line 14, the Redis URL start with <code>rediss</code> with 2 s to indicate that the connection should be encrypted</li>
<li>line 17, set the database password</li>
</ul>


<p>More information in the documentation &ldquo;<a href="https://redislabs.com/lp/redis-java/">Using Redis with Java</a>&rdquo;.</p>

<h2>Conclusion</h2>

<p>In this article, you have learned how to:</p>

<ul>
<li>retrieve the Redis Server certificate</li>
<li>generate a client certificate</li>
<li>protect your database to enforce transport level security (TLS) with 2 ways authentication</li>
<li>connect to the database from <code>redis-cli</code>, Python, Node and Java</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Redis Rolling Upgrade on Pivotal Cloud Foundry (PCF)]]></title>
    <link href="http://tgrall.github.io/blog/2019/09/19/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/"/>
    <updated>2019-09-19T05:05:23+02:00</updated>
    <id>http://tgrall.github.io/blog/2019/09/19/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf</id>
    <content type="html"><![CDATA[<h3>Introduction</h3>

<p>In this article, I will show you how to update Redis Enterprise on PCF and see how Redis Enterprise cluster will guarantee a service continuity using out of the box failover.</p>

<p>If you need a Cloud Foundry application that calls Redis automatically you can use this <a href="https://github.com/tgrall/simple-redis-spring-demo-pcf">project simple-redis-spring-demo-pcf</a>.</p>

<p>For this article, I will upgrade <a href="https://docs.pivotal.io/partners/redis-labs-enterprise-pack/index.html">Redis Enterprise for PCF</a> from the version v5.4.2400147 to the latest version, currently v5.4.40700169.</p>

<!--more -->


<p><strong>Prerequisites</strong></p>

<ul>
<li>Pivotal Cloud Foundry up &amp; running

<ul>
<li>Administrator access to Ops Manager and Apps Manager</li>
</ul>
</li>
<li>One of more Redis databases running on PCF

<ul>
<li>My environment has2 databases in version v5.4.2400147</li>
<li>One wit replication (<code>db:4</code>) another one without replication (<code>db:5</code>)</li>
</ul>
</li>
</ul>


<h3>Initial Environment</h3>

<p>Let&rsquo;s take a look to the environment before the update; for this you can access the Redis Enterprise Cluster Management Console:</p>

<ul>
<li><a href="https://">https://</a>[Cluster Management Console Subdomain].[System Domain]</li>
<li>for example <a href="https://console-redis.sys.my-domain.cf-app.com">https://console-redis.sys.my-domain.cf-app.com</a> .</li>
</ul>


<blockquote><p>Do not use this to create/delete a database, you must use Cloud Foundry to do it. (<code>cf</code> command or UI)</p></blockquote>

<p>In the Web console, go to &ldquo;Cluster&rdquo; then &ldquo;Configuration&rdquo;, you can see the version of Redis Labs Enterprise Cluster (5.4.0-24), and Redis (5.0.2) versions.</p>

<p><img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/img-001-webui-cluster-version.png"></p>

<p>You can also use the <code>rladmin</code> command line to achieve this.</p>

<p><strong>Checking Redis cluster using the command line</strong></p>

<p>SSH to your Ops Manager and, <code>bosh ssh</code> to one of the Redis cluster VMs.</p>

<p>When I run the <code>bosh vms</code> command on my environment I can see the following VMs related to my Redis deployment:</p>

<p><img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/img-002-redis-vms-list.png"></p>

<p>The deployment is made of 5 VMs:</p>

<ul>
<li>the 3 first VMs are the Redis Nodes</li>
<li>the 2 others are related to the PCF integration (Registrar and Service Broker)</li>
</ul>


<p>We can look in more details into the role of each VMS in the cluster, for this I will <code>bosh ssh</code> into one of the nodes:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ bosh -d redis-enterprise-[your-deployment-id] ssh redis-enterprise-node/[your-vm-id]</span></code></pre></td></tr></table></div></figure>


<p>Once connected use the <code>sudo rladmin status</code> to look at the Redis cluster deployed on PCF.</p>

<p><img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/img-003-rladmin-view.png"></p>

<p>In this cluster you see:</p>

<ul>
<li>in the <strong><em>Cluster Nodes</em></strong> section that we have 3 nodes in version 5.4.0-24</li>
<li>in the <strong><em>Databases</em></strong> section that we have 2 database instances, the name is generated by Cloud Foundry. In this environment, the <code>db:4</code> is replicated with shards on <code>node:1</code> (master) and <code>node:2</code> (slave/replica), while <code>db:5</code> is not replicated.</li>
</ul>


<p>Let&rsquo;s now see the Redis version of the databases using:</p>

<ul>
<li><code>sudo rladmin status databases extra redis_version</code></li>
</ul>


<p>As expected the version if 5.0.2, the same value that you have seen in the Web console.</p>

<h2>Installing the latest version of Redis Enterprise for PCF</h2>

<p>Once the latest release of Redis Enterprise on PCF is imported, the upgrade is easy to do:</p>

<ol>
<li>Click on &ldquo;Redis Enterprise on PCF&rdquo; in the left menu.</li>
<li>Click on the &ldquo;<strong>+</strong>&rdquo; link.

<ul>
<li>The tiles is updated to the new version, you can review the configuration, not needed in this tutorial.</li>
</ul>
</li>
<li><p>Click on &ldquo;Review Pending Changes&rdquo; button.
 <img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/img-004-tile-update.png"></p></li>
<li><p>Unselect all product except Redis</p></li>
<li>Click Apply Changes</li>
</ol>


<p>Once you have clicked the update process will start, and you can follow the progress using the log information.</p>

<p>Nevertheless, it is interesting to see what is happening behind the scene using the command line on the VMS.</p>

<p>The update process using PCF will do the following:</p>

<ul>
<li>Update and restart each node one by one (the 5 nodes of the Redis Enterprise deployment)</li>
<li>during these steps, Redis Cluster will fail over moving the master and endpoint to another node to provide service continuity to the applications.</li>
</ul>


<p>Let&rsquo;s look at the following screenshots to see how the rolling upgrade was done by PCF.</p>

<h4>Starting Point</h4>

<p>The cluster is up and running with 3 nodes with the version 5.4.0-24, and the <code>node:1</code> is the master of the cluster</p>

<p><strong><em>Cluster Nodes:</em></strong></p>

<p><img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/001-rladmin-cluster-version.png"></p>

<p><strong><em>Endpoints:</em></strong></p>

<p><img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/001-rladmin-cluster-version-endpoint.png"></p>

<p>The <code>node:1</code> is also the endpoint for the <code>db:4</code></p>

<p><strong><em>Shards:</em></strong></p>

<p><img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/001-rladmin-cluster-db.png"></p>

<p>You can see 3 shards in this deployment:</p>

<ul>
<li><code>db:4</code> is replicated and has 2 shards the master on <code>node:1</code> and a replica on <code>node:2</code>, the failover will automatically happen with no data loss.</li>
<li><code>db:5</code> is not replicated and has a single shard, so the database will be recreated fresh on a new node during the update.</li>
</ul>


<p>So if you want to have a full service continuity with no data loss it is mandatory to use replication.</p>

<h4>PCF Updating Node 1</h4>

<p>PCF has now started the process and stopped the <code>node:1</code>.</p>

<p><strong><em>Cluster Nodes:</em></strong></p>

<p><img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/002-rladmin-cluster-status.png"></p>

<p>All the nodes are still on the &ldquo;old version&rdquo;, but the cluster master has been moved now to <code>node:2</code>; so applications will continue to work.</p>

<p>The errors are here to indicate that the <code>node:1</code> is not accessible, and the <code>node:3</code> also raised an error since the replication link is not available.</p>

<p><strong><em>Endpoints:</em></strong></p>

<p><img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/002-rladmin-cluster-version-endpoint.png"></p>

<p>Here we see that the <code>db:4</code> endpoint, now on <code>node:2</code>, Redis Enterprises cluster manager has moved the endpoint to this node automatically.</p>

<p><strong><em>Shards:</em></strong></p>

<p><img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/002-rladmin-cluster-db.png"></p>

<ul>
<li><code>db:4</code> is up and the master shard has been moved from <code>node:1</code> to <code>node:2</code></li>
<li><code>db:5</code> is not present anymore, a new master will be created automatically on <code>node:3</code>, but empty.</li>
</ul>


<p>The fail over is done transparently with no impact for the application.</p>

<h4>PCF is restarting the updated Node 1</h4>

<p>Once the node:1 VM is restarted with the updated version of Redis Enterprise you can see the new version number and status.</p>

<p><strong><em>Cluster Nodes:</em></strong></p>

<p><img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/003-rladmin-cluster-version.png"></p>

<p>The 3 nodes of the cluster are up and running, and you can see that the <code>node:1</code> has been updated to the new version 5.4.4-7.</p>

<p>The master is still the <code>node:2</code></p>

<p>For a short time the cluster will have heterogeneous nodes, this is not an issue.</p>

<p><strong><em>Shards:</em></strong></p>

<p><img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/003-rladmin-cluster-db.png"></p>

<p>You can see that the <code>db:4</code> shards have the status <code>OK, OLD VERSION</code> that indicates that:</p>

<ul>
<li>the database is up and running</li>
<li>but the database itself has not yet been updated to the latest Redis version</li>
</ul>


<p>The update of the database is done automatically, so after a while, if you run the command <code>sudo rladmin status databases extra redis_version</code> you will see something like:</p>

<p><img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/003-rladmin-cluster-db-version.png"></p>

<h4>Updating all the nodes</h4>

<p>The PCF update will continue and upgrade:</p>

<ul>
<li><code>node:2</code>, Redis Cluster will move the masters (cluster, shard, endpoint) to another node, in our case <code>node:1</code> for the replicated database (<code>db:4</code>)</li>
<li>once the <code>node:2</code> is done the same work will be done on node 3.</li>
</ul>


<p><strong><em>Cluster Nodes:</em></strong></p>

<p>All the nodes of the clusters are now updated to the latest version of Redis Enterprise (5.4.4-7) supported on PCF.</p>

<p><img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/004-rladmin-cluster-version.png"></p>

<p><strong><em>Shards:</em></strong></p>

<p><img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/004-rladmin-cluster-db.png"></p>

<p>The update of the database is done automatically, so after a while if your run the command <code>sudo rladmin status databases extra redis_version</code> you will see something like:</p>

<p><img class="center" src="http://tgrall.github.io/images/posts/redis-rolling-upgrade-on-pivotal-cloud-foundry-pcf/steps/004-rladmin-cluster-db-version.png"></p>

<blockquote><p>In this example I am doing a “minor upgrade”, from Redis Cluster 5.4.0/Redis 5.0.2 to Redis Cluster 5.4.4/Redis 5.0.4, and everything is done automatically.</p>

<p>If you are doing a major upgrade for example from 4.x to 5.x, the cluster will automatically be updated to the proper release, but you will have to manually update the existing databases as documented here.</p></blockquote>

<h4>Updating Redis on PCF Services</h4>

<p>During the update, you will see other VMs stopped and started in the process. These VMs are used for:</p>

<ul>
<li>Redis Registrar</li>
<li>ResisLabs Service Broker</li>
</ul>


<p>These services and nodes are not part of the &ldquo;Redis Enterprise&rdquo; per se, but are part of the integration with PCF.</p>

<h2>Conclusion</h2>

<p>The update of the Redis Cluster is now complete:</p>

<ul>
<li>All the nodes are on 5.4.4-7 (from 5.4.0-024)</li>
<li>All the databases have been updated to the new Redis 5.0.4 (from 5.0.2)</li>
</ul>


<p>The upgrade has been done automatically without any interruptions of service:</p>

<ul>
<li>PCF scripts have been responsible for upgrading, stoping and starting each part of the installation in the correct order</li>
<li>while Redis Enterprise Cluster has been responsible for keeping the databases available for the applications, during the process.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Multi-Nodes Redis Cluster With Docker]]></title>
    <link href="http://tgrall.github.io/blog/2019/09/05/multi-nodes-redis-cluster-with-docker/"/>
    <updated>2019-09-05T11:33:56+02:00</updated>
    <id>http://tgrall.github.io/blog/2019/09/05/multi-nodes-redis-cluster-with-docker</id>
    <content type="html"><![CDATA[<p>As part of my on-boarding/training at RedisLabs I continue to play with the product, and I have decided today to install a local 3 nodes cluster of Redis Enterprise Software (RS); and show how easy is to move from a single node/shard database to a multi nodes highly available one.</p>

<p>Once your cluster is up &amp; running, you will kill some containers to see how the system automatically fail-over to guarantee service continuity.</p>

<p>The deployment will look more or less like the schema below, (<em><a href="https://docs.redislabs.com/latest/rs/getting-started/docker/">coming from RedisLabs documentation</a></em>)</p>

<p><img class="center" src="http://tgrall.github.io/images/posts/multi-nodes-redis-cluster-with-docker/docker-deployment.png"></p>

<p>This is a perfect environment for learning, developing and testing your applications, but it is not supported in production; for production, you can use:</p>

<ul>
<li><a href="https://redislabs.com/redis-enterprise/pro/">Redis Cloud</a></li>
<li><a href="https://docs.redislabs.com/latest/platforms/openshift/">Redis Enterprise Software with Kubernetes and Red Hat OpenShift</a></li>
<li><a href="https://docs.redislabs.com/latest/platforms/pks/">Redis Enterprise Software with Kubernetes Operator on PKS (Pivotal Container Service)</a></li>
<li><a href="https://docs.redislabs.com/latest/platforms/pcf/">Redis Enterprise for Pivotal Cloud Foundry (PCF)</a>.</li>
</ul>


<!--more-->


<p><strong>Prerequisites:</strong></p>

<ul>
<li>Docker Desktop (<em>I am running Docker on Mac</em>)</li>
</ul>


<h3>Installing and Running your First Redis Node</h3>

<p>As usual, installing a new product with Docker is very simple just run the following command:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>docker run -d --cap-add sys_resource <span class="se">\</span>
</span><span class='line'>--name  redis-node1 <span class="se">\</span>
</span><span class='line'>-p 8443:8443 <span class="se">\</span>
</span><span class='line'>-p 9443:9443 <span class="se">\</span>
</span><span class='line'>-p 12000:12000 <span class="se">\</span>
</span><span class='line'>redislabs/redis
</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s look at the parameters used here:</p>

<ul>
<li><code>-d</code>: run the container in the background</li>
<li><code>--cap-add sys_resource</code>: add Linux  <code>sys_resource</code>capabilities to set proper privileges</li>
<li><code>--name  redis-node1</code>: naming the container</li>
<li><code>-p 8443:8443</code>: to access the management web UI (HTTPS)</li>
<li><code>-p 9443:9443</code>: to access the REST API (HTTPS)</li>
<li><code>-p 12000:12000</code>: the TCP port that we will use for the database endpoint on this node</li>
<li><code>redislabs/redis</code>: use the RedisLabs image (the enterprise version of Redis)</li>
</ul>


<h4>Creating a new Cluster</h4>

<p>Once the container is started you can configure the &ldquo;cluster&rdquo;.</p>

<ol>
<li>Go top <a href="https://localhost:8443/">https://localhost:8443/</a> (accept the connect using the temporary certificate)</li>
<li>Click &ldquo;Setup&rdquo;</li>
<li>Change the Cluster Name to &ldquo;my-redis-cluster.tug-demo.com&rdquo;</li>
<li>Click &ldquo;Next&rdquo;</li>
<li>On the &ldquo;cluster authentication&rdquo; click &ldquo;Next&rdquo;  <em>(we will be using the free version)</em></li>
<li>Enter the user admin credentials and click &ldquo;Next&rdquo;.</li>
</ol>


<p>Once it is configured, connect to the console to the console using the credentials you have created.</p>

<h4>Adding a new database</h4>

<p>Now you have to create a new database.</p>

<ol>
<li>Select &ldquo;Redis Database&rdquo; and &ldquo;Single Region&rdquo;</li>
<li>Enter the name &ldquo;test-db-001&rdquo;, and &ldquo;0.5&rdquo; for the memory limit</li>
<li>Click &ldquo;Show Advanced Options&rdquo;</li>
<li>Enter 12000 in the &ldquo;Endpoint port number&rdquo; field</li>
<li>Click &ldquo;Activate&rdquo;.</li>
</ol>


<p>After  fewseconds, the database is created and available.</p>

<p>Note: we have not set anything special around clustering and replication; we will do that later.</p>

<h4>Using the Single Node Database</h4>

<p>You can now connect to the database. You can use  <code>redis-cli</code>from your host, or you can connect to the container and do it from there:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&gt; docker  <span class="nb">exec</span>-it redis-node1 /bin/bash
</span><span class='line'>
</span><span class='line'>redislabs@0a174e819a6b:/opt<span class="nv">$ </span>redis-cli -p 12000
</span><span class='line'>
</span><span class='line'>127.0.0.1:12000&gt; SET foo bar
</span><span class='line'>OK
</span><span class='line'>
</span><span class='line'>127.0.0.1:12000&gt; GET foo
</span><span class='line'><span class="s2">&quot;bar&quot;</span>
</span><span class='line'>
</span><span class='line'>127.0.0.1:12000&gt;  <span class="nb">exit</span>
</span></code></pre></td></tr></table></div></figure>


<p><strong><em>Checkpoint</em></strong></p>

<p>So far you have:</p>

<ol>
<li>Install a single node cluster of Redis Enterprise using Docker</li>
<li>Create a new cluster</li>
<li>Created a database that listens on port 12000.</li>
</ol>


<p>In the container, run the  <code>rladminstatus</code>command, to get information about your deployment.</p>

<p><img class="center" src="http://tgrall.github.io/images/posts/multi-nodes-redis-cluster-with-docker/rladmin-status.png"></p>

<p>In the cluster node section, you can see the address of the node, 172.17.0.2 in my case. This is the IP address of the container, that will be used to create the multi-node cluster.</p>

<p>It is time to add new nodes to the cluster and enable replication and sharding</p>

<h3>Adding new nodes</h3>

<p>To add new nodes to the cluster, you start new containers. Since the 3 containers will be running on the same host, it is necessary, to avoid conflicts, to use different mapping to the Web UI, REST API, and database endpoint ports.</p>

<p><strong>Start node 2:</strong></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>docker run -d --cap-add sys_resource <span class="se">\</span>
</span><span class='line'>--name redis-node2 <span class="se">\</span>
</span><span class='line'>-p 8444:8443 <span class="se">\</span>
</span><span class='line'>-p 9444:9443 <span class="se">\</span>
</span><span class='line'>-p 12001:12000 <span class="se">\</span>
</span><span class='line'>redislabs/redis
</span></code></pre></td></tr></table></div></figure>


<p><strong>Start node 3:</strong></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>docker run -d --cap-add sys_resource <span class="se">\</span>
</span><span class='line'>--name redis-node3 <span class="se">\</span>
</span><span class='line'>-p 8445:8443 <span class="se">\</span>
</span><span class='line'>-p 9445:9443 <span class="se">\</span>
</span><span class='line'>-p 12002:12000 <span class="se">\</span>
</span><span class='line'>redislabs/redis
</span></code></pre></td></tr></table></div></figure>


<p>So to configure each node you need to use the URLs:</p>

<ul>
<li>node 2: <a href="https://localhost:8444/">https://localhost:8444/</a></li>
<li>node 3: <a href="https://localhost:8445/">https://localhost:8445/</a></li>
</ul>


<p>I have just increase the port number of the Web UI (8443: node 1, 8444: node 2, 8445 node 3).</p>

<p>For these 2 new nodes, do the following steps to add them to the cluster:</p>

<ol>
<li>Click &ldquo;Setup&rdquo;</li>
<li>In  clusterconfiguration, select &ldquo;Join Cluster&rdquo;,

<ul>
<li>Enter the IP address of the first node, 172.17.0.2 in my environment</li>
<li>Enter the credentials you have used during the installation of the first node.</li>
</ul>
</li>
<li>Click &ldquo;Next&rdquo;</li>
</ol>


<p>After a few seconds, you will be redirected to the home page and see the list of nodes of your cluster.</p>

<p>Repeat the same steps for the third node.</p>

<p>Your environment should look like this after the installation and configuration of the 3 nodes.</p>

<p><img class="center" src="http://tgrall.github.io/images/posts/multi-nodes-redis-cluster-with-docker/cluster-view-3-nodes.png"></p>

<p>You can also reuse the  <code>rladminstatus</code>command on one of the containers and see the new configuration.</p>

<p>If you look carefully, you can see that you have only 1 shard in your cluster. Let&rsquo;s now add a new shard to the database.</p>

<p><strong>Enabling Clustering and Replication to the DB</strong></p>

<p>In the Redis Enterprise Admin Web UI, (<em>you can use any of the nodes</em>):</p>

<ol>
<li>Click on the &ldquo;databases&rdquo; tab</li>
<li>Click on &ldquo;test-db-001&rdquo; database</li>
<li>Click on the &ldquo;configuration&rdquo;</li>
<li>Go to the bottom of the page and click &ldquo;Edit&rdquo;</li>
<li>Check the &ldquo;Replication&rdquo; checkbox, to create new shard that will be a replica, to provide high availability</li>
<li>Check &ldquo;Database Clustering&rdquo; and increase the number of shards to 2. This will  <em>distribute</em>the data in your database into 2 shards, this for better scalability.
 <em>You can see that the UI indicated that you have  </em>4 shards with replication*. Yes because you have a database that you have &ldquo;divided in 2&rdquo;, and each of the portions of the database is replicated.
(Also with the free version of Redis Enterprise you are limited to 4 shards, so do not be surprised if you can not increase the number of shards to more than 4)</li>
<li>Click &ldquo;Update&rdquo; at the bottom of the page.</li>
</ol>


<p>Go back to the &ldquo;nodes&rdquo; tab, and you will see that you have now 4 shards distributed on your 3 nodes.</p>

<p><strong>Discovering the cluster topology</strong></p>

<p>Run  <code>rladminstatus</code>to inspect your cluster and see how the various components are installed:</p>

<p><img class="center" src="http://tgrall.github.io/images/posts/multi-nodes-redis-cluster-with-docker/rladmin-status-with-db.png"></p>

<p>For example, you can see, that in my environment:</p>

<p><em>in the &ldquo;CLUSTER NODES&rdquo; section that the &ldquo;node:1&rdquo; is the master of the cluster
</em>in the &ldquo;DATABASES&rdquo; section that replication is enabled, and the database uses a &ldquo;<a href="https://docs.redislabs.com/latest/rs/concepts/rebalancing-shard-placement/#dense-shard-placement-policy">dense placement</a>&rdquo;
<em>in the &ldquo;SHARDS&rdquo; section you can see the various shards and their placement (</em>node:1|2|3<em>), their role (</em>master|slave*) and their slots.</p>

<p>Using Redis Enterprise Enterprise Software (RS), all the clustering is managed transparently for you, and your applications. This means that you just have to connect your application to RS Cluster.</p>

<p><strong>Clustering in Action</strong></p>

<p>First of all, you have already seen a lot, just using the Web UI (and you could have done it using CLI and REST API), you have moved an existing database from a single instance to a distributed and highly available instance.</p>

<p>So now if something happens to the system, for example, if one of the masters disappears RS will automatically get another  oneelected.</p>

<p>Let me kill for example the node 3 that contains the 2 masters for my database.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&gt; docker  killredis-node3
</span></code></pre></td></tr></table></div></figure>


<p>After a few seconds, you should see that the master shards are now on another node, in my case node:1.</p>

<p><img class="center" src="http://tgrall.github.io/images/posts/multi-nodes-redis-cluster-with-docker/rladmin-status-with-db-002.png"></p>

<p>So if an application, is using this cluster it would be almost transparent as the election of the new master is happening in the  backgroud.</p>

<p>If you restart the node 3 it will rejoin the cluster, and the replicas will be updated on node 3 with any changes that happened to the masters.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&gt; docker start  redis-node3
</span></code></pre></td></tr></table></div></figure>


<p>The same automatic fail-over will happen if you kill a node with the cluster  manager,or the endpoint.</p>

<h4>Conclusion</h4>

<p>In this small article you have learned how to:</p>

<ul>
<li>deploy a 3 nodes Redis Enterprise Server (RS) on Docker (on a single host)</li>
<li>create a database, and make it highly available and distributed easily using the Admin UI</li>
<li>look at the deployment using  <code>rladminstatus</code>command.</li>
</ul>


<p>You have also seen, by killing some nodes, how the cluster fail-over will various master services (shards, endpoint, master cluster node) to another node automatically. This to ensure a continuity of service for your application.</p>

<p>In another  postI will show what is the exact behavior of client applications during the fail-over.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With Redis Streams &amp; Java]]></title>
    <link href="http://tgrall.github.io/blog/2019/09/02/getting-with-redis-streams-and-java/"/>
    <updated>2019-09-02T09:24:24+02:00</updated>
    <id>http://tgrall.github.io/blog/2019/09/02/getting-with-redis-streams-and-java</id>
    <content type="html"><![CDATA[<p>As you may have seen, I have joined <a href="https://www.redislabs.com">Redis Labs</a> a month ago; one of the first task as a new hire is to learn more about Redis. So I learned, and I am still learning.</p>

<p>This is when I discovered <a href="https://redis.io/topics/streams-intro">Redis Streams</a>. I am a big fan of streaming-based applications so it is natural that I start with a small blog post explaining how to use Redis Streams and Java.</p>

<p><strong><em>What is Redis Streams?</em></strong></p>

<p>Redis Streams is a Redis Data Type, that represents a log so you can add new information/message in an append-only mode <em>(this is not 100% accurate since you can remove messages from the log)</em>. Using Redis Streams you can build &ldquo;Kafka Like&rdquo; applications, what I mean by that you can:</p>

<ul>
<li>create applications that publish and consume messages (nothing extraordinary here, you could already do that with Redis Pub/Sub)</li>
<li>consume messages that are published even when your client application (consumer) is not running. This is a big difference with Redis Pub/Sub</li>
<li>consume messages starting a specific offset, for example, read the whole history, or only new messages</li>
</ul>


<p>In addition to this, Redis Streams has the concept of <strong>Consumer Groups</strong>. Redis Streams Consumer Groups, like Apache Kafka ones, allows the client applications to consume messages in a distributed fashion (multiple clients), providing an easy way to scale and create highly available systems.</p>

<p><img class="center" src="http://tgrall.github.io/images/posts/getting-with-redis-streams-and-java/redis-streams-101-img-1.png"></p>

<p>Enroll in the <a href="https://university.redislabs.com/courses/course-v1:redislabs+RU202+2019_03/about">Redis University: Redis Streams</a> to learn more and get certified.</p>

<p><strong><em>Sample Application</em></strong></p>

<p>The <a href="https://github.com/tgrall/redis-streams-101-java">redis-streams-101-java GitHub Repository</a> contains sample code that shows how to</p>

<ul>
<li>post messages to a streams</li>
<li>consume messages using a consumer group</li>
</ul>


<!--more -->


<h4>Prerequisites</h4>

<ul>
<li>Redis 5.x, you have here multiple options:

<ul>
<li><a href="https://redis.io">Download</a> and install Redis Community</li>
<li>Install and Run a Docker image: <a href="https://hub.docker.com/_/redis">Community</a> or <a href="https://hub.docker.com/r/redislabs/redis">Redis Enterprise</a></li>
<li>Create a online instance on <a href="https://redislabs.com/redis-enterprise/essentials/">Redis Labs Cloud</a> (30mb for free)</li>
</ul>
</li>
<li>Java 8 or later</li>
<li>Apache Maven 3.5.x</li>
<li>Git</li>
</ul>


<h3>Java &amp; Redis Streams</h3>

<p>Redis has many Java clients developed by the community, as you can see on the <a href="https://redis.io/clients#java">Redis.io site</a>.</p>

<p>It looks, based on my short experience with Redis so far, that the most complete one around Redis Streams support is <a href="https://lettuce.io">Lettuce</a>, this is the one I will be using in the following code.</p>

<h4>1- Adding Lettuce to Your Maven Project</h4>

<p>Add the following dependency to your project file:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>    <span class="nt">&lt;dependency&gt;</span>
</span><span class='line'>        <span class="nt">&lt;groupId&gt;</span>io.lettuce<span class="nt">&lt;/groupId&gt;</span>
</span><span class='line'>        <span class="nt">&lt;artifactId&gt;</span>lettuce-core<span class="nt">&lt;/artifactId&gt;</span>
</span><span class='line'>        <span class="nt">&lt;version&gt;</span>5.1.8.RELEASE<span class="nt">&lt;/version&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/dependency&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<h4>2- Connecting to Redis</h4>

<p>Import the following classes</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kn">import</span> <span class="nn">io.lettuce.core.*</span><span class="o">;</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">io.lettuce.core.api.StatefulRedisConnection</span><span class="o">;</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">io.lettuce.core.api.sync.RedisCommands</span><span class="o">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Then connect with:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">RedisClient</span> <span class="n">redisClient</span> <span class="o">=</span> <span class="n">RedisClient</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="s">&quot;redis://password@host:port&quot;</span><span class="o">);</span> <span class="c1">// change to reflect your environment</span>
</span><span class='line'><span class="n">StatefulRedisConnection</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">connection</span> <span class="o">=</span> <span class="n">redisClient</span><span class="o">.</span><span class="na">connect</span><span class="o">();</span>
</span><span class='line'><span class="n">RedisCommands</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">syncCommands</span> <span class="o">=</span> <span class="n">connection</span><span class="o">.</span><span class="na">sync</span><span class="o">();</span>
</span></code></pre></td></tr></table></div></figure>


<p>When your application is done with the connection you should disconnect with the following code:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">connection</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
</span><span class='line'><span class="n">redisClient</span><span class="o">.</span><span class="na">shutdown</span><span class="o">();</span>
</span></code></pre></td></tr></table></div></figure>


<h4>3- Sending Message to Streams</h4>

<p>Once you have a connection you can send a message. In this example, I will let Redis generate the message ID, which is time-based, and the body will be built using a Map representing IoT data, for example, a weather data capturing Wind speed and direction in real-time.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">RedisClient</span> <span class="n">redisClient</span> <span class="o">=</span> <span class="n">RedisClient</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="s">&quot;redis://localhost:6379&quot;</span><span class="o">);</span> <span class="c1">// change to reflect your environment</span>
</span><span class='line'>    <span class="n">StatefulRedisConnection</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">connection</span> <span class="o">=</span> <span class="n">redisClient</span><span class="o">.</span><span class="na">connect</span><span class="o">();</span>
</span><span class='line'>    <span class="n">RedisCommands</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">syncCommands</span> <span class="o">=</span> <span class="n">connection</span><span class="o">.</span><span class="na">sync</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">messageBody</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HashMap</span><span class="o">&lt;&gt;();</span>
</span><span class='line'>    <span class="n">messageBody</span><span class="o">.</span><span class="na">put</span><span class="o">(</span> <span class="s">&quot;speed&quot;</span><span class="o">,</span> <span class="s">&quot;15&quot;</span> <span class="o">);</span>
</span><span class='line'>    <span class="n">messageBody</span><span class="o">.</span><span class="na">put</span><span class="o">(</span> <span class="s">&quot;direction&quot;</span><span class="o">,</span> <span class="s">&quot;270&quot;</span> <span class="o">);</span>
</span><span class='line'>    <span class="n">messageBody</span><span class="o">.</span><span class="na">put</span><span class="o">(</span> <span class="s">&quot;sensor_ts&quot;</span><span class="o">,</span> <span class="n">String</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="n">System</span><span class="o">.</span><span class="na">currentTimeMillis</span><span class="o">())</span> <span class="o">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">String</span> <span class="n">messageId</span> <span class="o">=</span> <span class="n">syncCommands</span><span class="o">.</span><span class="na">xadd</span><span class="o">(</span>
</span><span class='line'>            <span class="s">&quot;weather_sensor:wind&quot;</span><span class="o">,</span>
</span><span class='line'>            <span class="n">messageBody</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span> <span class="n">String</span><span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;Message %s : %s posted&quot;</span><span class="o">,</span> <span class="n">messageId</span><span class="o">,</span> <span class="n">messageBody</span><span class="o">)</span> <span class="o">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">connection</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
</span><span class='line'>    <span class="n">redisClient</span><span class="o">.</span><span class="na">shutdown</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let me explain the code:</p>

<ul>
<li>Lines 3-5 are used to connect to Redis</li>
<li>Lines 7-10 are used to create the message body, using a Map, since Redis Streams messages are string key/values.</li>
<li>Lines 12-14 call the <code>syncCommands.xadd()</code> method using the streams key &ldquo;weather_sensor:wind&rdquo; and the message body itself

<ul>
<li>this method returns the message ID.</li>
</ul>
</li>
<li>line 16 just print the message ID and content</li>
<li>the lines 18-19 close the connection and client.</li>
</ul>


<p>The complete producer code is available <a href="https://github.com/tgrall/redis-streams-101-java/blob/master/src/main/java/com/kanibl/redis/streams/simple/RedisStreams101Producer.java">here</a>.</p>

<h4>4- Consuming Messages</h4>

<p>Redis Streams offers various way to consume/read messages using the commands: <a href="https://redis.io/commands/xrange">XRANGE</a>, <a href="https://redis.io/commands/xrevrange">XREVRANGE</a>, <a href="https://redis.io/commands/xread">XREAD</a>, <a href="https://redis.io/commands/xreadgroup">XREADGROUP</a>.</p>

<p>I want to keep the article short and close to the way you would build an application with Apache Kafka, this is why I will use the <a href="https://redis.io/commands/xreadgroup">XREADGROUP</a> command from Lettuce.</p>

<p>The Consumer Groups allow developers to create a group of clients that will cooperate to consume messages from the streams (for scale and high availability); it is also a way to associate the client to specific applications roles; for example:</p>

<ul>
<li>a consumer group called &ldquo;data warehouse&rdquo; will consume messages and send them to a data warehouse</li>
<li>another consumer group called &ldquo;aggregator&rdquo; will consume the messages and aggregate the data and send them to another sink (another stream or storage)</li>
</ul>


<p>Each of this group will act independently, and each of this group could have multiple &ldquo;consumers&rdquo; (client).</p>

<p>Let&rsquo;s see how you use this in Java.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="o">...</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">try</span> <span class="o">{</span>
</span><span class='line'>            <span class="c1">// WARNING: Streams must exist before creating the group</span>
</span><span class='line'>            <span class="c1">//          This will not be necessary in Lettuce 5.2, see https://github.com/lettuce-io/lettuce-core/issues/898</span>
</span><span class='line'>            <span class="n">syncCommands</span><span class="o">.</span><span class="na">xgroupCreate</span><span class="o">(</span> <span class="n">XReadArgs</span><span class="o">.</span><span class="na">StreamOffset</span><span class="o">.</span><span class="na">from</span><span class="o">(</span><span class="s">&quot;weather_sensor:wind&quot;</span><span class="o">,</span> <span class="s">&quot;0-0&quot;</span><span class="o">),</span> <span class="s">&quot;application_1&quot;</span>  <span class="o">);</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>        <span class="k">catch</span> <span class="o">(</span><span class="n">RedisBusyException</span> <span class="n">redisBusyException</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span> <span class="n">String</span><span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">&quot;\t Group &#39;%s already&#39; exists&quot;</span><span class="o">,</span><span class="s">&quot;application_1&quot;</span><span class="o">));</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>        <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">&quot;Waiting for new messages&quot;</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">while</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>            <span class="n">List</span><span class="o">&lt;</span><span class="n">StreamMessage</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;&gt;</span> <span class="n">messages</span> <span class="o">=</span> <span class="n">syncCommands</span><span class="o">.</span><span class="na">xreadgroup</span><span class="o">(</span>
</span><span class='line'>                    <span class="n">Consumer</span><span class="o">.</span><span class="na">from</span><span class="o">(</span><span class="s">&quot;application_1&quot;</span><span class="o">,</span> <span class="s">&quot;consumer_1&quot;</span><span class="o">),</span>
</span><span class='line'>                    <span class="n">XReadArgs</span><span class="o">.</span><span class="na">StreamOffset</span><span class="o">.</span><span class="na">lastConsumed</span><span class="o">(</span><span class="s">&quot;weather_sensor:wind&quot;</span><span class="o">)</span>
</span><span class='line'>            <span class="o">);</span>
</span><span class='line'>
</span><span class='line'>            <span class="k">if</span> <span class="o">(!</span><span class="n">messages</span><span class="o">.</span><span class="na">isEmpty</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>                <span class="k">for</span> <span class="o">(</span><span class="n">StreamMessage</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">String</span><span class="o">&gt;</span> <span class="n">message</span> <span class="o">:</span> <span class="n">messages</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>                    <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="n">message</span><span class="o">);</span>
</span><span class='line'>                    <span class="c1">// Confirm that the message has been processed using XACK</span>
</span><span class='line'>                    <span class="n">syncCommands</span><span class="o">.</span><span class="na">xack</span><span class="o">(</span><span class="n">STREAMS_KEY</span><span class="o">,</span> <span class="s">&quot;application_1&quot;</span><span class="o">,</span>  <span class="n">message</span><span class="o">.</span><span class="na">getId</span><span class="o">());</span>
</span><span class='line'>                <span class="o">}</span>
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="o">...</span>
</span></code></pre></td></tr></table></div></figure>


<p>This code is a subset of the <code>main()</code> method I have removed the connection management part, to add readability. Let&rsquo;s take a look to the code:</p>

<ul>
<li>line 3 to 10, using the method <code>xgroupCreate()</code>, that matches the <a href="https://redis.io/commands/xgroup">XGROUP CREATE</a> command,

<ul>
<li>is used to create a new group called <code>application_1</code>,</li>
<li>consume messages from the stream <code>weather_sensor:wind</code></li>
<li>starting at the first message in the stream, this is indicated using the message ID <code>0-0</code>. <em>Note that it is also possible to indicate to the group to start to read at a specific message ID, or only the new messages that arrive after the creating of the consumer group using <code>$</code> special ID (or the helper method <code>XReadArgs.StreamOffset.latest()</code></em>.</li>
</ul>
</li>
<li>line 15 to 30, in this example we use an infinite loop (<code>while(true)</code>) to wait for any new messages published to the streams</li>
<li>line 17 to 20, the method <code>xreadgroup()</code> returns the messages based on the group configuration

<ul>
<li>line 18 define the consumer named <code>consumer_1</code> that is associated with the group <code>application_1</code>: you can create new group do distribute the read to multiple clients</li>
<li>line 19 indicates where to start, in this case, <code>StreamOffset.lastConsumed("weather_sensor:wind")</code> the consumer will consume messages that have not been read already. With the current configuration of the group (offset <code>0-0</code>), when the consumer will start for the first time, it will read all the existing messages.</li>
</ul>
</li>
<li>line 22 to 28, the application iterates on each messages, and:

<ul>
<li>line 24, process the message, a simple print in this case</li>
<li>line 26, sends a acknowledgment using <code>xack()</code> command. You have to use the ack command to confirm that a message has been read and processed. The <a href="https://redis.io/commands/xack"><code>XACK</code></a> command removes the message from the pending list of the consumer group.</li>
</ul>
</li>
</ul>


<p>The complete consumer code is available <a href="https://github.com/tgrall/redis-streams-101-java/blob/master/src/main/java/com/kanibl/redis/streams/simple/RedisStreams101Consumer.java">here</a>.</p>

<h3>Build &amp; Run the Simple Java Application</h3>

<p>Now that you have a better understanding of the code, let&rsquo;s run the producer and consumer. You can run this from your IDE, or using Maven.</p>

<p>Let&rsquo;s do it using Maven CLI, for this open 2 terminals:</p>

<ul>
<li>one to produce messages</li>
<li>one to consume them</li>
</ul>


<p><em>1- Clone and Build the project</em></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&gt; git clone https://github.com/tgrall/redis-streams-101-java.git
</span><span class='line'>
</span><span class='line'>&gt; <span class="nb">cd </span>redis-streams-101-java
</span><span class='line'>
</span><span class='line'>&gt; mvn clean verify
</span></code></pre></td></tr></table></div></figure>


<p><em>2- Post a new message</em></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&gt; mvn <span class="nb">exec</span>:java -Dexec.mainClass<span class="o">=</span><span class="s2">&quot;com.kanibl.redis.streams.simple.RedisStreams101Producer&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p><em>3- Consume messages</em></p>

<p>Open a new terminal and run the following command:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&gt; mvn <span class="nb">exec</span>:java -Dexec.mainClass<span class="o">=</span><span class="s2">&quot;com.kanibl.redis.streams.simple.RedisStreams101Consumer&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>The consumer will start and consume the message you just posted, and wait for any new messages.</p>

<p><em>4- In the first terminal post 100 new messages</em></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&gt; mvn <span class="nb">exec</span>:java -Dexec.mainClass<span class="o">=</span><span class="s2">&quot;com.kanibl.redis.streams.simple.RedisStreams101Producer&quot;</span> -Dexec.args<span class="o">=</span><span class="s2">&quot;100&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>The consumer will receive and print all the messages.</p>

<p><em>5- Kill the consumer and post more messages</em></p>

<p>Let&rsquo;s now do another test, stop the consumer using a simple <code>ctrl+C</code>.</p>

<p>Then post 5 new messages.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&gt; mvn <span class="nb">exec</span>:java -Dexec.mainClass<span class="o">=</span><span class="s2">&quot;com.kanibl.redis.streams.simple.RedisStreams101Producer&quot;</span> -Dexec.args<span class="o">=</span><span class="s2">&quot;5&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>The messages are not yet consumed by any application, but still store in Redis Streams.</p>

<p>So when you start the consumer, it will consumes these new messages.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&gt; mvn <span class="nb">exec</span>:java -Dexec.mainClass<span class="o">=</span><span class="s2">&quot;com.kanibl.redis.streams.simple.RedisStreams101Consumer&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>This is a one of the differences between <a href="https://redis.io/topics/streams-intro">Redis Streams</a> and <a href="https://redis.io/topics/pubsub">Redis PubSub</a>. The producer application has publish many messages while the consumer application was not running. Since the consumer is ran with <code>StreamOffset.lastConsumed()</code>, when the consumer is starting, it looks to the last consumed ID, and start to read the streams from there. This method generate a XGROUPREAD command with the group</p>

<h3>Conclusion</h3>

<p>In this small project, you have learned, how to use Lettuce, a Java client for Redis to:</p>

<ul>
<li>publish messages to a stream</li>
<li>create a consumer group</li>
<li>consume messages using the consumer group.</li>
</ul>


<p>This is a very basic example, and in a next post I will show you how to work with multiple consumers, and to configure the Consumer Group and Consumers to control which messages you want to read</p>

<p>More to come!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With MapR-DB JSON REST API]]></title>
    <link href="http://tgrall.github.io/blog/2018/04/23/getting-started-with-mapr-db-json-rest-api/"/>
    <updated>2018-04-23T14:37:51+02:00</updated>
    <id>http://tgrall.github.io/blog/2018/04/23/getting-started-with-mapr-db-json-rest-api</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p>In this project you will learn how to use the MapR-DB JSON REST API to:</p>

<p>Create and Delete tables
Create, Read, Update and Delete documents (CRUD)
MapR Extension Package 5.0 (MEP) introduced the MapR-DB JSON REST API that allow application to use REST to interact with MapR-DB JSON.</p>

<p>You can find information about the MapR-DB JSON REST API in the documentation: <a href="https://maprdocs.mapr.com/home/MapR-DB/JSON_DB/UsingMapRDBJSONRESTAPI.html">Using the MapR-DB JSON REST API</a></p>

<!-- more -->


<h2>Prerequisites</h2>

<p>You system should have the following components:</p>

<ul>
<li>A running MapR 6.0.1 &amp; MEP 5.0 cluster with the MapR-DB REST API service installed</li>
<li><code>curl</code> or equivalent tool</li>
</ul>


<h2>Discover the MapR-DB JSON REST API</h2>

<p>The easiest way to discover it, is to use curl command (or equivalent).</p>

<p><strong>1 - Create a table</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -X PUT \
</span><span class='line'>  'https://mapr-node:8243/api/v2/table/%2Fapps%2Femp' \
</span><span class='line'>  -u root:mapr \
</span><span class='line'>  -k</span></code></pre></td></tr></table></div></figure>


<p>In this command:</p>

<ul>
<li>the MapR-DB REST Service (MapR Data Access Gateway) is running on the mapr-node host with the default port <code>8243</code> using HTTPS</li>
<li>the HTTP verb <code>PUT</code> on <code>/api/v2/table/</code> endoint creates a new table</li>
<li>the protocol is HTTP since HTTPS is not enabled on this cluster</li>
<li>the new table will be created wit the path <code>/apps/emp</code> that is encoded to <code>%2Fapps%2Femp</code></li>
<li>the user <code>root</code> with the password <code>mapr</code> is used for authentication, using basic authentication</li>
<li>the <code>-k</code> parameter is used to indicate to turn off curl’s verification of the certificate.</li>
</ul>


<p>In this example, you use the basic authentication, it is also possible to use <a href="https://jwt.io/introduction/">JSON Web Token</a>. You will learn more about this when you will write an application in Go.</p>

<p><strong>2 - Insert Documents</strong></p>

<p>Insert one document</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -X POST \
</span><span class='line'>  'https://mapr-node:8243/api/v2/table/%2Fapps%2Femp' \
</span><span class='line'>  -u root:mapr \
</span><span class='line'>  -H 'Content-Type: application/json' \
</span><span class='line'>  -d '{"_id":"user001","first_name":"John","last_name":"Doe", "age" : 28}' \
</span><span class='line'>  -k</span></code></pre></td></tr></table></div></figure>


<p>In this command:</p>

<ul>
<li>the <code>/api/v2/table/{path}</code> with the verb <code>GET</code> is used with a <code>condition</code> query parameter</li>
<li>the OJAI JSON syntax is used to express the condition: <code>{"$eq":{"last_name":"Doe"}}</code></li>
</ul>


<p><strong>3 - Update a document</strong></p>

<p>The following example will increment the age by 1 and update the last name.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -X POST \
</span><span class='line'>  'https://mapr-node:8243/api/v2/table/%2Fapps%2Femp%2F/document/user001' \
</span><span class='line'>  -u root:mapr \
</span><span class='line'>  -H 'Content-Type: application/json' \
</span><span class='line'>  -d '{"$set" : {"last_name" : "New Doe"}, "$increment" : {"age":1}}' \
</span><span class='line'>  -k</span></code></pre></td></tr></table></div></figure>


<p>In this comamnd:</p>

<ul>
<li>the URL points to the document <code>_id</code> to update</li>
<li>the HTTP verb <code>POST</code> is used to modify the resource</li>
<li>the request body <code>-d</code> is the OJAI JSON Mutation that update the last name and increment the age.</li>
</ul>


<p>You can check that the document has been updated using the following command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -X GET \
</span><span class='line'>  'https://mapr-node:8243/api/v2/table/%2Fapps%2Femp%2F/document/user001' \
</span><span class='line'>  -u root:mapr \
</span><span class='line'>  -k</span></code></pre></td></tr></table></div></figure>


<p><strong>4 - Delete a document</strong></p>

<p>Delete the document with the <code>_id</code> user001.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -X DELETE \
</span><span class='line'>  'https://mapr-node:8243/api/v2/table/%2Fapps%2Femp%2F/document/user001' \
</span><span class='line'>  -u root:mapr \
</span><span class='line'>  -k</span></code></pre></td></tr></table></div></figure>


<p>In this command:</p>

<ul>
<li>the URI <code>/api/v2/table/{path}/document/{id}</code> with the HTTP verb <code>DELETE</code> is used to delete the document</li>
</ul>


<p><strong>5 - Delete the MapR-DB JSON table</strong></p>

<p>The last step of this tutorial is to delete the table using the following command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -X DELETE \
</span><span class='line'>  'https://mapr-node:8243/api/v2/table/%2Fapps%2Femp' \
</span><span class='line'>  -u root:mapr \
</span><span class='line'>  -k</span></code></pre></td></tr></table></div></figure>


<h2>Conclusion</h2>

<p>In this tutorial you have learned how to use the MapR-DB JSON REST API to:</p>

<ul>
<li>Create a table</li>
<li>Insert and query documents</li>
<li>Update and delete documents</li>
<li>Drop table</li>
</ul>


<p>You can now use the API to create MapR-DB JSON Application using your favorite language.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With MapR-DB Table Replication]]></title>
    <link href="http://tgrall.github.io/blog/2017/08/08/getting-started-with-mapr-db-table-replication/"/>
    <updated>2017-08-08T10:01:19+02:00</updated>
    <id>http://tgrall.github.io/blog/2017/08/08/getting-started-with-mapr-db-table-replication</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p>MapR-DB Table Replication allows data to be replicated to another table that could be on on the same cluster or in another cluster. This is different from the automatic and intra-cluster replication that copies the data into different physical nodes for high availability and prevent data loss.</p>

<p>This tutorial focuses on the MapR-DB Table Replication that replicates data between tables on different clusters.</p>

<p>Replicating data between different clusters allows you to:</p>

<ul>
<li>provide another level of disaster recovery that protects your data and applications against global data center failure,</li>
<li>push data close to the applications and users,</li>
<li>aggregate the data from mutliple datacenters.</li>
</ul>


<p><strong>Replication Topologies</strong></p>

<p>MapR-DB Table Replication provides various topologies to adapt the replication to the business and technical requirements:</p>

<ul>
<li><em>Master-slave replication</em> : in this topology, you replicate one way from source tables to replicas. The replicas can be in a remote cluster or in the cluster where the source tables are located.</li>
<li><em>Multi-Master replication</em> : in this replication topology, there are two master-slave relationships, with each table playing both the role of a master and a slave. Client applications update both tables and each table replicates updates to the other.</li>
</ul>


<p>In this example you will learn how to setup multi-master replication.</p>

<!-- more -->


<h3>Prerequisites</h3>

<ul>
<li>2 MapR Clusters 5.x with Enterprise Edition license

<ul>
<li>in this demonstration they are called <code>cluster1</code> and <code>cluster2</code></li>
</ul>
</li>
</ul>


<h2>Setting Up Replication</h2>

<p>In the next steps you will configure your clusters to enable mutip-master replication as follow:</p>

<p><img src="http://tgrall.github.io/images/posts/maprdb-replication/replication.png" alt="Architecture" /></p>

<h3>Configuring the clusters</h3>

<p>Each node of the source cluster must communicate with the destination cluster&rsquo;s CLDB nodes. On each node of your source cluster edit the <code>mapr-clusters.conf</code> file and add the destination cluster information.</p>

<p><em>Cluster 1 Configuration</em></p>

<p>In all the nodes of <code>cluster1</code>, edit the  <code>/opt/mapr/conf/mapr-clusters.conf</code> file and add the <code>cluster2</code> configuration. The file should look like the following:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cluster1 secure=false cluster1-node1:7222 cluster1-node2:7222 cluster1-node2:7222
</span><span class='line'>
</span><span class='line'>cluster2 secure=false cluster2-node1:7222 cluster2-node2:7222 cluster2-node3:7222</span></code></pre></td></tr></table></div></figure>


<p><em>Cluster 2 Configuration</em></p>

<p>In all the nodes of <code>cluster2</code>, edit the  <code>/opt/mapr/conf/mapr-clusters.conf</code> file and add the <code>cluster1</code> configuration. The file should look like the following:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cluster2 secure=false cluster2-node1:7222 cluster2-node2:7222 cluster2-node3:7222
</span><span class='line'>
</span><span class='line'>cluster1 secure=false cluster1-node1:7222 cluster1-node2:7222 cluster1-node2:7222</span></code></pre></td></tr></table></div></figure>


<p>You can find information about the <code>mapr-clusters.conf</code> format in <a href="http://maprdocs.mapr.com/home/ReferenceGuide/mapr-clusters.conf.html">the documentation</a>.</p>

<p>Open a terminal window on one of the <code>cluster1</code> node using <code>mapr</code> user, and do the following:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ls /mapr/cluster1/
</span><span class='line'>apps   hbase  installer  opt  tmp  user  var
</span><span class='line'>
</span><span class='line'>$ ls /mapr/cluster2/
</span><span class='line'>apps   hbase  installer  opt  tmp  user  var
</span></code></pre></td></tr></table></div></figure>


<h3>Installing and Configuring the MapR Gateway</h3>

<p>A MapR gateway mediates one-way communication between a source MapR cluster and a destination MapR cluster. In this example you will use mult-master replication, this means that data will be replicated from <code>cluster1</code> to <code>cluster2</code> and from <code>cluster2</code> to <code>cluster1</code>.</p>

<p>The good practice is to install the MapR-Gateway to the destination cluster, so in our case let&rsquo;s install one gateway on one of the <code>cluster1</code> node, and one gateway on one of the <code>cluster2</code> node. Note that this configuration will not be highly available, and usually you will deploy more than 1 gateway by cluster.</p>

<h4>Installing the MapR-Gateway</h4>

<p>As root on one node of the <code>cluster1</code>, adapt the command to your linux environment, for example on the node <code>cluster1-node2</code></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ yum install mapr-gateway
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># Update MapR configuration
</span><span class='line'>$ /opt/mapr/server/configure.sh -N cluster1 -C cluster1-node1:7222,cluster1-node2:7222,cluster1-node3:7222 -R
</span></code></pre></td></tr></table></div></figure>


<p>Do the same on <code>cluster2</code>, for example on the node <code>cluster2-node2</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ yum install mapr-gateway
</span><span class='line'>
</span><span class='line'>
</span><span class='line'># Update MapR configuration
</span><span class='line'>$ /opt/mapr/server/configure.sh -N cluster1 -C cluster2-node1:7222,cluster2-node2:7222,cluster2-node3:7222 -R
</span></code></pre></td></tr></table></div></figure>


<h4>Registering the Gateway to the Clusters</h4>

<p>Now that we have a gateway running on each cluster, you have to <strong><em>register the gateway</em></strong> in each cluster.</p>

<p>On <code>cluster1</code> run the following command to register the <code>cluster2</code> gateway as destination:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ maprcli cluster gateway set -dstcluster cluster2 -gateways cluster2-node2
</span><span class='line'>
</span><span class='line'># Check the configuration
</span><span class='line'>$ maprcli cluster gateway list</span></code></pre></td></tr></table></div></figure>


<p>On <code>cluster2</code> run the following command to register the <code>cluster1</code> gateway as destination:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ maprcli cluster gateway set -dstcluster cluster1 -gateways cluster1-node2
</span><span class='line'>
</span><span class='line'># Check the configuration
</span><span class='line'>$ maprcli cluster gateway list</span></code></pre></td></tr></table></div></figure>


<h3>Creating Table with Replication</h3>

<p>In a terminal window, as <code>mapr</code> user on <code>cluster1</code>, create a table and insert documents:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ maprcli table create -path /apps/user_profiles  -tabletype json
</span></code></pre></td></tr></table></div></figure>


<p>This create a new JSON table; it is also possible to use <code>/mapr/cluster1/apps/user_profiles</code>.</p>

<p>Let&rsquo;s now add documents using MapR-DB Shell:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mapr dbshell
</span><span class='line'>
</span><span class='line'>maprdb mapr:&gt; insert /apps/user_profiles --value '{"_id":"user001" , "first_name":"John", "last_name":"Doe"}'
</span><span class='line'>
</span><span class='line'>maprdb mapr:&gt; find /apps/user_profiles
</span></code></pre></td></tr></table></div></figure>


<h4>Adding Table Replication</h4>

<p>Let&rsquo;s now enable replication between <code>user_profiles</code> on <code>cluster1</code> to a <code>user_profiles</code> table in <code>cluster2</code>.</p>

<p>In <code>cluster1</code>, on a terminal window as <code>mapr</code> run the following command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ maprcli table replica autosetup -path /apps/user_profiles -replica /mapr/cluster2/apps/user_profiles -multimaster yes</span></code></pre></td></tr></table></div></figure>


<p>You can get information about the replication configuration for the table using the following command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ maprcli table replica list -path /apps/user_profiles -json</span></code></pre></td></tr></table></div></figure>


<h4>Testing Replication</h4>

<p>Open another terminal in <code>cluster2</code> and use MapR-DB Shell to look at the replicated data:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mapr dbshell
</span><span class='line'>
</span><span class='line'>maprdb mapr:&gt; find /apps/user_profiles
</span><span class='line'>{"_id":"user001","first_name":"John","last_name":"Doe"}
</span><span class='line'>1 document(s) found.
</span></code></pre></td></tr></table></div></figure>


<p>You can also use the full path <code>/mapr/cluster2/apps/user_profiles</code></p>

<p>In <code>cluster1</code> add a new document using MapR-DB Shell:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mapr dbshell
</span><span class='line'>
</span><span class='line'>maprdb mapr:&gt; insert /apps/user_profiles --value '{"_id":"user002" , "first_name":"Simon", "last_name":"Dupont"}'
</span><span class='line'>
</span><span class='line'>maprdb mapr:&gt; find /apps/user_profiles</span></code></pre></td></tr></table></div></figure>


<p>Do a find in <code>cluster2</code> table, and you will see that the data have been replicated.</p>

<p>You can insert or delete a document in <code>cluster2</code> and do a find in <code>cluster1</code>, you will see that the new document is also replicated in the other direction.</p>

<p>Note, for this demonstration, we use 2 terminals connected to each cluster you can do some test using the Global Namespace in a single MapR-DB Shell.</p>

<h2>Conclusion</h2>

<p>In this tutorial you have learned how to setup the MapR-DB Multi-Master replication to have data automatically replicated between 2 clusters.</p>

<p>MapR-DB Table Replication provides many options, not only in term of topology (master-slave/mult-master), but also some options and commands to:</p>

<ul>
<li>replicate some columns/attributes or column family</li>
<li>configure replication in a secured cluster</li>
<li>pause replication.</li>
</ul>


<p>You can find more information about the MapR-DB Table Replication, and MapR-Gateway in the documentation:</p>

<ul>
<li><a href="http://maprdocs.mapr.com/home/MapR-DB/ReplicatingMapR-DBTables.html">Table Replication</a></li>
<li><a href="http://maprdocs.mapr.com/home/MapR-DB/ConfiguringMapRClustersForTR.html">Setting up Table Replication</a></li>
<li><a href="http://maprdocs.mapr.com/home/Gateways/MapRGateways.html">Configuring and Managing MapR Gateways</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With Kafka REST Proxy for MapR Streams]]></title>
    <link href="http://tgrall.github.io/blog/2017/01/20/getting-started-with-kafka-rest-proxy-for-mapr-streams/"/>
    <updated>2017-01-20T10:31:22+01:00</updated>
    <id>http://tgrall.github.io/blog/2017/01/20/getting-started-with-kafka-rest-proxy-for-mapr-streams</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p>MapR Ecosystem Package 2.0 (MEP) is coming with some new features related to MapR Streams:</p>

<ul>
<li><a href="http://maprdocs.mapr.com/home/Kafka/kafkaREST.html">Kafka REST Proxy for MapR Streams</a> provides a RESTful interface to MapR Streams and Kafka clusters to consume and product messages and to perform administrative operations.</li>
<li><a href="http://maprdocs.mapr.com/home/Kafka/kafkaConnect.html">Kafka Connect for MapR Streams</a> is a utility for streaming data between MapR Streams and Apache Kafka and other storage systems.</li>
</ul>


<p>MapR Ecosystem Packs (MEPs) are a way to deliver ecosystem upgrades decoupled from core upgrades - allowing you to upgrade your tooling independently of your Converged Data Platform. You can lean more about MEP 2.0 in <a href="https://www.mapr.com/blog/announcing-mapr-ecosystem-pack-mep-20">this article</a>.</p>

<p>In this blog we describe how to use the REST Proxy to publish and consume messages to/from MapR Streams. The REST Proxy is a great addition to the MapR Converged Data Platform allowing any programming language to use MapR Streams.</p>

<p>The Kafka REST Proxy provided with the MapR Streams tools, can be used with MapR Streams (default), but also used in a hybrid mode with Apache Kafka. In this article we will focus on MapR Streams.</p>

<!-- more -->


<h2>Prerequisites</h2>

<ul>
<li>MapR Converged Data Platform 5.2 with MEP 2.0

<ul>
<li>with MapR Streams Tools</li>
</ul>
</li>
<li>curl, wget or any HTTP/REST Client tool</li>
</ul>


<h2>Create the MapR Streams and Topic</h2>

<p>A stream is a collection of topics that you can manage as a group by:</p>

<ol>
<li>Setting security policies that apply to all topics in that stream</li>
<li>Setting a default number of partitions for each new topic that is created in the stream</li>
<li>Set a time-to-live for messages in every topic in the stream</li>
</ol>


<p>You can find more information about MapR Streams concepts in the <a href="http://maprdocs.mapr.com/home/MapR_Streams/mapr_streams.html">documentation</a>.</p>

<p>On your Mapr Cluster or Sandbox, run the following commands:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ maprcli stream create -path /apps/iot-stream -produceperm p -consumeperm p -topicperm p
</span><span class='line'>
</span><span class='line'>$ maprcli stream topic create -path /apps/iot-stream -topic sensor-json -partitions 3
</span><span class='line'>
</span><span class='line'>$ maprcli stream topic create -path /apps/iot-stream -topic sensor-binary -partitions 3</span></code></pre></td></tr></table></div></figure>


<h2>Start Kafka Console Producers and Consumers</h2>

<p>Open two terminal windows and run the consumer Kafka utilities using the following commands:</p>

<h4>Consumer</h4>

<ul>
<li>Topic sensor-json</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ /opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-consumer.sh --new-consumer --bootstrap-server this.will.be.ignored:9092 --topic /apps/iot-stream:sensor-json</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Topic sensor-binary</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ /opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-consumer.sh --new-consumer --bootstrap-server this.will.be.ignored:9092 --topic /apps/iot-stream:sensor-binary</span></code></pre></td></tr></table></div></figure>


<p>This two terminal windows will allow you to see the messages posted on the different topics</p>

<h2>Using Kafka REST Proxy</h2>

<h3>Inspect Topic Metadata</h3>

<p>The endpoint <code>/topics/[topic_name]</code> allows you to get some informations about the topic. In MapR Streams, topics are part of a <em>stream</em> identified by a path;
to use the topic using the REST API you have to use the full path, and encode it in the URL; for example:</p>

<ul>
<li><code>/apps/iot-stream:sensor-json</code> will be encoded with <code>%2Fapps%2Fiot-stream%3Asensor-json</code></li>
</ul>


<p>Run the following command, to get information about the <code>sensor-json</code> topic</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ curl -X GET  http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-json</span></code></pre></td></tr></table></div></figure>


<p>Note: For simplicity reason I am running the command from the node where the Kafka REST proxy is running, so it is possible to use <code>localhost</code>.</p>

<p>You can print JSON in a pretty way, by adding a Python command such as :</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ curl -X GET  http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-json | python -m json.tool</span></code></pre></td></tr></table></div></figure>


<p><strong>Default Stream</strong></p>

<p>As mentioned above, the Stream path is part of the topic name you have to use in the command;
however it is possible to configure the MapR Kafka REST Proxy to use a default stream.
For this you should add the following property in the <code>/opt/mapr/kafka-rest/kafka-rest-2.0.1/config/kafka-rest.properties</code> file:</p>

<ul>
<li><code>streams.default.stream=/apps/iot-stream</code></li>
</ul>


<p> When you change the Kafka REST proxy configuration, you must restart the service using maprcli or MCS.</p>

<p> The main reason to use the <code>streams.default.stream</code> properties is to simplify the URLs used by the application for example
 * with <code>streams.default.stream</code> you can use <code>curl -X GET  http://localhost:8082/topics/</code>
 * without this configuration, or if you want to use a specific stream you must specify it in the URL <code>http://localhost:8082/topics/%2Fapps%2Fiot-stream%3Asensor-json</code></p>

<p> In this article, all the URLs contains the encoded stream name, like that you can start using the Kafka REST proxy without changind the configuration and also use it with different streams.</p>

<h3>Publishing Messages</h3>

<p>The Kafka REST Proxy for MapR Streams allows application to publish messages to MapR Streams. Messages could be send as JSON or Binary content (base64 encoding).</p>

<h4>To send a JSON Message:</h4>

<ul>
<li>the query should be a HTTP <code>POST</code></li>
<li>the Content-Type should be : <code>application/vnd.kafka.json.v1+json</code></li>
<li>the Body:</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='JSON'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;records&quot;</span><span class="p">:</span>
</span><span class='line'>  <span class="p">[</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;value&quot;</span><span class="p">:</span>
</span><span class='line'>      <span class="p">{</span>
</span><span class='line'>        <span class="nt">&quot;temp&quot;</span> <span class="p">:</span> <span class="mi">10</span> <span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;speed&quot;</span> <span class="p">:</span> <span class="mi">40</span> <span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;direction&quot;</span> <span class="p">:</span> <span class="s2">&quot;NW&quot;</span>
</span><span class='line'>        <span class="p">}</span>
</span><span class='line'>      <span class="p">}</span>
</span><span class='line'>  <span class="p">]</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The complete request is:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='JSON'><span class='line'><span class="err">curl</span> <span class="err">-X</span> <span class="err">POST</span> <span class="err">-H</span> <span class="s2">&quot;Content-Type: application/vnd.kafka.json.v1+json&quot;</span> <span class="err">\</span>
</span><span class='line'>  <span class="err">--data</span> <span class="err">&#39;</span><span class="p">{</span><span class="nt">&quot;records&quot;</span><span class="p">:[{</span><span class="nt">&quot;value&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;temp&quot;</span> <span class="p">:</span> <span class="mi">10</span> <span class="p">,</span> <span class="nt">&quot;speed&quot;</span> <span class="p">:</span> <span class="mi">40</span> <span class="p">,</span> <span class="nt">&quot;direction&quot;</span> <span class="p">:</span> <span class="s2">&quot;NW&quot;</span><span class="p">}</span>  <span class="p">}]}</span><span class="err">&#39;</span> <span class="err">\</span>
</span><span class='line'>  <span class="err">http://localhost:</span><span class="mi">8082</span><span class="err">/topics/%</span><span class="mi">2</span><span class="err">Fapps%</span><span class="mi">2</span><span class="err">Fiot-stream%</span><span class="mi">3</span><span class="err">Asensor-json</span>
</span></code></pre></td></tr></table></div></figure>


<p>You should see the message printed in the terminal window where the <code>/apps/iot-stream:sensor-json</code> consumer is running.</p>

<h4>To send a binary Message:</h4>

<ul>
<li>the query should be a HTTP <code>POST</code></li>
<li>the Content-Type should be : <code>application/vnd.kafka.binary.v1+json</code></li>
<li>the Body:</li>
</ul>


<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='JSON'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;records&quot;</span><span class="p">:</span>
</span><span class='line'>  <span class="p">[</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;value&quot;</span><span class="p">:</span><span class="s2">&quot;SGVsbG8gV29ybGQ=&quot;</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="p">]</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Note that <code>SGVsbG8gV29ybGQ=</code> is the string &ldquo;Hello World&rdquo; encoded in Base64.</p>

<p>The complete request is:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='JSON'><span class='line'><span class="err">curl</span> <span class="err">-X</span> <span class="err">POST</span> <span class="err">-H</span> <span class="s2">&quot;Content-Type: application/vnd.kafka.binary.v1+json&quot;</span> <span class="err">\</span>
</span><span class='line'>  <span class="err">--data</span> <span class="err">&#39;</span><span class="p">{</span><span class="nt">&quot;records&quot;</span><span class="p">:[{</span><span class="nt">&quot;value&quot;</span><span class="p">:</span><span class="s2">&quot;SGVsbG8gV29ybGQ=&quot;</span><span class="p">}]}</span><span class="err">&#39;</span> <span class="err">\</span>
</span><span class='line'>  <span class="err">http://localhost:</span><span class="mi">8082</span><span class="err">/topics/%</span><span class="mi">2</span><span class="err">Fapps%</span><span class="mi">2</span><span class="err">Fiot-stream%</span><span class="mi">3</span><span class="err">Asensor-binary</span>
</span></code></pre></td></tr></table></div></figure>


<p>You should see the message printed in the terminal window where the <code>/apps/iot-stream:sensor-binary</code> consumer is running.</p>

<h4>Sending multiple messages</h4>

<p>The <code>records</code> field of the HTTP Body allows you to send multiple messages for example you can send:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='JSON'><span class='line'><span class="err">curl</span> <span class="err">-X</span> <span class="err">POST</span> <span class="err">-H</span> <span class="s2">&quot;Content-Type: application/vnd.kafka.json.v1+json&quot;</span> <span class="err">\</span>
</span><span class='line'>  <span class="err">--data</span> <span class="err">&#39;</span><span class="p">{</span><span class="nt">&quot;records&quot;</span><span class="p">:[{</span><span class="nt">&quot;value&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;temp&quot;</span> <span class="p">:</span> <span class="mi">12</span> <span class="p">,</span> <span class="nt">&quot;speed&quot;</span> <span class="p">:</span> <span class="mi">42</span> <span class="p">,</span> <span class="nt">&quot;direction&quot;</span> <span class="p">:</span> <span class="s2">&quot;NW&quot;</span><span class="p">}</span>  <span class="p">},</span> <span class="p">{</span><span class="nt">&quot;value&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;temp&quot;</span> <span class="p">:</span> <span class="mi">10</span> <span class="p">,</span> <span class="nt">&quot;speed&quot;</span> <span class="p">:</span> <span class="mi">37</span> <span class="p">,</span> <span class="nt">&quot;direction&quot;</span> <span class="p">:</span> <span class="s2">&quot;N&quot;</span><span class="p">}</span>  <span class="p">}</span> <span class="p">]}</span><span class="err">&#39;</span> <span class="err">\</span>
</span><span class='line'>  <span class="err">http://localhost:</span><span class="mi">8082</span><span class="err">/topics/%</span><span class="mi">2</span><span class="err">Fapps%</span><span class="mi">2</span><span class="err">Fiot-stream%</span><span class="mi">3</span><span class="err">Asensor-json</span>
</span></code></pre></td></tr></table></div></figure>


<p>This command will send 2 messages, and increment the offset by 2. You can do the same
with binary content, just add new element in the JSON array; for example:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='JSON'><span class='line'><span class="err">curl</span> <span class="err">-X</span> <span class="err">POST</span> <span class="err">-H</span> <span class="s2">&quot;Content-Type: application/vnd.kafka.binary.v1+json&quot;</span> <span class="err">\</span>
</span><span class='line'>  <span class="err">--data</span> <span class="err">&#39;</span><span class="p">{</span><span class="nt">&quot;records&quot;</span><span class="p">:[{</span><span class="nt">&quot;value&quot;</span><span class="p">:</span><span class="s2">&quot;SGVsbG8gV29ybGQ=&quot;</span><span class="p">},</span> <span class="p">{</span><span class="nt">&quot;value&quot;</span><span class="p">:</span><span class="s2">&quot;Qm9uam91cg==&quot;</span><span class="p">}]}</span><span class="err">&#39;</span> <span class="err">\</span>
</span><span class='line'>  <span class="err">http://localhost:</span><span class="mi">8082</span><span class="err">/topics/%</span><span class="mi">2</span><span class="err">Fapps%</span><span class="mi">2</span><span class="err">Fiot-stream%</span><span class="mi">3</span><span class="err">Asensor-binary</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you probably know, it is possible to set a key to a message to be sure that all the messages
with the same key will arrive in the same partition. For this, add the <code>key</code> attribute to the message as follow:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='JSON'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;records&quot;</span><span class="p">:</span>
</span><span class='line'>  <span class="p">[</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;K001&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;value&quot;</span><span class="p">:</span>
</span><span class='line'>      <span class="p">{</span>
</span><span class='line'>        <span class="nt">&quot;temp&quot;</span> <span class="p">:</span> <span class="mi">10</span> <span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;speed&quot;</span> <span class="p">:</span> <span class="mi">40</span> <span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;direction&quot;</span> <span class="p">:</span> <span class="s2">&quot;NW&quot;</span>
</span><span class='line'>        <span class="p">}</span>
</span><span class='line'>      <span class="p">}</span>
</span><span class='line'>  <span class="p">]</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now that you know how to post messages to MapR Stream topics usinf the REST Proxy, let&rsquo;s see how to consume the messages.</p>

<h3>Consuming Messages</h3>

<p>The REST proxy can also be used to consume messages from topics; for this you need to:</p>

<ol>
<li>Create a consumer instance.</li>
<li>Use this URL returned by the first call to read message.</li>
<li>Delete the consumer instanced if needed.</li>
</ol>


<h4>Creating the consumer instance</h4>

<p>The following request creates the consumer instance:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='JSON'><span class='line'><span class="err">curl</span> <span class="err">-X</span> <span class="err">POST</span> <span class="err">-H</span> <span class="s2">&quot;Content-Type: application/vnd.kafka.v1+json&quot;</span> <span class="err">\</span>
</span><span class='line'>      <span class="err">--data</span> <span class="err">&#39;</span><span class="p">{</span><span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;iot_json_consumer&quot;</span><span class="p">,</span> <span class="nt">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;json&quot;</span><span class="p">,</span> <span class="nt">&quot;auto.offset.reset&quot;</span><span class="p">:</span> <span class="s2">&quot;earliest&quot;</span><span class="p">}</span><span class="err">&#39;</span> <span class="err">\</span>
</span><span class='line'>      <span class="err">http://localhost:</span><span class="mi">8082</span><span class="err">/consumers/%</span><span class="mi">2</span><span class="err">Fapps%</span><span class="mi">2</span><span class="err">Fiot-stream%</span><span class="mi">3</span><span class="err">Asensor-json</span>
</span></code></pre></td></tr></table></div></figure>


<p>The response from the server looks like:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;instance_id&quot;</span><span class="p">:</span><span class="s2">&quot;iot_json_consumer&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;base_uri&quot;</span><span class="p">:</span><span class="s2">&quot;http://localhost:8082/consumers/%2Fapps%2Fiot-stream%3Asensor-json/instances/iot_json_consumer&quot;</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Note that we have used the <code>/consumers/[topic_name]</code> to create the consumer.</p>

<p>The <code>base_uri</code> will be used by the subsequent requests to get the messages from the topic. Like any MapR Streams/Kafka consumer the <code>auto.offset.reset</code> defines its behavior. In this example the value is set to <code>earliest</code>, this means that the consumer will read the messages from the beginning. You can find more information about the consumer configuration in the <a href="http://maprdocs.mapr.com/home/MapR_Streams/configuration_parameters_for_consumers.html">MapR Streams documentation</a>.</p>

<h4>Consuming the messages</h4>

<p>To consume the messages, just add the Mapr Streams topic to the URL of the consumer isntance.</p>

<p>The following request consumes the messages from the topic:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="err">curl</span> <span class="err">-X</span> <span class="err">GET</span> <span class="err">-H</span> <span class="s2">&quot;Accept: application/vnd.kafka.json.v1+json&quot;</span> <span class="err">\</span>
</span><span class='line'><span class="err">http://localhost:</span><span class="mi">8082</span><span class="err">/consumers/%</span><span class="mi">2</span><span class="err">Fapps%</span><span class="mi">2</span><span class="err">Fiot-stream%</span><span class="mi">3</span><span class="err">Asensor-json/instances/iot_json_consumer/topics/%</span><span class="mi">2</span><span class="err">Fapps%</span><span class="mi">2</span><span class="err">Fiot-stream%</span><span class="mi">3</span><span class="err">Asensor-json</span>
</span></code></pre></td></tr></table></div></figure>


<p>This call returns the messages in a JSON document:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">[</span>
</span><span class='line'>  <span class="p">{</span><span class="nt">&quot;key&quot;</span><span class="p">:</span><span class="kc">null</span><span class="p">,</span><span class="nt">&quot;value&quot;</span><span class="p">:{</span><span class="nt">&quot;temp&quot;</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span><span class="nt">&quot;speed&quot;</span><span class="p">:</span><span class="mi">40</span><span class="p">,</span><span class="nt">&quot;direction&quot;</span><span class="p">:</span><span class="s2">&quot;NW&quot;</span><span class="p">},</span><span class="nt">&quot;topic&quot;</span><span class="p">:</span><span class="s2">&quot;/apps/iot-stream:sensor-json&quot;</span><span class="p">,</span><span class="nt">&quot;partition&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="nt">&quot;offset&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">},</span>
</span><span class='line'>  <span class="p">{</span><span class="nt">&quot;key&quot;</span><span class="p">:</span><span class="kc">null</span><span class="p">,</span><span class="nt">&quot;value&quot;</span><span class="p">:{</span><span class="nt">&quot;temp&quot;</span><span class="p">:</span><span class="mi">12</span><span class="p">,</span><span class="nt">&quot;speed&quot;</span><span class="p">:</span><span class="mi">42</span><span class="p">,</span><span class="nt">&quot;direction&quot;</span><span class="p">:</span><span class="s2">&quot;NW&quot;</span><span class="p">},</span><span class="nt">&quot;topic&quot;</span><span class="p">:</span><span class="s2">&quot;/apps/iot-stream:sensor-json&quot;</span><span class="p">,</span><span class="nt">&quot;partition&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="nt">&quot;offset&quot;</span><span class="p">:</span><span class="mi">2</span><span class="p">},</span>
</span><span class='line'>  <span class="p">{</span><span class="nt">&quot;key&quot;</span><span class="p">:</span><span class="kc">null</span><span class="p">,</span><span class="nt">&quot;value&quot;</span><span class="p">:{</span><span class="nt">&quot;temp&quot;</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span><span class="nt">&quot;speed&quot;</span><span class="p">:</span><span class="mi">37</span><span class="p">,</span><span class="nt">&quot;direction&quot;</span><span class="p">:</span><span class="s2">&quot;N&quot;</span><span class="p">},</span><span class="nt">&quot;topic&quot;</span><span class="p">:</span><span class="s2">&quot;/apps/iot-stream:sensor-json&quot;</span><span class="p">,</span><span class="nt">&quot;partition&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="nt">&quot;offset&quot;</span><span class="p">:</span><span class="mi">3</span><span class="p">}</span>
</span><span class='line'><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Each call to the API returns the new messages published, based on the offset of the last call.</p>

<p>Note that the Consumer will be destroyed:</p>

<ul>
<li>after some idle time set by the <code>consumer.instance.timeout.ms</code> (default value set to 300000ms / 5 minutes)</li>
<li>where it is destroyed using a REST API call (see below).</li>
</ul>


<h3>Consuming binary format messages</h3>

<p>The approach is the same if you need to consume binary messages, you need to change the format and accept header.</p>

<p>Call this URL to create a consumer instance for the binary topic:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="err">curl</span> <span class="err">-X</span> <span class="err">POST</span> <span class="err">-H</span> <span class="s2">&quot;Content-Type: application/vnd.kafka.v1+json&quot;</span> <span class="err">\</span>
</span><span class='line'>      <span class="err">--data</span> <span class="err">&#39;</span><span class="p">{</span><span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;iot_binary_consumer&quot;</span><span class="p">,</span> <span class="nt">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="nt">&quot;auto.offset.reset&quot;</span><span class="p">:</span> <span class="s2">&quot;earliest&quot;</span><span class="p">}</span><span class="err">&#39;</span> <span class="err">\</span>
</span><span class='line'>      <span class="err">http://localhost:</span><span class="mi">8082</span><span class="err">/consumers/%</span><span class="mi">2</span><span class="err">Fapps%</span><span class="mi">2</span><span class="err">Fiot-stream%</span><span class="mi">3</span><span class="err">Asensor-binary</span>
</span></code></pre></td></tr></table></div></figure>


<p>Then consume messages, the accept header is set to <code>application/vnd.kafka.binary.v1+json</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="err">curl</span> <span class="err">-X</span> <span class="err">GET</span> <span class="err">-H</span> <span class="s2">&quot;Accept: application/vnd.kafka.binary.v1+json&quot;</span> <span class="err">\</span>
</span><span class='line'><span class="err">http://localhost:</span><span class="mi">8082</span><span class="err">/consumers/%</span><span class="mi">2</span><span class="err">Fapps%</span><span class="mi">2</span><span class="err">Fiot-stream%</span><span class="mi">3</span><span class="err">Asensor-binary/instances/iot_binary_consumer/topics/%</span><span class="mi">2</span><span class="err">Fapps%</span><span class="mi">2</span><span class="err">Fiot-stream%</span><span class="mi">3</span><span class="err">Asensor-binary</span>
</span></code></pre></td></tr></table></div></figure>


<p>This call returns the messages in a JSON document, and the value is encoded in Base64</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">[</span>
</span><span class='line'>  <span class="p">{</span><span class="nt">&quot;key&quot;</span><span class="p">:</span><span class="kc">null</span><span class="p">,</span><span class="nt">&quot;value&quot;</span><span class="p">:</span><span class="s2">&quot;SGVsbG8gV29ybGQ=&quot;</span><span class="p">,</span><span class="nt">&quot;topic&quot;</span><span class="p">:</span><span class="s2">&quot;/apps/iot-stream:sensor-binary&quot;</span><span class="p">,</span><span class="nt">&quot;partition&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="nt">&quot;offset&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">},</span>
</span><span class='line'>  <span class="p">{</span><span class="nt">&quot;key&quot;</span><span class="p">:</span><span class="kc">null</span><span class="p">,</span><span class="nt">&quot;value&quot;</span><span class="p">:</span><span class="s2">&quot;Qm9uam91cg==&quot;</span><span class="p">,</span><span class="nt">&quot;topic&quot;</span><span class="p">:</span><span class="s2">&quot;/apps/iot-stream:sensor-binary&quot;</span><span class="p">,</span><span class="nt">&quot;partition&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="nt">&quot;offset&quot;</span><span class="p">:</span><span class="mi">2</span><span class="p">}</span>
</span><span class='line'><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Delete consumer instances</h3>

<p>As mentioned before the consumer will be destroyed automatically based on the <code>consumer.instance.timeout.ms</code> configuration of the REST Proxy;
it is also possible to destroyed the instance using the consumer instance URI and an HTTP DELETE call, as follow:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="err">curl</span> <span class="err">-X</span> <span class="err">DELETE</span> <span class="err">http://localhost:</span><span class="mi">8082</span><span class="err">/consumers/%</span><span class="mi">2</span><span class="err">Fapps%</span><span class="mi">2</span><span class="err">Fiot-stream%</span><span class="mi">3</span><span class="err">Asensor-binary/instances/iot_binary_consumer</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Conclusion</h2>

<p>In this article you have learned how to use the Kafka REST Proxy for MapR Streams that allow any application to
use messages published in the MapR Converged Data Platform.</p>

<p>You can find more information about the Kafka REST Proxy in the <a href="http://maprdocs.mapr.com/home/Kafka/REST-proxy.html">MapR documentation</a> and the following resources:</p>

<ul>
<li><a href="https://www.mapr.com/blog/getting-started-sample-programs-mapr-streams">Getting Started with MapR Streams</a></li>
<li><a href="https://www.mapr.com/streaming-architecture-using-apache-kafka-mapr-streams">&ldquo;Streaming Architecture: New Designs Using Apache Kafka and MapR Streams&rdquo; ebook by Ted Dunning and Ellen Friedman</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With MQTT and Java]]></title>
    <link href="http://tgrall.github.io/blog/2017/01/02/getting-started-with-mqtt/"/>
    <updated>2017-01-02T16:03:09+01:00</updated>
    <id>http://tgrall.github.io/blog/2017/01/02/getting-started-with-mqtt</id>
    <content type="html"><![CDATA[<p>MQTT (MQ Telemetry Transport) is a lightweight publish/subscribe messaging protocol.
MQTT is used a lot in the Internet of Things applications, since it has been designed to
run on remote locations with system with small footprint.</p>

<p>The MQTT 3.1 is an OASIS standard, and you can find all the information at <a href="http://mqtt.org/">http://mqtt.org/</a></p>

<p>This article will guide you into the various steps to run your first MQTT application:</p>

<ol>
<li>Install and Start a MQTT Broker</li>
<li>Write an application that publishes messages</li>
<li>Write an application that consumes messages</li>
</ol>


<p>The source code of the sample application is available on <a href="https://github.com/tgrall/mqtt-sample-java">GitHub</a>.</p>

<!-- more -->


<h4>Prerequisites</h4>

<ul>
<li>Apache Maven 3.x</li>
<li>Git</li>
</ul>


<h3>Install and Start a MQTT Broker</h3>

<p>You can find many MQTT Brokers, for this example I will use one of the most common broker <a href="https://mosquitto.org">Mosquitto</a>.</p>

<p>You can download and install from the <a href="https://mosquitto.org/download/">binary package</a>. I have used <a href="http://brew.sh/">Homebrew</a> to install it on my Mac:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ brew install mosquitto</span></code></pre></td></tr></table></div></figure>


<p>Start the MQTT Broker with the default configuration</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ /usr/local/sbin/mosquitto</span></code></pre></td></tr></table></div></figure>


<h3>Publish and Consume messages</h3>

<p>Open two terminal windows and run the following commands :</p>

<p>Consume</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mosquitto_sub -h 127.0.0.1 -t iot_data</span></code></pre></td></tr></table></div></figure>


<p>Publish</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ mosquitto_pub -h 127.0.0.1 -t iot_data -m "Hello world"</span></code></pre></td></tr></table></div></figure>


<p>You should see the message <code>Hello world</code> in the consumer/subscriber window.</p>

<h3>Write your first MQTT Application</h3>

<p>For this example I will write a small Java application, since it is the language
that I am using in my global project.</p>

<h4>Maven Dependencies</h4>

<p>Add the <a href="https://eclipse.org/paho/">Eclipse Paho</a> dependency to your Maven project</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;dependency&gt;</span>
</span><span class='line'>  <span class="nt">&lt;groupId&gt;</span>org.eclipse.paho<span class="nt">&lt;/groupId&gt;</span>
</span><span class='line'>  <span class="nt">&lt;artifactId&gt;</span>org.eclipse.paho.client.mqttv3<span class="nt">&lt;/artifactId&gt;</span>
</span><span class='line'>  <span class="nt">&lt;version&gt;</span>1.1.0<span class="nt">&lt;/version&gt;</span>
</span><span class='line'><span class="nt">&lt;/dependency&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<h4>Publishing a Message</h4>

<p>Publishing a message is quite easy, create a MqttClient and use it to post on a topic.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">MqttClient</span> <span class="n">client</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">MqttClient</span><span class="o">(</span><span class="s">&quot;tcp://localhost:1883&quot;</span><span class="o">,</span> <span class="n">MqttClient</span><span class="o">.</span><span class="na">generateClientId</span><span class="o">());</span>
</span><span class='line'><span class="n">client</span><span class="o">.</span><span class="na">connect</span><span class="o">();</span>
</span><span class='line'><span class="n">MqttMessage</span> <span class="n">message</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">MqttMessage</span><span class="o">();</span>
</span><span class='line'><span class="n">message</span><span class="o">.</span><span class="na">setPayload</span><span class="o">(</span><span class="s">&quot;Hello world from Java&quot;</span><span class="o">.</span><span class="na">getBytes</span><span class="o">());</span>
</span><span class='line'><span class="n">client</span><span class="o">.</span><span class="na">publish</span><span class="o">(</span><span class="s">&quot;iot_data&quot;</span><span class="o">,</span> <span class="n">message</span><span class="o">);</span>
</span><span class='line'><span class="n">client</span><span class="o">.</span><span class="na">disconnect</span><span class="o">();</span>
</span></code></pre></td></tr></table></div></figure>


<p>You have many other options, configurations that you can use when posting a message
such as security, quality of service (QoS), and more; but in this post I want to simply
show how easy is to publish and consume MQTT messages.</p>

<h4>Consuming messages</h4>

<p>To consume messages you need to implement a <code>org.eclipse.paho.client.mqttv3.MqttCallback</code> that will receive the message and used this Callback class in the MqttClient of the Subscriber application.</p>

<p>The Callback class:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">SimpleMqttCallBack</span> <span class="kd">implements</span> <span class="n">MqttCallback</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">connectionLost</span><span class="o">(</span><span class="n">Throwable</span> <span class="n">throwable</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">&quot;Connection to MQTT broker lost!&quot;</span><span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">messageArrived</span><span class="o">(</span><span class="n">String</span> <span class="n">s</span><span class="o">,</span> <span class="n">MqttMessage</span> <span class="n">mqttMessage</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">&quot;Message received:\n\t&quot;</span><span class="o">+</span> <span class="k">new</span> <span class="nf">String</span><span class="o">(</span><span class="n">mqttMessage</span><span class="o">.</span><span class="na">getPayload</span><span class="o">())</span> <span class="o">);</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">deliveryComplete</span><span class="o">(</span><span class="n">IMqttDeliveryToken</span> <span class="n">iMqttDeliveryToken</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// not used in this example</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>This Callback class is used in the Subscriber application as follow:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">MqttClient</span> <span class="n">client</span><span class="o">=</span><span class="k">new</span> <span class="nf">MqttClient</span><span class="o">(</span><span class="s">&quot;tcp://localhost:1883&quot;</span><span class="o">,</span> <span class="n">MqttClient</span><span class="o">.</span><span class="na">generateClientId</span><span class="o">());</span>
</span><span class='line'><span class="n">client</span><span class="o">.</span><span class="na">setCallback</span><span class="o">(</span> <span class="k">new</span> <span class="nf">SimpleMqttCallBack</span><span class="o">()</span> <span class="o">);</span>
</span><span class='line'><span class="n">client</span><span class="o">.</span><span class="na">connect</span><span class="o">();</span>
</span></code></pre></td></tr></table></div></figure>


<p>Like for the publisher, I am using the broker and client without any option (QoS, security).</p>

<h2>Build and Run the Application</h2>

<p><strong>1- Get the Sample Code</strong></p>

<p>Clone the project from GitHub</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">$</span> <span class="n">git</span> <span class="n">clone</span> <span class="nl">https:</span><span class="c1">//github.com/tgrall/mqtt-sample-java.git</span>
</span></code></pre></td></tr></table></div></figure>


<p><strong>2- Build the project with Apache Maven:</strong></p>

<p>This project is a simple Java application that runs a publisher and subscriber using the <a href="https://eclipse.org/paho/">Eclipse Paho library</a>.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">$</span> <span class="n">mvn</span> <span class="n">clean</span> <span class="kn">package</span>
</span></code></pre></td></tr></table></div></figure>


<p>For convenience, the example programs project is set up so that the maven package target produces a single executable,
<code>/mqtt-sample</code>, that includes all of the example programs and dependencies.</p>

<p><strong>3- Run the Subscriber</strong></p>

<p>The subscriber will receive and print all messages published on the <code>iot_data</code> topic.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">$</span> <span class="o">./</span><span class="n">target</span><span class="o">/</span><span class="n">mqtt</span><span class="o">-</span><span class="n">sample</span> <span class="n">subscriber</span>
</span></code></pre></td></tr></table></div></figure>


<p><strong>4- Run the Publisher</strong></p>

<p>Run the publisher with the following command, the second parameter is the message to publish</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">$</span> <span class="o">./</span><span class="n">target</span><span class="o">/</span><span class="n">mqtt</span><span class="o">-</span><span class="n">sample</span> <span class="n">publisher</span> <span class="s">&quot;My first MQTT message...&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Conclusion</h2>

<p>In this article you have learned how to:</p>

<ul>
<li>Install and start a MQTT Broker, Mosquitto</li>
<li>Create a publisher and subscriber developed in Java</li>
</ul>


<p>This article is very simple by choice, to quickly run your first MQTT Application. I wrote this article as part of a global IoT project I am working on that will capture devices data, publish them into MapR Converged Data Platform using MQTT and MapR Streams; this is why I used Java for the application. You can use any MQTT client library to build the publishers and subscribers.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With Apache Flink and Mapr Streams]]></title>
    <link href="http://tgrall.github.io/blog/2016/10/17/getting-started-with-apache-flink-and-mapr-streams/"/>
    <updated>2016-10-17T10:12:10+02:00</updated>
    <id>http://tgrall.github.io/blog/2016/10/17/getting-started-with-apache-flink-and-mapr-streams</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p><a href="https://flink.apache.org/">Apache Flink</a> is an open source platform for distributed stream and batch data processing. Flink is a streaming data flow engine with several APIs to create data streams oriented application.</p>

<p>It is very common for Flink applications to use <a href="http://kafka.apache.org/">Apache Kafka</a> for data input and output.</p>

<p>This article will guide you into  the steps to use Apache Flink with <a href="https://www.mapr.com/products/mapr-streams">MapR Streams</a>. MapR Streams is a distributed messaging system for streaming event data at scale, and it’s integrated into the <a href="https://www.mapr.com/products/mapr-converged-data-platform">MapR Converged Data Platform</a>, based on the Apache Kafka API (0.9.0),
this article use the same code and approach than the <a href="http://tgrall.github.io/blog/2016/10/12/getting-started-with-apache-flink-and-kafka/">Flink and Kafka Getting Started</a>.</p>

<p><img src="http://tgrall.github.io/images/posts/flink-kafka/flink-mapr-streams.png" alt="MapR Streams and Flink" />.</p>

<!-- more -->


<h3>Prerequisites</h3>

<ul>
<li>MapR 5.2

<ul>
<li>You can use <a href="https://www.mapr.com/products/mapr-sandbox-hadoop">MapR Converged Data Platform Sandbox</a></li>
</ul>
</li>
<li>MapR Client installed on your development host

<ul>
<li><a href="http://maprdocs.mapr.com/home/AdvancedInstallation/SettingUptheClient-install-mapr-client.html">Installation and Configuration steps</a></li>
</ul>
</li>
<li>Git</li>
<li>Maven 3.x or later</li>
</ul>


<h2>Create your Flink Streaming Project</h2>

<p>The first step is to create an Java application, the easiest is to use the flink-quickstart-java archetype, that contains the core dependencies and packaging tasks. This article is similar with the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/quickstart/run_example_quickstart.html">Apache Flink Quick Start Example</a>, with a clear focus on data input and output with MapR Streams.</p>

<p>In this application we will create two jobs:</p>

<ul>
<li><code>WriteToKafka</code> : that generates random string and post them to a MapR Streams Topic using the Kafka Flink Connector and its Producer API.</li>
<li><code>ReadFromKafka</code> : that reads the same topic and print the messages in the standard output using the Kafka Flink Connector and its Consumer. API.</li>
</ul>


<p>The full project is available on GitHub:</p>

<ul>
<li><a href="https://github.com/mapr-demos/mapr-streams-flink-demo">MapR Streams Flink Demo</a></li>
</ul>


<p>Let’s create the project using Apache Maven:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>mvn archetype:generate \
</span><span class='line'>      -DarchetypeGroupId=org.apache.flink\
</span><span class='line'>      -DarchetypeArtifactId=flink-quickstart-java \
</span><span class='line'>      -DarchetypeVersion=1.1.2 \
</span><span class='line'>      -DgroupId=com.mapr.demos \
</span><span class='line'>      -DartifactId=mapr-streams-flink-demo \
</span><span class='line'>      -Dversion=1.0-SNAPSHOT \
</span><span class='line'>      -DinteractiveMode=false 
</span></code></pre></td></tr></table></div></figure>


<p>Maven will create the following structure:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ tree mapr-streams-flink-demo/
</span><span class='line'>mapr-streams-flink-demo/
</span><span class='line'>├── pom.xml
</span><span class='line'>└── src
</span><span class='line'>    └── main
</span><span class='line'>        ├── java
</span><span class='line'>        │   └── com
</span><span class='line'>        │       └── mapr
</span><span class='line'>        │           └── demos
</span><span class='line'>        │               ├── BatchJob.java
</span><span class='line'>        │               ├── SocketTextStreamWordCount.java
</span><span class='line'>        │               ├── StreamingJob.java
</span><span class='line'>        │               └── WordCount.java
</span><span class='line'>        └── resources
</span><span class='line'>            └── log4j.properties</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>This project is configured to create a Jar file that contains your flink project code and also includes all dependencies needed to run it.</p>

<p>The project contains some other sample jobs, we do not need them for this article, you can either keep them to educational purposes or simply remove them from the project.</p>

<h2>Add Kafka &amp; MapR Streams Dependencies</h2>

<p>Open the <code>pom.xml</code> and add the following dependencies to your project:</p>

<p><strong>1- Add MapR Maven Repository</strong></p>

<p>In the <code>&lt;repositories&gt;</code> element add :</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>   &lt;repository&gt;
</span><span class='line'>     &lt;id&gt;mapr-releases&lt;/id&gt;
</span><span class='line'>     &lt;url&gt;http://repository.mapr.com/maven/&lt;/url&gt;
</span><span class='line'>     &lt;snapshots&gt;&lt;enabled&gt;false&lt;/enabled&gt;&lt;/snapshots&gt;
</span><span class='line'>     &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt;
</span><span class='line'>   &lt;/repository&gt;</span></code></pre></td></tr></table></div></figure>


<p><strong>2- Add MapR Streams libraries</strong></p>

<p>In the <code>&lt;dependencies&gt;</code> element:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> &lt;dependency&gt;
</span><span class='line'>   &lt;groupId&gt;com.mapr.streams&lt;/groupId&gt;
</span><span class='line'>   &lt;artifactId&gt;mapr-streams&lt;/artifactId&gt;
</span><span class='line'>   &lt;version&gt;5.2.0-mapr&lt;/version&gt;
</span><span class='line'> &lt;/dependency&gt;
</span><span class='line'> &lt;dependency&gt;
</span><span class='line'>   &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
</span><span class='line'>   &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
</span><span class='line'>   &lt;version&gt;0.9.0.0-mapr-1602&lt;/version&gt;
</span><span class='line'> &lt;/dependency&gt;</span></code></pre></td></tr></table></div></figure>


<p><strong>3- Add Flink Kafka Connector libraries</strong></p>

<p>As a first step, we have to add the Flink Kafka connector as a dependency so that we can use the Kafka sink. Add this to the pom.xml file in the dependencies section:</p>

<p>You must add now the Flink Kafka Connector dependency to use the Kafka sink. Add the following entry in the <code>&lt;dependencies&gt;</code> element:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> &lt;dependency&gt;
</span><span class='line'>      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
</span><span class='line'>      &lt;artifactId&gt;flink-connector-kafka-0.9_2.10&lt;/artifactId&gt;
</span><span class='line'>      &lt;version&gt;${flink.version}&lt;/version&gt;
</span><span class='line'> &lt;/dependency&gt;</span></code></pre></td></tr></table></div></figure>


<p><strong>4- Exclude Kafka Client to allow use of MapR Streams Client</strong></p>

<p>As you may know, MapR Streams uses the Kafka 0.9.0 API to produce and consume messages. So we need now to remove (exclude) tha Apache Kafka Client API to be sure that Flink use MapR Streams.</p>

<p>In the Flink Kafka Connector dependency add the following exclusion:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  &lt;dependency&gt;
</span><span class='line'>    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
</span><span class='line'>    &lt;artifactId&gt;flink-connector-kafka-0.9_2.10&lt;/artifactId&gt;
</span><span class='line'>    &lt;version&gt;${flink.version}&lt;/version&gt;
</span><span class='line'>      &lt;exclusions&gt;
</span><span class='line'>        &lt;exclusion&gt;
</span><span class='line'>          &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
</span><span class='line'>          &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
</span><span class='line'>        &lt;/exclusion&gt;
</span><span class='line'>        &lt;exclusion&gt;
</span><span class='line'>          &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
</span><span class='line'>          &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt;
</span><span class='line'>        &lt;/exclusion&gt;
</span><span class='line'>      &lt;/exclusions&gt;
</span><span class='line'>  &lt;/dependency&gt;</span></code></pre></td></tr></table></div></figure>


<p>The Flink project is now ready to use the DataStream using the Kafka Connector so you can send and receive messages from MapR Streams.</p>

<p>Let’s now create a Stream in MapR and write some simple Flink code to use it.</p>

<h2>Create the MapR Streams and Topic</h2>

<p>A stream is a collection of topics that you can manage as a group by:</p>

<ol>
<li>Setting security policies that apply to all topics in that stream</li>
<li>Setting a default number of partitions for each new topic that is created in the stream</li>
<li>Set a time-to-live for messages in every topic in the stream</li>
</ol>


<p>You can find more information about MapR Streams concepts in the <a href="http://maprdocs.mapr.com/51/MapR_Streams/concepts.html">documentation</a>.</p>

<p>On your Mapr Cluster or Sandbox run the following commands:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ maprcli stream create -path /apps/application-stream -produceperm p -consumeperm p -topicperm p
</span><span class='line'>$ maprcli stream topic create -path /apps/application-stream -topic flink-demo </span></code></pre></td></tr></table></div></figure>


<h3>Install and use MapR Kafka utilities</h3>

<p>Install <code>the mapr-kafka</code> package on your cluster :</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>yum install mapr-kafka</span></code></pre></td></tr></table></div></figure>


<p>Open two terminal windows and run the producer and consumer kafka utilities using the following commands:</p>

<p>Producer</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-producer.sh --broker-list this.will.be.ignored:9092 --topic /apps/application-stream:flink-demo</span></code></pre></td></tr></table></div></figure>


<p>Consumer</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/opt/mapr/kafka/kafka-0.9.0/bin/kafka-console-consumer.sh --new-consumer --bootstrap-server this.will.be.ignored:9092 --topic /apps/application-stream:flink-demo</span></code></pre></td></tr></table></div></figure>


<p>In the producer window, you can post some messages and see them in the consumer windows. We will use these tools to follow the interactions between MapR Streams and Flink.</p>

<h2>Write your Flink application</h2>

<p>Let’s now use the Flink Kafka Connector to send messages to MapR Streams and consume them.</p>

<h3>Producer</h3>

<p>The producer generates messages using the <code>SimpleStringGenerator()</code> class and send the string to the <code>/apps/application-stream:flink-demo</code> topic.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public static void main(String[] args) throws Exception {
</span><span class='line'>    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
</span><span class='line'>
</span><span class='line'>    Properties properties = new Properties();
</span><span class='line'>    // properties.setProperty("bootstrap.servers", "&lt;kafka-broker&gt;:9092"); // not used by MapR Streams
</span><span class='line'>    properties.setProperty("streams.buffer.max.time.ms", "200");
</span><span class='line'>
</span><span class='line'>    DataStream&lt;String&gt; stream = env.addSource(new SimpleStringGenerator());
</span><span class='line'>    stream.addSink(new FlinkKafkaProducer09&lt;&gt;("/apps/application-stream:flink-demo", new SimpleStringSchema(), properties));
</span><span class='line'>
</span><span class='line'>    env.execute();
</span><span class='line'>  }
</span><span class='line'>    </span></code></pre></td></tr></table></div></figure>


<p>The <code>SimpleStringGenerator()</code> method code is available <a href="https://github.com/mapr-demos/mapr-streams-flink-demo/blob/master/src/main/java/com/mapr/demos/WriteToKafka.java#L46-L61">here</a>.</p>

<p>The main steps are:</p>

<ul>
<li>create a new <code>StreamExecutionEnvironment</code> the basis of any Flink application</li>
<li>create a new <code>DataStream</code> in the application environment, the <code>SimpleStringGenerator</code> class implements the <code>[SourceFunction](https://ci.apache.org/projects/flink/flink-docs-release-1.1/api/java/)</code> the base interface for all streams data sources in Flink.</li>
<li>add the <code>FlinkKafkaProducer09</code> sink to the streams; since MapR Streams is based on Kafka API 0.9, it is possible to use the FlinkKafkaProducer09 class; with 2 small differences:

<ul>
<li>the broker list (first parameter) is not used since MapR Streams use the cluster location defined in the <code>/opt/mapr/conf/mapr-clusters.conf</code> class.</li>
<li>the topic name include the path and name of the MapR Stream stream in which the topic is located for example <code>/apps/application-stream:flink-demo</code></li>
</ul>
</li>
</ul>


<h3>Consumer</h3>

<p>The consumer simply reads the messages from the <code>/apps/application-stream:flink-demo</code> topic, and print them into the console.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  public static void main(String[] args) throws Exception {
</span><span class='line'>    // create execution environment
</span><span class='line'>    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
</span><span class='line'>
</span><span class='line'>    Properties properties = new Properties();
</span><span class='line'>    // properties.setProperty("bootstrap.servers", "&lt;kafka-broker&gt;:9092"); // not used by MapR Streams
</span><span class='line'>    properties.setProperty("group.id", "flink_consumer");
</span><span class='line'>
</span><span class='line'>    DataStream&lt;String&gt; stream = env.addSource(new FlinkKafkaConsumer09&lt;&gt;(
</span><span class='line'>      "/apps/application-stream:flink-demo", new SimpleStringSchema(), properties) );
</span><span class='line'>
</span><span class='line'>    stream.map(new MapFunction&lt;String, String&gt;() {
</span><span class='line'>      private static final long serialVersionUID = -6867736771747690202L;
</span><span class='line'>
</span><span class='line'>      @Override
</span><span class='line'>      public String map(String value) throws Exception {
</span><span class='line'>        return "Stream Value: " + value;
</span><span class='line'>      }
</span><span class='line'>    }).print();
</span><span class='line'>
</span><span class='line'>    env.execute();
</span><span class='line'>  }
</span><span class='line'>  ```
</span><span class='line'>  
</span><span class='line'>The main steps are:
</span><span class='line'>
</span><span class='line'>* create a new `StreamExecutionEnvironment` the basis of any Flink application
</span><span class='line'>* create a set of properties with the consumer information, in this application we can only set the consumer `group.id`. Note that the `bootstrap.servers` property is not used by MapR Streams, so no need to set it.
</span><span class='line'>* use the `FlinkKafkaConsumer09` to get the message from the MapR Streams topic `/apps/application-stream:flink-demo`
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>## Build and Run the application
</span><span class='line'>
</span><span class='line'>Let’s run the application directly from Maven (or from your favorite IDE).
</span><span class='line'>
</span><span class='line'>1- Build the project:
</span></code></pre></td></tr></table></div></figure>


<p>$ mvn clean package</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>2- Run the Flink Producer Job
</span></code></pre></td></tr></table></div></figure>


<p>$ mvn exec:java -Dexec.mainClass=com.mapr.demos.WriteToKafka</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>3- Run the Flink Consumer Job
</span></code></pre></td></tr></table></div></figure>


<p>$ mvn exec:java -Dexec.mainClass=com.mapr.demos.ReadFromKafka
&#8220;`</p>

<p>In the terminal, you should see the messages generated from the producer</p>

<p>You can now deploy and execute this job on your Flink cluster.</p>

<h2>Conclusion</h2>

<p>In this article you have learned how to use Flink with MapR Streams to write and read data streams. The key element is the configuration of the Maven Dependencies to configure the project to use MapR Streams libraries instead of Kafka ones.</p>

<p>This was originally published on the <a href="https://www.mapr.com/blog/getting-started-apache-flink-and-mapr-streams">MapR blog here</a>.</p>

<p>Learn about what Apache Flink can do and how it maintains consistency and provides flexibility in the &ldquo;<a href="https://www.mapr.com/introduction-to-apache-flink">Introduction to Apache Flink</a>&rdquo; ebook.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With Apache Flink and Kafka]]></title>
    <link href="http://tgrall.github.io/blog/2016/10/12/getting-started-with-apache-flink-and-kafka/"/>
    <updated>2016-10-12T04:54:17+02:00</updated>
    <id>http://tgrall.github.io/blog/2016/10/12/getting-started-with-apache-flink-and-kafka</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p><a href="https://flink.apache.org/">Apache Flink</a> is an open source platform for distributed stream and batch data processing. Flink is a streaming data flow engine with several APIs to create data streams oriented application.</p>

<p>It is very common for Flink applications to use <a href="http://kafka.apache.org/">Apache Kafka</a> for data input and output. This article will guide you into  the steps to use Apache Flink with Kafka.</p>

<p><img class="center" src="http://tgrall.github.io/images/posts/flink-kafka/flink-kafka.png" title="Flink-Kafka" ></p>

<!-- more -->


<h3>Prerequisites</h3>

<ul>
<li>Apache Kafka 0.9.x</li>
<li>Git</li>
<li>Maven 3.x or later</li>
</ul>


<h2>Create your Flink Streaming Project</h2>

<p>The first step is to create an Java application, the easiest is to use the flink-quickstart-java archetype, that contains the core dependencies and packaging tasks. This article is similar with the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/quickstart/run_example_quickstart.html">Apache Flink Quick Start Example</a>, with a clear focus on data input and output with MapR Streams.</p>

<p>In this application we will create two jobs:</p>

<ul>
<li><code>WriteToKafka</code> : that generates random string and post them to a MapR Streams Topic using the Kafka Flink Connector and its Producer API.</li>
<li><code>ReadFromKafka</code> : that reads the same topic and print the messages in the standard output using the Kafka Flink Connector and its Consumer. API.</li>
</ul>


<p>The full project is available on GitHub:</p>

<ul>
<li><a href="https://github.com/tgrall/kafka-flink-101">Flink and Kakfa Application</a></li>
</ul>


<p>Let’s create the project using Apache Maven:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>mvn archetype:generate <span class="se">\</span>
</span><span class='line'>      -DarchetypeGroupId<span class="o">=</span>org.apache.flink<span class="se">\</span>
</span><span class='line'>      -DarchetypeArtifactId<span class="o">=</span>flink-quickstart-java <span class="se">\</span>
</span><span class='line'>      -DarchetypeVersion<span class="o">=</span>1.1.2 <span class="se">\</span>
</span><span class='line'>      -DgroupId<span class="o">=</span>com.grallandco.demos <span class="se">\</span>
</span><span class='line'>      -DartifactId<span class="o">=</span>kafka-flink-101 <span class="se">\</span>
</span><span class='line'>      -Dversion<span class="o">=</span>1.0-SNAPSHOT <span class="se">\</span>
</span><span class='line'>      -DinteractiveMode<span class="o">=</span><span class="nb">false</span>
</span></code></pre></td></tr></table></div></figure>


<p>Maven will create the following structure:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>tree kafka-flink-101/
</span><span class='line'>kafka-flink-101/
</span><span class='line'>├── pom.xml
</span><span class='line'>└── src
</span><span class='line'>    └── main
</span><span class='line'>        ├── java
</span><span class='line'>        │   └── com
</span><span class='line'>        │       └── grallandco
</span><span class='line'>        │           └── demos
</span><span class='line'>        │               ├── BatchJob.java
</span><span class='line'>        │               ├── SocketTextStreamWordCount.java
</span><span class='line'>        │               ├── StreamingJob.java
</span><span class='line'>        │               └── WordCount.java
</span><span class='line'>        └── resources
</span><span class='line'>            └── log4j.properties
</span><span class='line'>
</span><span class='line'><span class="m">7</span> directories, <span class="m">6</span> files
</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>This project is configured to create a Jar file that contains your flink project code and also includes all dependencies needed to run it.</p>

<p>The project contains some other sample jobs, we do not need them for this article, you can either keep them to educational purposes or simply remove them from the project.</p>

<h2>Add Kafka Connector</h2>

<p>Open the <code>pom.xml</code> and add the following dependencies to your project:</p>

<p>As a first step, we have to add the Flink Kafka connector as a dependency so that we can use the Kafka sink. Add this to the pom.xml file in the dependencies section:</p>

<p>You must add now the Flink Kafka Connector dependency to use the Kafka sink. Add the following entry in the <code>&lt;dependencies&gt;</code> element:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'> &lt;dependency&gt;
</span><span class='line'>      &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
</span><span class='line'>      &lt;artifactId&gt;flink-connector-kafka-0.9_2.10&lt;/artifactId&gt;
</span><span class='line'>      &lt;version&gt;<span class="k">${</span><span class="nv">flink</span><span class="p">.version</span><span class="k">}</span>&lt;/version&gt;
</span><span class='line'> &lt;/dependency&gt;
</span></code></pre></td></tr></table></div></figure>


<p>The Flink project is now ready to use the DataStream using the Kafka Connector so you can send and receive messages from Apache Kafka.</p>

<h2>Install and Start Kafka</h2>

<p>Download Kafka, enter the following commands in your terminal:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>curl -O http://www.us.apache.org/dist/kafka/0.9.0.0/kafka_2.11-0.9.0.0.tgz
</span><span class='line'>tar -xzf kafka_2.11-0.9.0.0.tgz
</span><span class='line'><span class="nb">cd </span>kafka_2.11-0.9.0.0
</span></code></pre></td></tr></table></div></figure>


<p>Kafka uses ZooKeeper, if you do not have Zookeeper running, you can start it using the following command:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>./bin/zookeeper-server-start.sh config/zookeeper.properties
</span></code></pre></td></tr></table></div></figure>


<p>Start a Kafka broker by running the following command in a new terminal:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>./bin/kafka-server-start.sh config/server.properties
</span></code></pre></td></tr></table></div></figure>


<p>In another terminal, run the following command to create a Kafka topic called <code>flink-demo</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor <span class="m">1</span> --partitions <span class="m">1</span> --topic flink-demo
</span></code></pre></td></tr></table></div></figure>


<p>Use the Kafka tools to post and consume messages to the <code>flink-demo</code> topic.</p>

<p>Producer</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic flink-demo
</span></code></pre></td></tr></table></div></figure>


<p>Consumer</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic flink-demo --from-beginning
</span></code></pre></td></tr></table></div></figure>


<p>In the producer window, you can post some messages and see them in the consumer windows. We will use these tools to follow the interactions between Kafka and Flink.</p>

<h2>Write your Flink application</h2>

<p>Let’s now use the Flink Kafka Connector to send messages to Kafka and consume them.</p>

<h3>Producer</h3>

<p>The producer generates messages using the <code>SimpleStringGenerator()</code> class and send the string to the <code>flink-demo</code> topic.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>  public static void main<span class="o">(</span>String<span class="o">[]</span> args<span class="o">)</span> throws Exception <span class="o">{</span>
</span><span class='line'>    StreamExecutionEnvironment <span class="nv">env</span> <span class="o">=</span> StreamExecutionEnvironment.getExecutionEnvironment<span class="o">()</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>    Properties <span class="nv">properties</span> <span class="o">=</span> new Properties<span class="o">()</span><span class="p">;</span>
</span><span class='line'>    properties.setProperty<span class="o">(</span><span class="s2">&quot;bootstrap.servers&quot;</span>, “localhost:9092<span class="s2">&quot;); </span>
</span><span class='line'>
</span><span class='line'><span class="s2">    DataStream&lt;String&gt; stream = env.addSource(new SimpleStringGenerator());</span>
</span><span class='line'><span class="s2">    stream.addSink(new FlinkKafkaProducer09&lt;&gt;(&quot;</span>flink-demo<span class="err">&quot;</span>, new SimpleStringSchema<span class="o">()</span>, properties<span class="o">))</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>    env.execute<span class="o">()</span><span class="p">;</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p>The <code>SimpleStringGenerator()</code> method code is available <a href="https://github.com/tgrall/kafka-flink-101/blob/master/src/main/java/com/grallandco/demos/WriteToKafka.java#L45-L60">here</a>.</p>

<p>The main steps are:</p>

<ul>
<li>create a new <code>StreamExecutionEnvironment</code> the basis of any Flink application</li>
<li>create a new <code>DataStream</code> in the application environment, the <code>SimpleStringGenerator</code> class implements the <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.1/api/java/">SourceFunction</a> the base interface for all streams data sources in Flink.</li>
<li>add the <code>FlinkKafkaProducer09</code> sink to the topic.</li>
</ul>


<h3>Consumer</h3>

<p>The consumer simply reads the messages from the <code>flink-demo</code> topic, and print them into the console.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>  public static void main<span class="o">(</span>String<span class="o">[]</span> args<span class="o">)</span> throws Exception <span class="o">{</span>
</span><span class='line'>    // create execution environment
</span><span class='line'>    StreamExecutionEnvironment <span class="nv">env</span> <span class="o">=</span> StreamExecutionEnvironment.getExecutionEnvironment<span class="o">()</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>    Properties <span class="nv">properties</span> <span class="o">=</span> new Properties<span class="o">()</span><span class="p">;</span>
</span><span class='line'>    properties.setProperty<span class="o">(</span><span class="s2">&quot;bootstrap.servers&quot;</span>, “localhost:9092<span class="s2">&quot;);</span>
</span><span class='line'><span class="s2">    properties.setProperty(&quot;</span>group.id<span class="s2">&quot;, &quot;</span>flink_consumer<span class="s2">&quot;);</span>
</span><span class='line'>
</span><span class='line'><span class="s2">    DataStream&lt;String&gt; stream = env.addSource(new FlinkKafkaConsumer09&lt;&gt;(</span>
</span><span class='line'><span class="s2">     &quot;</span>flink-demo<span class="s2">&quot;, new SimpleStringSchema(), properties) );</span>
</span><span class='line'>
</span><span class='line'><span class="s2">    stream.map(new MapFunction&lt;String, String&gt;() {</span>
</span><span class='line'><span class="s2">      private static final long serialVersionUID = -6867736771747690202L;</span>
</span><span class='line'>
</span><span class='line'><span class="s2">      @Override</span>
</span><span class='line'><span class="s2">      public String map(String value) throws Exception {</span>
</span><span class='line'><span class="s2">        return &quot;</span>Stream Value: <span class="err">&quot;</span> + value<span class="p">;</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>    <span class="o">})</span>.print<span class="o">()</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>    env.execute<span class="o">()</span><span class="p">;</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The main steps are:</p>

<ul>
<li>create a new <code>StreamExecutionEnvironment</code> the basis of any Flink application</li>
<li>create a set of properties with the consumer information, in this application we can only set the consumer <code>group.id</code>.</li>
<li>use the <code>FlinkKafkaConsumer09</code> to get the message from the topic <code>flink-demo</code></li>
</ul>


<h2>Build and Run the application</h2>

<p>Let’s run the application directly from Maven (or from your favorite IDE).</p>

<p>1- Build the project:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>mvn clean package
</span></code></pre></td></tr></table></div></figure>


<p>2- Run the Flink Producer Job</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>mvn <span class="nb">exec</span>:java -Dexec.mainClass<span class="o">=</span>com.mapr.demos.WriteToKafka
</span></code></pre></td></tr></table></div></figure>


<p>3- Run the Flink Consumer Job</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>mvn <span class="nb">exec</span>:java -Dexec.mainClass<span class="o">=</span>com.mapr.demos.ReadFromKafka
</span></code></pre></td></tr></table></div></figure>


<p>In the terminal, you should see the messages generated from the producer</p>

<p>You can now deploy and execute this job on your Flink cluster.</p>

<h2>Conclusion</h2>

<p>In this article you have learned how to use Flink with kafka to write and read data streams.</p>

<p>Learn about what Apache Flink can do and how it maintains consistency and provides flexibility in the &ldquo;<a href="https://www.mapr.com/introduction-to-apache-flink">Introduction to Apache Flink</a>&rdquo; ebook.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Streaming Analytics in a Digitally Industrialized World]]></title>
    <link href="http://tgrall.github.io/blog/2016/09/26/streaming-analytics-in-a-digitally-industrialized-world/"/>
    <updated>2016-09-26T15:30:20+02:00</updated>
    <id>http://tgrall.github.io/blog/2016/09/26/streaming-analytics-in-a-digitally-industrialized-world</id>
    <content type="html"><![CDATA[<p>Get an introduction to streaming analytics, which allows you real-time insight from captured events and big data. There are applications across industries, from finance to wine making, though there are two primary challenges to be addressed.</p>

<p>Did you know that a plane flying from Texas to London can generate 30 million data points per flight? As Jim Daily of GE Aviation notes, that equals <a href="https://www.ge.com/digital/blog/industrial-iot-improving-airline-economics">10 billion data points</a> in one year. And we’re talking about one plane alone. So you can understand why <a href="http://cloudblog.ericsson.com/cloud-scalability-combined-with-speed-inside-ges-cloud-transformation">another top GE executive recently told Ericsson Business Review</a> that &ldquo;Cloud is the future of IT,&rdquo; with a focus on supporting challenging applications in industries such as aviation and energy.</p>

<!-- more -->


<h3>The benefits of big data</h3>

<p>Today, thanks to modern big data platforms, many companies are able to take advantage of the same methods as industry giant GE to store, process, and analyze massive amounts of data. This means that you can also capture core business data, such as that coming from a CRM system, or traffic sensors, or say jet engines, and associate it to other data such as social, application, blog or industrial data. Ultimately, this will result in greater data insights and can enable things like better customer segmentation and prediction.</p>

<p>Sounds great, right? There’s a catch. The challenge is that these data are processed in batch mode, meaning that you have to wait a few hours or even days to access relevant KPI’s and insights. Not only is there a delay, but analysis is based on data that’s out of date – even if only by a few hours.</p>

<h3>Analysis of big event streams</h3>

<p>That’s where streaming analytics comes in. Streaming analytics is the <a href="http://searchcloudapplications.techtarget.com/opinion/Streaming-analytics-lets-you-view-the-past-to-see-the-future?utm_medium=EM&amp;asrc=EM_NLN_58517129&amp;utm_campaign=20160606_Seeing%20the%20future...with%20the%20past?_fchurchville&amp;utm_source=NLN&amp;track=NL-1839&amp;ad=908120&amp;src=908120">analysis of large event streams</a>, which is data that is in constant movement. These streams can include actions that can be incredibly small, say one click, yet result in an explosion of data. The benefit is that you can capture events and data as they happen, delivering value to the enterprise in near real time.</p>

<p>What does streaming analytics look like regarding the previously mentioned CRM system? It means that an enterprise can get immediate feedback on something like a specific marketing campaign, website update, or product alteration. When a user clicks within one of these realms and an order is immediately processed, that information is pushed out in real time to various tools, allowing the enterprise to adjust its customer interaction.</p>

<h3>Streaming analytics in practice</h3>

<p>Where else is streaming analytics applicable? Think about the benefits of fraud detection in the financial sector, or the growth of sensors in manufacturing, or collection of data from large scale machines. One example is where Ericsson has collaborated with <a href="http://cloudblog.ericsson.com/can-the-networked-society-and-iot-make-better-wine">vintners in Germany, using IoT to improve traditional harvesting methods</a> through greater precision and speed of response to outside factors. In these streaming analytics scenarios, the key is “<a href="https://www.mapr.com/blog/lets-get-real-acting-data-real-time">High-Frequency Decisioning,</a>” where you move from knowing to doing in real-time synergy. Time to action is compressed, resulting in dramatic results like achieving greater user satisfaction, higher revenue, or reduced risk.</p>

<h3>Two primary challenges with streaming analytics</h3>

<p>Streaming analytics is not without challenges. Systems must capture events in near real time, at scale, and in a distributed fashion. And of course, events must be stored and processed in real time. Since big data is generated one event at a time, you need to have an incredibly powerful storage and processing layer, which can provide deep analytics and rich features like machine learning systems.</p>

<p>To take on the first challenge, new messaging systems like <a href="http://kafka.apache.org">Apache Kafka</a> and <a href="https://www.mapr.com/products/mapr-streams">MapR Streams</a> provide a common API for developers to publish and subscribe to any event. And to process and store data in real-time, one of the most efficient methods would be to use a <a href="https://www.mapr.com/products/mapr-fs">distributed file system</a> and <a href="https://www.mapr.com/products/mapr-db-in-hadoop-nosql">NoSQL databases</a>. This provides horizontal scalability and flexibility. A storage system must also process and do analytics calculations in real time in a distributed manner.</p>

<p>Tools such as <a href="https://www.mapr.com/products/apache-spark">Apache Spark</a> and <a href="http://flink.apache.org">Flink</a> come to mind, which provide rich analytical functions that can be integrated with any tool, including real time alerting systems. It is also interesting to mention that all the events, data, that are now stored in real time continue to be accessible with traditional analytics tools; thanks to the distributed, and scalable distributed SQL Engines provided in modern big data platforms.</p>

<p>This was originally published on the <a href="http://cloudblog.ericsson.com/streaming-analytics-of-big-data-in-real-time">Ericsson Cloud blog here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting Up Spark Dynamic Allocation on MapR]]></title>
    <link href="http://tgrall.github.io/blog/2016/09/01/setting-up-spark-dynamic-allocation-on-mapr/"/>
    <updated>2016-09-01T11:15:57+02:00</updated>
    <id>http://tgrall.github.io/blog/2016/09/01/setting-up-spark-dynamic-allocation-on-mapr</id>
    <content type="html"><![CDATA[<p>Apache Spark can use various cluster manager to execute application (Stand Alone, YARN, Apache Mesos). When you install Apache Spark on MapR you can submit application in a Stand Alone mode or using YARN.</p>

<p>This article focuses on YARN and Dynamic Allocation, a feature that lets Spark add or remove executors dynamically based on the workload. You can find more information about this feature in this presentation from Databricks:</p>

<ul>
<li><a href="http://www.slideshare.net/databricks/dynamic-allocation-in-spark">Dynamic Allocation in Spark</a></li>
</ul>


<p>Let’s see how to configure Spark and YARN to use dynamic allocation (that is disabled by default).</p>

<!-- more -->


<h4>Prerequisites</h4>

<ul>
<li>MapR Converged Data Platform Cluster</li>
<li>Apache Spark for MapR installed</li>
</ul>


<p>This example has been described for MapR 5.2 with Apache Spark 1.6.1, you just need to adapt the version to your environment.</p>

<h3>Enabling Dynamic Allocation in Apache Spark</h3>

<p>The first thing to do is to enable Dynamic Allocation in Spark, for this you need to edit the spark configuration file on each Spark node.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/opt/mapr/spark/spark-1.6.1/conf/spark-defaults.conf</span></code></pre></td></tr></table></div></figure>


<p>and add the following entries:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>spark.dynamicAllocation.enabled = true
</span><span class='line'>spark.shuffle.service.enabled = true
</span><span class='line'>spark.dynamicAllocation.minExecutors = 5 
</span><span class='line'>spark.executor.instances = 0</span></code></pre></td></tr></table></div></figure>


<p>You can find additional configuration options in the <a href="http://spark.apache.org/docs/1.6.1/configuration.html#dynamic-allocation">Apache Spark Documentation</a>.</p>

<h3>Enabling Spark External Shuffle for YARN</h3>

<p>You have now to edit YARN configuration to add information about Spark Shuffle Service, edit the following file, on each YARN node:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/opt/mapr/hadoop/hadoop-2.7.0/etc/hadoop/yarn-site.xml</span></code></pre></td></tr></table></div></figure>


<p>add these properties:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>  &lt;property&gt;
</span><span class='line'>    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;mapreduce_shuffle,mapr_direct_shuffle,spark_shuffle&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;
</span><span class='line'>  &lt;property&gt;
</span><span class='line'>    &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt;
</span><span class='line'>    &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;
</span><span class='line'>  &lt;/property&gt;</span></code></pre></td></tr></table></div></figure>


<h4>Add Spark Shuffle to YARN classpath</h4>

<p>Spark Shuffle service must be added to the YARN classpath. The jar is located in the spark distribution:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/opt/mapr/spark/spark-1.6.1/lib/spark-1.6.1-mapr-1605-yarn-shuffle.jar</span></code></pre></td></tr></table></div></figure>


<p>To achieve this add the jar in the following folder on each node:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/opt/mapr/hadoop/hadoop-2.7.0/share/hadoop/yarn/lib</span></code></pre></td></tr></table></div></figure>


<p>You can either copyy the file or create a symlink:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ln -s /opt/mapr/spark/spark-1.6.1/lib/spark-1.6.1-mapr-1605-yarn-shuffle.jar /opt/mapr/hadoop/hadoop-2.7.0/share/hadoop/yarn/lib</span></code></pre></td></tr></table></div></figure>


<h4>Restart YARN</h4>

<p>Since you have changed the YARN configuration <em>you must restart your node managers</em> using the following command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ maprcli node services -name nodemanager -action restart -nodes [list of nodes]</span></code></pre></td></tr></table></div></figure>


<h3>Submitting a Spark Job</h3>

<p>Your MapR Cluster is not ready to use Spark dynamic allocation, this means that when you submit a job you do not need to specify any resource configuration, for example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/opt/mapr/spark/spark-1.6.1/bin/spark-submit \
</span><span class='line'>  --class com.mapr.demo.WordCountSorted \
</span><span class='line'>  --master yarn \
</span><span class='line'>  ~/spark-examples-1.0-SNAPSHOT.jar \
</span><span class='line'>  /mapr/my.cluster.com/input/4gb_txt_file.txt \
</span><span class='line'>  /mapr/my.cluster.com/user/mapr/output/</span></code></pre></td></tr></table></div></figure>


<p>note that you can still specify the resources, but in this case the dynamic allocation will not be used for this specific job, for example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>/opt/mapr/spark/spark-1.6.1/bin/spark-submit \
</span><span class='line'>  --class com.mapr.demo.WordCountSorted \
</span><span class='line'>  --master yarn \
</span><span class='line'>  --num-executors 3
</span><span class='line'>  --executor-memory 1G \
</span><span class='line'>  ~/spark-examples-1.0-SNAPSHOT.jar \
</span><span class='line'>  /mapr/my.cluster.com/input/4gb_txt_file.txt \
</span><span class='line'>  /mapr/my.cluster.com/user/mapr/output/</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Save MapR Streams Messages Into MapR DB JSON]]></title>
    <link href="http://tgrall.github.io/blog/2016/03/31/save-mapr-streams-messages-into-mapr-db-json/"/>
    <updated>2016-03-31T09:00:07+02:00</updated>
    <id>http://tgrall.github.io/blog/2016/03/31/save-mapr-streams-messages-into-mapr-db-json</id>
    <content type="html"><![CDATA[<p>In this article you will learn how to create a MapR Streams Consumer that saves all the messages into a <a href="http://maprdocs.mapr.com/51/#MapR-DB/JSON_DB/mapr_db_json_top.html">MapR-DB JSON Table</a>.</p>

<!-- more -->


<h3>Install and Run the sample MapR Streams application</h3>

<p>The steps to install and run the applications are the same as the one defined in the following article:</p>

<ul>
<li><a href="https://www.mapr.com/blog/getting-started-sample-programs-mapr-streams">MapR Streams application</a></li>
</ul>


<p>Once you have the default producer and consumer running in your environment using the commands:</p>

<p>Producer:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ java -cp $(mapr classpath):./mapr-streams-examples-1.0-SNAPSHOT-jar-with-dependencies.jar com.mapr.examples.Run producer</span></code></pre></td></tr></table></div></figure>


<p>Consumer:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ java -cp $(mapr classpath):./mapr-streams-examples-1.0-SNAPSHOT-jar-with-dependencies.jar com.mapr.examples.Run consumer</span></code></pre></td></tr></table></div></figure>


<h3>Save messages into MapR-DB JSON</h3>

<p>The <a href="https://github.com/mapr-demos/mapr-streams-sample-programs/blob/master/src/main/java/com/mapr/examples/DBConsumer.java">DBConsumer</a> class is a copy of the <a href="https://github.com/mapr-demos/mapr-streams-sample-programs/blob/master/src/main/java/com/mapr/examples/Consumer.java">Consumer</a> class with small changes to save the messages coming from the <code>/sample-stream:fast-messages</code> topic into a MapR-DB table named <code>/apps/fast-messages</code>.</p>

<p><strong>1- Add MapR-DB Maven dependency to your project</strong></p>

<p>Edit the <code>pom.xml</code> file and add the following entry in the <code>dependencies</code> tag:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>   <span class="nt">&lt;dependency&gt;</span>
</span><span class='line'>      <span class="nt">&lt;groupId&gt;</span>com.mapr.db<span class="nt">&lt;/groupId&gt;</span>
</span><span class='line'>      <span class="nt">&lt;artifactId&gt;</span>maprdb<span class="nt">&lt;/artifactId&gt;</span>
</span><span class='line'>      <span class="nt">&lt;version&gt;</span>5.1.0-mapr<span class="nt">&lt;/version&gt;</span>
</span><span class='line'>   <span class="nt">&lt;/dependency&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>This add support for:</p>

<ul>
<li><a href="http://ojai.io/">OJAI</a> Open JSON Application Interface</li>
<li><a href="http://maprdocs.mapr.com/51/#MapR-DB/JSON_DB/crud_with_maprdb_ojai_java_api.html">MapR-DB JSON API</a></li>
</ul>


<p><strong>2- Create and Get a JSON Table</strong></p>

<p>To save the messages, the application must access a JSON Table, for this just call the <code>MapRDB.getTable(TABLE_PATH)</code> method. If the table does not exist, create it with the <code>MapRDB.createTable(TABLE_PATH)</code>.</p>

<p>This is what the <a href="https://github.com/mapr-demos/mapr-streams-sample-programs/blob/master/src/main/java/com/mapr/examples/DBConsumer.java#L113-L119"><code>DBConsumer.getTable(TABLE_PATH)</code></a> method is doing.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>  <span class="kd">private</span> <span class="kd">static</span> <span class="n">Table</span> <span class="nf">getTable</span><span class="o">(</span><span class="n">String</span> <span class="n">tablePath</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span> <span class="o">!</span> <span class="n">MapRDB</span><span class="o">.</span><span class="na">tableExists</span><span class="o">(</span><span class="n">tablePath</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">return</span> <span class="n">MapRDB</span><span class="o">.</span><span class="na">createTable</span><span class="o">(</span><span class="n">tablePath</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">return</span> <span class="n">MapRDB</span><span class="o">.</span><span class="na">getTable</span><span class="o">(</span><span class="n">tablePath</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>When the DBConsumer starts the <a href="https://github.com/mapr-demos/mapr-streams-sample-programs/blob/master/src/main/java/com/mapr/examples/DBConsumer.java#L41"><code>getTable("/apps/fast-messages")</code></a> method is called.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>  <span class="n">Table</span> <span class="n">fastMessagesTable</span> <span class="o">=</span> <span class="n">getTable</span><span class="o">(</span><span class="s">&quot;/apps/fast-messages&quot;</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>The table <code>fastMessagesTable</code> is not available to the consumer.</p>

<p><strong>3- Save messages into the JSON Table</strong></p>

<p>Messages can be saved into the table using the <a href="https://github.com/mapr-demos/mapr-streams-sample-programs/blob/master/src/main/java/com/mapr/examples/DBConsumer.java#L78-L81">MapR-DB JSON Java API</a>.</p>

<p>The producer sends the message as JSON String that is converted into a JSON object names <code>msg</code>. This object can be used to create an OJAI Document:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>  <span class="n">Document</span> <span class="n">messageDocument</span> <span class="o">=</span> <span class="n">MapRDB</span><span class="o">.</span><span class="na">newDocument</span><span class="o">(</span><span class="n">msg</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>To be saved into MapR-DB, a document must have a <code>_id</code> field. In this example let’s use the message number generated by the producer <em>(JSON field <code>k</code>)</em>.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>  <span class="n">messageDocument</span><span class="o">.</span><span class="na">setId</span><span class="o">(</span> <span class="n">Integer</span><span class="o">.</span><span class="na">toString</span><span class="o">(</span><span class="n">messageDocument</span><span class="o">.</span><span class="na">getInt</span><span class="o">(</span><span class="s">&quot;k&quot;</span><span class="o">)));</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let’s now save the document into the table:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>  <span class="n">fastMessagesTable</span><span class="o">.</span><span class="na">insertOrReplace</span><span class="o">(</span> <span class="n">messageDocument</span> <span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>Each time the producer will be executed, the message number counter will be initialized to 0. So the document _id will be the same, and the document into the table must be replaced; this is why the <code>insertOrReplace</code> method is used.</p>

<p>Let’s run the new consumer.</p>

<p><strong>4- Run the DBConsumer</strong></p>

<p>To run the DBConsumer just pass the parameter <code>dbconsumer</code> as follow:</p>

<p>Consumer:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">$</span> <span class="n">java</span> <span class="o">-</span><span class="n">cp</span> <span class="nf">$</span><span class="o">(</span><span class="n">mapr</span> <span class="n">classpath</span><span class="o">):./</span><span class="n">mapr</span><span class="o">-</span><span class="n">streams</span><span class="o">-</span><span class="n">examples</span><span class="o">-</span><span class="mf">1.0</span><span class="o">-</span><span class="n">SNAPSHOT</span><span class="o">-</span><span class="n">jar</span><span class="o">-</span><span class="n">with</span><span class="o">-</span><span class="n">dependencies</span><span class="o">.</span><span class="na">jar</span> <span class="n">com</span><span class="o">.</span><span class="na">mapr</span><span class="o">.</span><span class="na">examples</span><span class="o">.</span><span class="na">Run</span> <span class="n">dbconsumer</span>
</span></code></pre></td></tr></table></div></figure>


<p>Note that a new <a href="https://github.com/mapr-demos/mapr-streams-sample-programs/blob/master/src/main/java/com/mapr/examples/DBConsumer.java#L48-L54">group is created</a> to be sure that messages are read by the two different consumers (Consumer and DBConsumer).</p>

<p><strong>5- Query the messages saved into MapR-DB</strong></p>

<p>Messages are saved into the <code>/apps/fast-messages</code> table, let’s used the MapR DBShell to query the data. On your cluster run the following commands, as <code>mapr</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">$</span> <span class="n">mapr</span> <span class="n">dbshell</span>
</span><span class='line'><span class="n">maprdb</span> <span class="nl">mapr:</span><span class="o">&gt;</span> <span class="n">find</span> <span class="o">/</span><span class="n">apps</span><span class="o">/</span><span class="n">fast</span><span class="o">-</span><span class="n">messages</span> <span class="o">--</span><span class="n">id</span> <span class="mi">100</span>
</span><span class='line'><span class="o">{</span><span class="s">&quot;_id&quot;</span><span class="o">:</span><span class="s">&quot;100&quot;</span><span class="o">,</span><span class="s">&quot;type&quot;</span><span class="o">:</span><span class="s">&quot;test&quot;</span><span class="o">,</span><span class="s">&quot;t&quot;</span><span class="o">:</span><span class="mf">64986.787</span><span class="o">,</span><span class="s">&quot;k&quot;</span><span class="o">:{</span><span class="s">&quot;$numberLong&quot;</span><span class="o">:</span><span class="mi">100</span><span class="o">}}</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Conclusion</h3>

<p>In this very simple example, the DBConsumer takes each message and saved it as a simple JSON Document into MapR-DB JSON. The table can be used to create any type of application, or using Apache Drill <em>(1.6 or later)</em> to do some analytics.</p>

<p>In a real application the messages will probably be modified, enriched and/or aggregated and then the result be saved into MapR-DB Table. The goal of this sample is just to show that it is easy to integrate MapR Streams and MapR-DB.</p>

<p>You have also other alternative to achieve the same thing using for example:</p>

<ul>
<li>Spark Streaming</li>
<li>3rd Party ETL and Tools</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With MapR Streams]]></title>
    <link href="http://tgrall.github.io/blog/2016/03/10/getting-started-with-mapr-streams/"/>
    <updated>2016-03-10T10:09:32+01:00</updated>
    <id>http://tgrall.github.io/blog/2016/03/10/getting-started-with-mapr-streams</id>
    <content type="html"><![CDATA[<p>You can find a new tutorial that explains how to deploy an Apache Kafka application to MapR Streams, the tutorial is available here:</p>

<ul>
<li><a href="https://www.mapr.com/blog/getting-started-sample-programs-mapr-streams">Getting Started with MapR Streams</a></li>
</ul>


<p>MapR Streams is a new distributed messaging system for streaming event data at scale, and it’s integrated into the MapR converged platform.
MapR Streams uses the Apache Kafka API, so if you’re already familiar with Kafka, you’ll find it particularly easy to get started with MapR Streams.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting Started With Sample Programs for Apache Kafka 0.9]]></title>
    <link href="http://tgrall.github.io/blog/2016/02/10/getting-started-with-sample-programs-for-apache-kafka-0-dot-9/"/>
    <updated>2016-02-10T10:25:44+01:00</updated>
    <id>http://tgrall.github.io/blog/2016/02/10/getting-started-with-sample-programs-for-apache-kafka-0-dot-9</id>
    <content type="html"><![CDATA[<p>Ted Dunning and I have worked on a tutorial that explains how to write your first Kafka application. In this tutorial you will learn how to:</p>

<ul>
<li>Install and start Kafka</li>
<li>Create and Run a producer and a consumer</li>
</ul>


<p>You can find the tutorial on the MapR blog:</p>

<ul>
<li><a href="https://goo.gl/cWmbmY">Getting Started with Sample Programs for Apache Kafka 0.9</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Apache Drill REST API to Build ASCII Dashboard With Node]]></title>
    <link href="http://tgrall.github.io/blog/2015/12/10/using-apache-drill-rest-api-to-build-ascii-dashboard-with-node/"/>
    <updated>2015-12-10T11:30:44+01:00</updated>
    <id>http://tgrall.github.io/blog/2015/12/10/using-apache-drill-rest-api-to-build-ascii-dashboard-with-node</id>
    <content type="html"><![CDATA[<p><a href="http://drill.apache.org">Apache Drill</a> has a hidden gem: an easy to use REST interface. This API can be used to Query, Profile and Configure Drill engine.</p>

<p>In this blog post I will explain how to use Drill REST API to create ascii dashboards using <a href="https://www.npmjs.com/package/blessed-contrib">Blessed Contrib</a>.</p>

<p>The ASCII Dashboard looks like</p>

<p><img class="center" src="http://tgrall.github.io/images/posts/drill_dashboard/dashboard_demo.gif" title="Dashboard" ></p>

<!-- more -->


<h4>Prerequisites</h4>

<ul>
<li>Node.js</li>
<li>Apache Drill 1.2</li>
<li>For this post, you will use the SFO Passengers CSV file available <a href="http://www.flysfo.com/media/facts-statistics/air-traffic-statistics">here</a>.

<ul>
<li>Download this locally, unzip the files and put the CSV into a folder that will be access uzing the following path in Drill : <code>dfs.data.`/airport/*.csv`</code></li>
</ul>
</li>
</ul>


<p><em>Note: I am still using Apache 1.2 to allow this example to be executed in context of a MapR cluster.</em></p>

<h2>The Query and View</h2>

<p>In Drill 1.2, CSV headers are not automatically parsed. (This is one of the new features of 1.3: look for <code>extractHeader</code> in the <a href="https://drill.apache.org/docs/text-files-csv-tsv-psv/">documentation</a>).</p>

<p>For simplicity, remove the first line of the CSV.</p>

<p>The basic query will look like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>SELECT
</span><span class='line'>CAST(SUBSTR(columns[0],1,4) AS INT)  `YEAR`,
</span><span class='line'>CAST(SUBSTR(columns[0],5,2) AS INT) `MONTH`,
</span><span class='line'>columns[1] as `AIRLINE`,
</span><span class='line'>columns[2] as `IATA_CODE`,
</span><span class='line'>columns[3] as `AIRLINE_2`,
</span><span class='line'>columns[4] as `IATA_CODE_2`,
</span><span class='line'>columns[5] as `GEO_SUMMARY`,
</span><span class='line'>columns[6] as `GEO_REGION`,
</span><span class='line'>columns[7] as `ACTIVITY_CODE`,
</span><span class='line'>columns[8] as `PRICE_CODE`,
</span><span class='line'>columns[9] as `TERMINAL`,
</span><span class='line'>columns[10] as `BOARDING_AREA`,
</span><span class='line'>CAST(columns[11] AS DOUBLE) as `PASSENGER_COUNT`
</span><span class='line'>FROM dfs.data.`/airport/*.csv`
</span><span class='line'>LIMIT 10</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s now create a view with these columns: <em>(do not put any limit !)</em></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>CREATE OR REPLACE VIEW dfs.tmp.`airport_data_view` AS
</span><span class='line'>SELECT
</span><span class='line'>CAST(SUBSTR(columns[0],1,4) AS INT)  `YEAR`,
</span><span class='line'>CAST(SUBSTR(columns[0],5,2) AS INT) `MONTH`,
</span><span class='line'>columns[1] as `AIRLINE`,
</span><span class='line'>columns[2] as `IATA_CODE`,
</span><span class='line'>columns[3] as `AIRLINE_2`,
</span><span class='line'>columns[4] as `IATA_CODE_2`,
</span><span class='line'>columns[5] as `GEO_SUMMARY`,
</span><span class='line'>columns[6] as `GEO_REGION`,
</span><span class='line'>columns[7] as `ACTIVITY_CODE`,
</span><span class='line'>columns[8] as `PRICE_CODE`,
</span><span class='line'>columns[9] as `TERMINAL`,
</span><span class='line'>columns[10] as `BOARDING_AREA`,
</span><span class='line'>CAST(columns[11] AS DOUBLE) as `PASSENGER_COUNT`
</span><span class='line'>FROM dfs.data.`/airport/*.csv`</span></code></pre></td></tr></table></div></figure>


<p>So you can now use the view in your query:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>select * from dfs.tmp.`airport_data_view` limit 5;</span></code></pre></td></tr></table></div></figure>


<h2>Use the REST API</h2>

<p>Now that you have the query you can use the REST API to retrieve the data as JSON document over HTTP. Open a terminal and run this curl command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl  \
</span><span class='line'>  --header "Content-type: application/json" \
</span><span class='line'>  --request POST \
</span><span class='line'>  --data '{
</span><span class='line'>    "queryType" : "SQL",
</span><span class='line'>    "query" : "select * from dfs.tmp.`airport_data_view` limit 5 " }' \
</span><span class='line'>  http://localhost:8047/query.json</span></code></pre></td></tr></table></div></figure>


<p>The returned JSON document looks like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>  "columns" : [ "YEAR", "MONTH", ... , "PASSENGER_COUNT" ],
</span><span class='line'>  "rows" : [ {
</span><span class='line'>    "GEO_REGION" : "US",
</span><span class='line'>    "IATA_CODE_2" : "TZ",
</span><span class='line'>      ...
</span><span class='line'>      ...
</span><span class='line'>    "AIRLINE" : "ATA Airlines",
</span><span class='line'>    "MONTH" : "7",
</span><span class='line'>    "ACTIVITY_CODE" : "Deplaned"
</span><span class='line'>  }, {
</span><span class='line'>    "GEO_REGION" : "US",
</span><span class='line'>    "IATA_CODE_2" : "TZ",
</span><span class='line'>    "GEO_SUMMARY" : "Domestic",
</span><span class='line'>    ...
</span><span class='line'>  }
</span><span class='line'>  ]
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>As you can see it is quite simple:</p>

<ul>
<li>a first JSON attribute that list the columns</li>
<li>the list of rows, as JSON documents in an array.</li>
</ul>


<h2>Create a Graph using Node.js &amp; Blessed Contrib</h2>

<p>Let&rsquo;s create a node application.</p>

<p>First you have to include:</p>

<ul>
<li><code>request</code> : to call the REST API</li>
<li><code>blessed</code> : to get a rich Terminal API</li>
<li><code>blessed-contrib</code> : for the dashboard</li>
</ul>


<p>and then create a <code>screen</code> and a <code>bar</code> chard from Contrib.</p>

<p>So the <em>header</em> of your Javascript file looks like:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="kd">var</span> <span class="nx">blessed</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s1">&#39;blessed&#39;</span><span class="p">)</span>
</span><span class='line'>  <span class="p">,</span> <span class="nx">contrib</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s1">&#39;blessed-contrib&#39;</span><span class="p">)</span>
</span><span class='line'>  <span class="p">,</span> <span class="nx">request</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s1">&#39;request&#39;</span><span class="p">)</span>
</span><span class='line'>  <span class="p">,</span> <span class="nx">screen</span> <span class="o">=</span> <span class="nx">blessed</span><span class="p">.</span><span class="nx">screen</span><span class="p">()</span>
</span><span class='line'>  <span class="p">,</span> <span class="nx">bar</span> <span class="o">=</span> <span class="nx">contrib</span><span class="p">.</span><span class="nx">bar</span><span class="p">(</span>
</span><span class='line'>       <span class="p">{</span> <span class="nx">label</span><span class="o">:</span> <span class="s1">&#39;Bar Chart&#39;</span>
</span><span class='line'>       <span class="p">,</span> <span class="nx">barWidth</span><span class="o">:</span> <span class="mi">20</span>
</span><span class='line'>       <span class="p">,</span> <span class="nx">barSpacing</span><span class="o">:</span> <span class="mi">20</span>
</span><span class='line'>       <span class="p">,</span> <span class="nx">maxHeight</span><span class="o">:</span> <span class="mi">9</span>
</span><span class='line'>       <span class="p">,</span> <span class="nx">height</span><span class="o">:</span> <span class="s2">&quot;100%&quot;</span>
</span><span class='line'>       <span class="p">,</span> <span class="nx">width</span><span class="o">:</span> <span class="s2">&quot;100%&quot;</span><span class="p">})</span>
</span></code></pre></td></tr></table></div></figure>


<p>So here we have defined a bar char, that will be populated with the columns and rows. For this we need a query, let&rsquo;s use the number of passengers per year, as follow:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">SELECT</span> <span class="err">`</span><span class="nx">YEAR</span><span class="err">`</span><span class="p">,</span> <span class="nx">SUM</span><span class="p">(</span><span class="err">`</span><span class="nx">PASSENGER_COUNT</span><span class="err">`</span><span class="p">)</span> <span class="nx">FROM</span> <span class="nx">dfs</span><span class="p">.</span><span class="nx">tmp</span><span class="p">.</span><span class="err">`</span><span class="nx">airport_data_view</span><span class="err">`</span> <span class="nx">GROUP</span> <span class="nx">BY</span> <span class="err">`</span><span class="nx">YEAR</span><span class="err">`</span>
</span></code></pre></td></tr></table></div></figure>


<p>The complete Bar Chat application looks like:</p>

<div><script src='https://gist.github.com/00c5d83b85f59d80ad95.js?file=app001.js'></script>
<noscript><pre><code>var blessed = require(&#39;blessed&#39;)
  , contrib = require(&#39;blessed-contrib&#39;)
  , request = require(&#39;request&#39;)
  , screen = blessed.screen()
  , bar = contrib.bar(
       { label: &#39;Bar Chart&#39;
       , barWidth: 12
       , barSpacing: 5
       , maxHeight: 9
       , height: &quot;100%&quot;
       , width: &quot;100%&quot;})

screen.append(bar);

var query =  {
    &quot;queryType&quot; : &quot;SQL&quot;,
    &quot;query&quot; : &quot;select `YEAR`, SUM(`PASSENGER_COUNT`) from dfs.tmp.`airport_data_view` GROUP BY `YEAR`&quot;
};

request(
    {
        url: &#39;http://localhost:8047/query.json&#39;, //URL to hit
        method: &#39;POST&#39;,
        json: query
    }, 
    function(error, response, body){
        var columns = body.columns;
        var data = {
            titles : [],
            data : []
        };
        for(var entry of body.rows){
            data.titles.push(entry[columns[0]]);
            data.data.push(entry[columns[1]]);
        }
        bar.setData(data);
        screen.render()
    });


screen.key([&#39;escape&#39;, &#39;q&#39;, &#39;C-c&#39;], function(ch, key) {
      return process.exit(0);
});
</code></pre></noscript></div>


<ul>
<li>The lines 15-17 contain the query object used by the Drill REST API</li>
<li>The lines 26-38 contain the callback from the HTTP call, and the results values are store in the data object (lines 33-34), and then set in the bar chart (line 36)</li>
</ul>


<h3>Run the &ldquo;Dashboard&rdquo;</h3>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">npm</span> <span class="nx">install</span> <span class="nx">request</span> <span class="nx">blessed</span> <span class="nx">blessed</span><span class="o">-</span><span class="nx">contrib</span>
</span><span class='line'>
</span><span class='line'><span class="nx">node</span> <span class="nx">app001</span><span class="p">.</span><span class="nx">js</span>
</span></code></pre></td></tr></table></div></figure>


<p>This application shows a simple bar chart, in your terminal. Let&rsquo;s now create a richer dashboard.</p>

<h2>Complete Dashboard</h2>

<p>The Bless-Contrib node package allows developer to create rich dashboards that aggregate multiple graphs and could be refresh automatically, as seen in the screencast at the top of this post.</p>

<p>You can find a simple dashboard in this <a href="https://github.com/tgrall/drill-node-dashboard.git">Github repository</a>, once you have cloned it, you just need to run: (be sure that your view is called &lsquo;dfs.tmp.<code>airport_data_view</code>&rsquo;</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">git</span> <span class="nx">clone</span> <span class="nx">https</span><span class="o">:</span><span class="c1">//github.com/tgrall/drill-node-dashboard.git</span>
</span><span class='line'>
</span><span class='line'><span class="nx">cd</span> <span class="nx">drill</span><span class="o">-</span><span class="nx">node</span><span class="o">-</span><span class="nx">dashboard</span>
</span><span class='line'>
</span><span class='line'><span class="nx">npm</span> <span class="nx">install</span>
</span><span class='line'>
</span><span class='line'><span class="nx">node</span> <span class="nx">dashboard</span><span class="p">.</span><span class="nx">js</span> <span class="nx">http</span><span class="o">:</span><span class="c1">//localhost:8047</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can even change the CSV file, for example adding new months, and the line chart on the right will be refreshed automatically.</p>

<p><em>Note: this dashboard sample is very basic and just a quick example explaning how to use Drill REST API in a node.js application</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Convert a CSV File to Apache Parquet With Drill]]></title>
    <link href="http://tgrall.github.io/blog/2015/08/17/convert-csv-file-to-apache-parquet-dot-dot-dot-with-drill/"/>
    <updated>2015-08-17T14:07:00+02:00</updated>
    <id>http://tgrall.github.io/blog/2015/08/17/convert-csv-file-to-apache-parquet-dot-dot-dot-with-drill</id>
    <content type="html"><![CDATA[<p>A very common use case when working with Hadoop is to store and query simple files (CSV, TSV, &hellip;); then to get better performance and efficient storage convert these files into more efficient format, for example <a href="https://parquet.apache.org/">Apache Parquet</a>.</p>

<p><a href="https://parquet.apache.org/">Apache Parquet</a> is a <a href="https://en.wikipedia.org/wiki/Column-oriented_DBMS">columnar storage format</a> available to any project in the Hadoop ecosystem. Apache Parquet has the following characteristics:</p>

<ul>
<li>Self-describing</li>
<li>Columnar format</li>
<li>Language-independent</li>
</ul>


<p>Let&rsquo;s take a concrete example, you can find many interesting Open Data sources that distribute data as CSV files- or equivalent format-. So you can store them into your distributed file system and use them in your applications/jobs/analytics queries. This is not the most efficient way especially when we know that these data won&rsquo;t move that often. So instead of simply storing the CSV let&rsquo;s copy this information into Parquet.</p>

<h3>How to convert CSV files into Parquet files?</h3>

<p>You can use code to achieve this, as you can see in the <a href="https://github.com/Parquet/parquet-compatibility/blob/master/parquet-compat/src/test/java/parquet/compat/test/ConvertUtils.java">ConvertUtils</a> sample/test class. You can use a simpler way with Apache Drill. Drill allows you save the result of a query as Parquet files.</p>

<p>The following steps will show you how to do convert a simple CSV into a Parquet file using Drill.</p>

<!-- more -->


<h4>Prerequisites</h4>

<ul>
<li>Apache Drill : Standalone <a href="https://drill.apache.org/">Apache Drill</a> or using <a href="https://www.mapr.com/products/mapr-sandbox-hadoop/download-sandbox-drill">Apache Drill Sandbox from MapR</a></li>
<li>Some CSV Files: for example <a href="http://www.flysfo.com/media/facts-statistics/air-traffic-statistics">Passenger Dataset from SFO Air Traffic Statistics</a></li>
</ul>


<h4>Querying the CSV file</h4>

<p>Let&rsquo;s execute a basic query:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span> <span class="o">*</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">dfs</span><span class="p">.</span><span class="o">`/</span><span class="n">opendata</span><span class="o">/</span><span class="n">Passenger</span><span class="o">/</span><span class="n">SFO_Passenger_Data</span><span class="o">/</span><span class="n">MonthlyPassengerData_200507_to_201503</span><span class="p">.</span><span class="n">csv</span><span class="o">`</span>
</span><span class='line'><span class="k">LIMIT</span> <span class="mi">5</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="p">[</span><span class="ss">&quot;200507&quot;</span><span class="p">,</span><span class="ss">&quot;ATA Airlines&quot;</span><span class="p">,</span><span class="ss">&quot;TZ&quot;</span><span class="p">,</span><span class="ss">&quot;ATA Airlines&quot;</span><span class="p">,</span><span class="ss">&quot;TZ&quot;</span><span class="p">,</span><span class="ss">&quot;Domestic&quot;</span><span class="p">,</span><span class="ss">&quot;US&quot;</span><span class="p">,</span><span class="ss">&quot;Deplaned&quot;</span><span class="p">,</span><span class="ss">&quot;Low Fare&quot;</span><span class="p">,</span><span class="ss">&quot;Terminal 1&quot;</span><span class="p">,</span><span class="ss">&quot;B&quot;</span><span class="p">,</span><span class="ss">&quot;27271\r&quot;</span><span class="p">]</span>
</span><span class='line'><span class="p">...</span>
</span><span class='line'><span class="p">...</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you can see, by default Drill processes each line as an array of columns, all values being simple String. So if you need to do some operations with these values (projection or where clause) you must use the column index, and cast the value to the proper type. You can see a simple example below:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span>
</span><span class='line'><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">as</span> <span class="o">`</span><span class="nb">DATE</span><span class="o">`</span><span class="p">,</span>
</span><span class='line'><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">as</span> <span class="o">`</span><span class="n">AIRLINE</span><span class="o">`</span><span class="p">,</span>
</span><span class='line'><span class="k">CAST</span><span class="p">(</span><span class="n">columns</span><span class="p">[</span><span class="mi">11</span><span class="p">]</span> <span class="k">AS</span> <span class="n">DOUBLE</span><span class="p">)</span> <span class="k">as</span> <span class="o">`</span><span class="n">PASSENGER_COUNT</span><span class="o">`</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">dfs</span><span class="p">.</span><span class="o">`/</span><span class="n">opendata</span><span class="o">/</span><span class="n">Passenger</span><span class="o">/</span><span class="n">SFO_Passenger_Data</span><span class="cm">/*.csv`</span>
</span><span class='line'><span class="cm">WHERE CAST(columns[11] AS DOUBLE) &lt; 5</span>
</span><span class='line'><span class="cm">;</span>
</span><span class='line'>
</span><span class='line'><span class="cm">+---------+-----------------------------------+------------------+</span>
</span><span class='line'><span class="cm">|  DATE   |              AIRLINE              | PASSENGER_COUNT  |</span>
</span><span class='line'><span class="cm">+---------+-----------------------------------+------------------+</span>
</span><span class='line'><span class="cm">| 200610  | United Airlines - Pre 07/01/2013  | 2.0              |</span>
</span><span class='line'><span class="cm">...</span>
</span><span class='line'><span class="cm">...</span>
</span></code></pre></td></tr></table></div></figure>


<p>We are now ready to create our Parquet files using the &ldquo;Create Table As Select&rdquo; (aka <a href="http://drill.apache.org/docs/create-table-as-ctas-command/">CTAS</a>)</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">alter</span> <span class="k">session</span> <span class="k">set</span> <span class="o">`</span><span class="n">store</span><span class="p">.</span><span class="n">format</span><span class="o">`=</span><span class="s1">&#39;parquet&#39;</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">dfs</span><span class="p">.</span><span class="n">tmp</span><span class="p">.</span><span class="o">`/</span><span class="n">stats</span><span class="o">/</span><span class="n">airport_data</span><span class="o">/`</span> <span class="k">AS</span>
</span><span class='line'><span class="k">SELECT</span>
</span><span class='line'><span class="k">CAST</span><span class="p">(</span><span class="n">SUBSTR</span><span class="p">(</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> <span class="k">AS</span> <span class="nb">INT</span><span class="p">)</span>  <span class="o">`</span><span class="k">YEAR</span><span class="o">`</span><span class="p">,</span>
</span><span class='line'><span class="k">CAST</span><span class="p">(</span><span class="n">SUBSTR</span><span class="p">(</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">AS</span> <span class="nb">INT</span><span class="p">)</span> <span class="o">`</span><span class="k">MONTH</span><span class="o">`</span><span class="p">,</span>
</span><span class='line'><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">as</span> <span class="o">`</span><span class="n">AIRLINE</span><span class="o">`</span><span class="p">,</span>
</span><span class='line'><span class="n">columns</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">as</span> <span class="o">`</span><span class="n">IATA_CODE</span><span class="o">`</span><span class="p">,</span>
</span><span class='line'><span class="n">columns</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">as</span> <span class="o">`</span><span class="n">AIRLINE_2</span><span class="o">`</span><span class="p">,</span>
</span><span class='line'><span class="n">columns</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="k">as</span> <span class="o">`</span><span class="n">IATA_CODE_2</span><span class="o">`</span><span class="p">,</span>
</span><span class='line'><span class="n">columns</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="k">as</span> <span class="o">`</span><span class="n">GEO_SUMMARY</span><span class="o">`</span><span class="p">,</span>
</span><span class='line'><span class="n">columns</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="k">as</span> <span class="o">`</span><span class="n">GEO_REGION</span><span class="o">`</span><span class="p">,</span>
</span><span class='line'><span class="n">columns</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span> <span class="k">as</span> <span class="o">`</span><span class="n">ACTIVITY_CODE</span><span class="o">`</span><span class="p">,</span>
</span><span class='line'><span class="n">columns</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="k">as</span> <span class="o">`</span><span class="n">PRICE_CODE</span><span class="o">`</span><span class="p">,</span>
</span><span class='line'><span class="n">columns</span><span class="p">[</span><span class="mi">9</span><span class="p">]</span> <span class="k">as</span> <span class="o">`</span><span class="n">TERMINAL</span><span class="o">`</span><span class="p">,</span>
</span><span class='line'><span class="n">columns</span><span class="p">[</span><span class="mi">10</span><span class="p">]</span> <span class="k">as</span> <span class="o">`</span><span class="n">BOARDING_AREA</span><span class="o">`</span><span class="p">,</span>
</span><span class='line'><span class="k">CAST</span><span class="p">(</span><span class="n">columns</span><span class="p">[</span><span class="mi">11</span><span class="p">]</span> <span class="k">AS</span> <span class="n">DOUBLE</span><span class="p">)</span> <span class="k">as</span> <span class="o">`</span><span class="n">PASSENGER_COUNT</span><span class="o">`</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">dfs</span><span class="p">.</span><span class="o">`/</span><span class="n">opendata</span><span class="o">/</span><span class="n">Passenger</span><span class="o">/</span><span class="n">SFO_Passenger_Data</span><span class="cm">/*.csv`</span>
</span></code></pre></td></tr></table></div></figure>


<p>That&rsquo;s it! You have now a Parquet file, a single file in our case since our dataset is really small. Apache Drill will create multiples files for the tables depending of the size and configuration your environment.</p>

<p>I invite you to read this Chapter in the Apache Drill documentation to learn more about <a href="https://drill.apache.org/docs/parquet-format/">Drill and Parquet</a>.</p>

<h3>Query Parquet Files</h3>

<p>Now that you have created your Parquet files you can use them in any of your Hadoop processes, but you can also use them in Drill, as follow:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span> <span class="o">*</span>
</span><span class='line'><span class="k">FROM</span> <span class="n">dfs</span><span class="p">.</span><span class="n">tmp</span><span class="p">.</span><span class="o">`/</span><span class="n">stats</span><span class="o">/</span><span class="n">airport_data</span><span class="cm">/*`</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Conclusion</h2>

<p>In this article you have learned how to convert a CSV file using an Apache Drill query.</p>

<p>You can do that with any source supported by Drill, for example from JSON to Parquet, or even a complex join query between multiple data sources. You can also chose a different output format for example JSON, or a CSV.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache Drill : How to Create a New Function?]]></title>
    <link href="http://tgrall.github.io/blog/2015/07/22/apache-drill-how-to-create-a-new-function/"/>
    <updated>2015-07-22T02:32:54+02:00</updated>
    <id>http://tgrall.github.io/blog/2015/07/22/apache-drill-how-to-create-a-new-function</id>
    <content type="html"><![CDATA[<p><a href="https://drill.apache.org/">Apache Drill</a> allows users to explore <em>any type of</em> data using ANSI SQL. This is great, but Drill goes even further than that and allows you to create custom functions to extend the query engine. These custom functions have all the performance of any of the Drill primitive operations, but allowing that performance makes writing these functions a little trickier than you might expect.</p>

<p>In this article, I&rsquo;ll explain step by step how to create and deploy a new function using a very basic example. Note that you can find lot of information about <a href="https://drill.apache.org/docs/develop-custom-functions-introduction/">Drill Custom Functions in the documentation</a>.</p>

<p>Let&rsquo;s create a new function that allows you to mask some characters in a string, and let&rsquo;s make it very simple. The new function will allow user to hide <em>x</em> number of characters from the start and replace then by any characters of their choice. This will look like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>MASK( 'PASSWORD' , '#' , 4 ) =&gt; ####WORD</span></code></pre></td></tr></table></div></figure>


<p>You can find the full project in the following <a href="https://github.com/tgrall/drill-simple-mask-function">Github Repository</a>.</p>

<p>As mentioned before, we could imagine many advanced features to this, but my goal is to focus on the steps to write a custom function, not
so much on what the function does.</p>

<!--more-->


<h2>Prerequisites</h2>

<p>For this you will need:</p>

<ul>
<li>Java Developer Kit 7 or later</li>
<li>Apache Drill 1.1 or later</li>
<li>Maven 3.0 or later</li>
</ul>


<h2>Dependencies</h2>

<p>The following Drill dependency should be added to your maven project</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;dependency&gt;</span>
</span><span class='line'>      <span class="nt">&lt;groupId&gt;</span>org.apache.drill.exec<span class="nt">&lt;/groupId&gt;</span>
</span><span class='line'>      <span class="nt">&lt;artifactId&gt;</span>drill-java-exec<span class="nt">&lt;/artifactId&gt;</span>
</span><span class='line'>      <span class="nt">&lt;version&gt;</span>1.1.0<span class="nt">&lt;/version&gt;</span>
</span><span class='line'><span class="nt">&lt;/dependency&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Source</h2>

<p>The <code>Mask</code> function is an implementation of the <a href="https://github.com/apache/drill/blob/master/exec/java-exec/src/main/java/org/apache/drill/exec/expr/DrillSimpleFunc.java"><code>DrillSimpleFunc</code></a>.</p>

<p>Developers can create 2 types of custom functions:</p>

<ul>
<li>Simple Functions: these functions have a single row as input and produce a single value as output</li>
<li>Aggregation Functions: that will accept multiple rows as input and produce one value as output</li>
</ul>


<p>Simple functions are often referred to as UDF&rsquo;s which stands for user defined function.  Aggregation functions are referred to as UDAF which
stands for user defined aggregation function.</p>

<p>In this example, we just need to transform the value of a column on each row, so a simple function is enough.</p>

<h4>Create the function</h4>

<p>The first step is to implement the <a href="https://github.com/apache/drill/blob/master/exec/java-exec/src/main/java/org/apache/drill/exec/expr/DrillSimpleFunc.java"><code>DrillSimpleFunc</code></a> interface.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kn">package</span> <span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">drill</span><span class="o">.</span><span class="na">contrib</span><span class="o">.</span><span class="na">function</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="kn">import</span> <span class="nn">org.apache.drill.exec.expr.DrillSimpleFunc</span><span class="o">;</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">org.apache.drill.exec.expr.annotations.FunctionTemplate</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="nd">@FunctionTemplate</span><span class="o">(</span>
</span><span class='line'>        <span class="n">name</span><span class="o">=</span><span class="s">&quot;mask&quot;</span><span class="o">,</span>
</span><span class='line'>        <span class="n">scope</span><span class="o">=</span> <span class="n">FunctionTemplate</span><span class="o">.</span><span class="na">FunctionScope</span><span class="o">.</span><span class="na">SIMPLE</span><span class="o">,</span>
</span><span class='line'>        <span class="n">nulls</span> <span class="o">=</span> <span class="n">FunctionTemplate</span><span class="o">.</span><span class="na">NullHandling</span><span class="o">.</span><span class="na">NULL_IF_NULL</span>
</span><span class='line'><span class="o">)</span>
</span><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">SimpleMaskFunc</span> <span class="kd">implements</span> <span class="n">DrillSimpleFunc</span><span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">setup</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">eval</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The behavior of the function is driven by annotations (line 6-10)
  * <em>Name</em> of the function
  * <em>Scope</em> of the function, in our case Simple
  * What to do when the value is NULL, in this case Reverse will just returns NULL</p>

<p>Now we need to implement the logic of the function using <code>setup()</code> and <code>eval()</code> methods.</p>

<ul>
<li><code>setup</code> is self-explanatory, and in our case we do not need to setup anything.</li>
<li><code>eval</code> that is the core of the function. As you can see this method does not have any parameter, and return void. So how does it work?</li>
</ul>


<p>In fact the function will be generated dynamically (see <a href="https://github.com/apache/drill/blob/master/exec/java-exec/src/main/java/org/apache/drill/exec/expr/fn/DrillSimpleFuncHolder.java#L42">DrillSimpleFuncHolder</a>), and the input parameters and output holders are defined using holders by annotations. Let&rsquo;s look into this.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kn">import</span> <span class="nn">io.netty.buffer.DrillBuf</span><span class="o">;</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">org.apache.drill.exec.expr.DrillSimpleFunc</span><span class="o">;</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">org.apache.drill.exec.expr.annotations.FunctionTemplate</span><span class="o">;</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">org.apache.drill.exec.expr.annotations.Output</span><span class="o">;</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">org.apache.drill.exec.expr.annotations.Param</span><span class="o">;</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">org.apache.drill.exec.expr.holders.IntHolder</span><span class="o">;</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">org.apache.drill.exec.expr.holders.NullableVarCharHolder</span><span class="o">;</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">org.apache.drill.exec.expr.holders.VarCharHolder</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'><span class="kn">import</span> <span class="nn">javax.inject.Inject</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="nd">@FunctionTemplate</span><span class="o">(</span>
</span><span class='line'>        <span class="n">name</span> <span class="o">=</span> <span class="s">&quot;mask&quot;</span><span class="o">,</span>
</span><span class='line'>        <span class="n">scope</span> <span class="o">=</span> <span class="n">FunctionTemplate</span><span class="o">.</span><span class="na">FunctionScope</span><span class="o">.</span><span class="na">SIMPLE</span><span class="o">,</span>
</span><span class='line'>        <span class="n">nulls</span> <span class="o">=</span> <span class="n">FunctionTemplate</span><span class="o">.</span><span class="na">NullHandling</span><span class="o">.</span><span class="na">NULL_IF_NULL</span>
</span><span class='line'><span class="o">)</span>
</span><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">SimpleMaskFunc</span> <span class="kd">implements</span> <span class="n">DrillSimpleFunc</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@Param</span>
</span><span class='line'>    <span class="n">NullableVarCharHolder</span> <span class="n">input</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@Param</span><span class="o">(</span><span class="n">constant</span> <span class="o">=</span> <span class="kc">true</span><span class="o">)</span>
</span><span class='line'>    <span class="n">VarCharHolder</span> <span class="n">mask</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@Param</span><span class="o">(</span><span class="n">constant</span> <span class="o">=</span> <span class="kc">true</span><span class="o">)</span>
</span><span class='line'>    <span class="n">IntHolder</span> <span class="n">toReplace</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@Output</span>
</span><span class='line'>    <span class="n">VarCharHolder</span> <span class="n">out</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'>    <span class="nd">@Inject</span>
</span><span class='line'>    <span class="n">DrillBuf</span> <span class="n">buffer</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">setup</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">eval</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>We need to define the parameters of the function. In this case we have 3 parameters, each defined using the <code>@Param</code> annotation.  In addition, we also have to define the returned value using the <code>@Output</code> annotation.</p>

<p>The parameters of our mask function are:</p>

<ul>
<li>A nullable string</li>
<li>The mask char or string</li>
<li>The number of characters to replace starting from the first</li>
</ul>


<p>The function returns :</p>

<ul>
<li>A string</li>
</ul>


<p>For each of these parameters you have to use an holder class. For the <code>String</code>, this is managed by a <code>VarCharHolder</code> or <code>NullableVarCharHolder</code> -lines 21, 24,30- that provides a buffer to manage larger objects in a efficient way. Since we are manipulating a <code>VarChar</code> you also have to inject another buffer that will be used for the output -line 33-. Note that Drill doesn&rsquo;t actually use the Java heap for data being processed in a query but instead keeps this data off the heap and manages the life-cycle for us without using the Java
garbage collector.</p>

<p>We are almost done since we have the proper class, the input/output object, we just need to implement the <code>eval()</code> method itself, and use these objects.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kt">void</span> <span class="nf">eval</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// get the value and replace with</span>
</span><span class='line'>    <span class="n">String</span> <span class="n">maskValue</span> <span class="o">=</span> <span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">drill</span><span class="o">.</span><span class="na">exec</span><span class="o">.</span><span class="na">expr</span><span class="o">.</span><span class="na">fn</span><span class="o">.</span><span class="na">impl</span><span class="o">.</span><span class="na">StringFunctionHelpers</span><span class="o">.</span><span class="na">getStringFromVarCharHolder</span><span class="o">(</span><span class="n">mask</span><span class="o">);</span>
</span><span class='line'>    <span class="n">String</span> <span class="n">stringValue</span> <span class="o">=</span> <span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">drill</span><span class="o">.</span><span class="na">exec</span><span class="o">.</span><span class="na">expr</span><span class="o">.</span><span class="na">fn</span><span class="o">.</span><span class="na">impl</span><span class="o">.</span><span class="na">StringFunctionHelpers</span><span class="o">.</span><span class="na">toStringFromUTF8</span><span class="o">(</span><span class="n">input</span><span class="o">.</span><span class="na">start</span><span class="o">,</span> <span class="n">input</span><span class="o">.</span><span class="na">end</span><span class="o">,</span> <span class="n">input</span><span class="o">.</span><span class="na">buffer</span><span class="o">);</span>
</span><span class='line'>
</span><span class='line'>    <span class="kt">int</span> <span class="n">numberOfCharToReplace</span> <span class="o">=</span> <span class="n">Math</span><span class="o">.</span><span class="na">min</span><span class="o">(</span><span class="n">toReplace</span><span class="o">.</span><span class="na">value</span><span class="o">,</span> <span class="n">stringValue</span><span class="o">.</span><span class="na">length</span><span class="o">());</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// build the mask substring</span>
</span><span class='line'>    <span class="n">String</span> <span class="n">maskSubString</span> <span class="o">=</span> <span class="n">com</span><span class="o">.</span><span class="na">google</span><span class="o">.</span><span class="na">common</span><span class="o">.</span><span class="na">base</span><span class="o">.</span><span class="na">Strings</span><span class="o">.</span><span class="na">repeat</span><span class="o">(</span><span class="n">maskValue</span><span class="o">,</span> <span class="n">numberOfCharToReplace</span><span class="o">);</span>
</span><span class='line'>    <span class="n">String</span> <span class="n">outputValue</span> <span class="o">=</span> <span class="o">(</span><span class="k">new</span> <span class="nf">StringBuilder</span><span class="o">(</span><span class="n">maskSubString</span><span class="o">)).</span><span class="na">append</span><span class="o">(</span><span class="n">stringValue</span><span class="o">.</span><span class="na">substring</span><span class="o">(</span><span class="n">numberOfCharToReplace</span><span class="o">)).</span><span class="na">toString</span><span class="o">();</span>
</span><span class='line'>
</span><span class='line'>    <span class="c1">// put the output value in the out buffer</span>
</span><span class='line'>    <span class="n">out</span><span class="o">.</span><span class="na">buffer</span> <span class="o">=</span> <span class="n">buffer</span><span class="o">;</span>
</span><span class='line'>    <span class="n">out</span><span class="o">.</span><span class="na">start</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>
</span><span class='line'>    <span class="n">out</span><span class="o">.</span><span class="na">end</span> <span class="o">=</span> <span class="n">outputValue</span><span class="o">.</span><span class="na">getBytes</span><span class="o">().</span><span class="na">length</span><span class="o">;</span>
</span><span class='line'>    <span class="n">buffer</span><span class="o">.</span><span class="na">setBytes</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">outputValue</span><span class="o">.</span><span class="na">getBytes</span><span class="o">());</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The code is quite simple:</p>

<ul>
<li>Get the mask itself - line 4</li>
<li>Get the value - line 5</li>
<li>Get the number of character to replace - line 7</li>
<li>Generate a new string with masked values - lines 10/11</li>
<li>Create and populate the output buffer - lines 14 to 17</li>
</ul>


<p>This code does, however, look a bit strange to somebody used to reading Java code. This strangeness arises because the final code that is executed in a query will actually be generated on the fly. This allows Drill to leverage Java&rsquo;s just-in-time (JIT) compiler for maximum speed. To make this work, you have to respect some basic rules:</p>

<ul>
<li><strong>Do not use imports, but instead use the fully qualified class name</strong>, this is what is done on line 10 with the <code>Strings</code> class. (coming from the Google Guava API packaged in Apache Drill)</li>
<li>The <code>ValueHolders</code> classes, in our case <code>VarCharHolder</code> and <code>IntHolder</code> should be manipulated like structs, so you must call helper methods, for example <code>getStringFromVarCharHolder</code> and <code>toStringFromUTF8</code>. Calling methods like <code>toString</code> will result in very bad problems.</li>
</ul>


<p>Starting in Apache Drill 1.3.x, it is mandatory to specify the package name of your function in the <code>./resources/drill-module.conf</code> file as follow:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">drill</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">classpath</span><span class="o">.</span><span class="na">scanning</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">packages</span> <span class="o">:</span> <span class="n">$</span><span class="o">{?</span><span class="n">drill</span><span class="o">.</span><span class="na">classpath</span><span class="o">.</span><span class="na">scanning</span><span class="o">.</span><span class="na">packages</span><span class="o">}</span> <span class="o">[</span>
</span><span class='line'>      <span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">drill</span><span class="o">.</span><span class="na">contrib</span><span class="o">.</span><span class="na">function</span>
</span><span class='line'>    <span class="o">]</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>We are now ready to deploy and test this new function.</p>

<h3>Package</h3>

<p>Once again since, Drill will generate source, <em><strong>you must prepare your package in a way that classes and sources of the function are present in the classpath</strong></em>. This is different from the way that Java code is normally packaged but is necessary for Drill to be able to do the necessary code generation. Drill uses the compiled code to access the annotations and uses the source code to do code generation.</p>

<p>An easy way to do that is to use maven to build your project, and, in particular, use the <a href="https://maven.apache.org/plugins/maven-source-plugin/">maven-source-plugin</a> like this in your <code>pom.xml</code> file:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;plugin&gt;</span>
</span><span class='line'>    <span class="nt">&lt;groupId&gt;</span>org.apache.maven.plugins<span class="nt">&lt;/groupId&gt;</span>
</span><span class='line'>    <span class="nt">&lt;artifactId&gt;</span>maven-source-plugin<span class="nt">&lt;/artifactId&gt;</span>
</span><span class='line'>    <span class="nt">&lt;version&gt;</span>2.4<span class="nt">&lt;/version&gt;</span>
</span><span class='line'>    <span class="nt">&lt;executions&gt;</span>
</span><span class='line'>        <span class="nt">&lt;execution&gt;</span>
</span><span class='line'>            <span class="nt">&lt;id&gt;</span>attach-sources<span class="nt">&lt;/id&gt;</span>
</span><span class='line'>            <span class="nt">&lt;phase&gt;</span>package<span class="nt">&lt;/phase&gt;</span>
</span><span class='line'>            <span class="nt">&lt;goals&gt;</span>
</span><span class='line'>                <span class="nt">&lt;goal&gt;</span>jar-no-fork<span class="nt">&lt;/goal&gt;</span>
</span><span class='line'>            <span class="nt">&lt;/goals&gt;</span>
</span><span class='line'>        <span class="nt">&lt;/execution&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/executions&gt;</span>
</span><span class='line'><span class="nt">&lt;/plugin&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now, when you build using <code>mvn package</code>, Maven will generate 2 jars:</p>

<ul>
<li>The default jar with the classes and resources (<em>drill-simple-mask-1.0.jar</em>)</li>
<li>A second jar with the sources (<em>drill-simple-mask-1.0-sources.jar</em>)</li>
</ul>


<p>Finally you must add a <code>drill-module.conf</code> file in the resources folder of your project, to tell Drill that your jar contains a custom function. If you have no specific configuration to set for your function you can keep this file empty.</p>

<p>We are all set, you can now package and deploy the new function, just package and copy the Jars into the Drill 3rd party folder; $DRILL_HOME/jars/3rdparty , where $DRILL_HOME being your Drill installation folder.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>mvn clean package
</span><span class='line'>
</span><span class='line'>cp target/*.jar  $DRILL_HOME/jars/3rdparty
</span></code></pre></td></tr></table></div></figure>


<p>Restart drill.</p>

<h3>Run !</h3>

<p>You should now be able to use your function in your queries:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'>SELECT MASK(first_name, &#39;*&#39; , 3) FIRST , MASK(last_name, &#39;#&#39;, 7) LAST  FROM cp.`employee.json` LIMIT 5;
</span><span class='line'>+----------+------------+
</span><span class='line'>|  FIRST   |    LAST    |
</span><span class='line'>+----------+------------+
</span><span class='line'>| ***ri    | ######     |
</span><span class='line'>| ***rick  | #######    |
</span><span class='line'>| ***hael  | ######     |
</span><span class='line'>| ***a     | #######ez  |
</span><span class='line'>| ***erta  | #######    |
</span><span class='line'>+----------+------------+
</span></code></pre></td></tr></table></div></figure>


<h2>Conclusion</h2>

<p>In this simple project you have learned how to write, deploy and use a custom Apache Drill Function. You can now extend this to create your own function.</p>

<p>One important thing to remember when extending Apache Drill (using a custom function, storage plugin or format), is that Drill runtime is generating dynamically lot of code. This means you may have to use a very specific pattern when writing and deploying your extensions. With our basic function this meant we had to:</p>

<ul>
<li>deploy <strong>classes AND sources</strong></li>
<li>use <strong>fully Qualified Class Names</strong></li>
<li>use value holder classes and helper methods to manipulate parameters
*</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MongoDB : Playing With Arrays]]></title>
    <link href="http://tgrall.github.io/blog/2015/04/21/mongodb-playing-with-arrays/"/>
    <updated>2015-04-21T15:45:39+02:00</updated>
    <id>http://tgrall.github.io/blog/2015/04/21/mongodb-playing-with-arrays</id>
    <content type="html"><![CDATA[<p>As you know,  you have many differences between relational and document databases. The biggest, for the developer, is probably the data model: Row versus Document. This is particularly true when we talk about &ldquo;relations&rdquo; versus &ldquo;embedded documents <em>(or values)</em>&rdquo;. Let&rsquo;s look at some examples, then see what are the various operations provided by MongoDB to help you to deal with this.</p>

<!-- more -->


<p>I won&rsquo;t use this post to go in all the details about &ldquo;document design&rdquo;, but I just want to focus on the operations you can to with these arrays/list (so useful operations once you have chosen to embed documents).</p>

<p>Let&rsquo;s use a very simple example based on e-commerce platform, with two document types : <strong><em>user</em></strong> and <strong><em>orders</em></strong>.</p>

<p>The first thing you have, is a simple list of values into a JSON array. Let&rsquo;s look at a user profile where user have a list of interests  (field <code>interested_by</code>) :</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;_id&quot;</span> <span class="p">:</span> <span class="mi">654321</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;first_name&quot;</span> <span class="p">:</span> <span class="s2">&quot;John&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;last_name&quot;</span> <span class="p">:</span> <span class="s2">&quot;Doe&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;interested_by&quot;</span> <span class="p">:</span> <span class="p">[</span> <span class="s2">&quot;electronics&quot;</span><span class="p">,</span> <span class="s2">&quot;sports&quot;</span><span class="p">,</span> <span class="s2">&quot;music&quot;</span> <span class="p">],</span>
</span><span class='line'>  <span class="nt">&quot;address&quot;</span> <span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;name&quot;</span> <span class="p">:</span> <span class="s2">&quot;John Doe&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;company&quot;</span> <span class="p">:</span> <span class="s2">&quot;Resultri&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;street&quot;</span> <span class="p">:</span> <span class="s2">&quot;1015 Mapple Street&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;city&quot;</span> <span class="p">:</span> <span class="s2">&quot;San Francisco&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;state&quot;</span> <span class="p">:</span> <span class="s2">&quot;CA&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;zip_code&quot;</span> <span class="p">:</span> <span class="mi">94105</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Another thing you do with array, is to represent &ldquo;one-to-many&rdquo; relations. These relations in your RDBMS are based on multiple tables and foreign keys.
In document databases, like MongoDB, these relations are, most of the time, represented using an <em>array of documents</em>, something like (look at the <code>items</code> field):</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;_id&quot;</span> <span class="p">:</span> <span class="mi">45218468309</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;date&quot;</span> <span class="p">:</span> <span class="err">ISODate(</span><span class="s2">&quot;2015-01-28T09:40:50.615Z&quot;</span><span class="err">)</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;customer&quot;</span> <span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;id&quot;</span> <span class="p">:</span> <span class="mi">654321</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;name&quot;</span> <span class="p">:</span> <span class="s2">&quot;John Doe&quot;</span>
</span><span class='line'>  <span class="p">},</span>
</span><span class='line'>  <span class="nt">&quot;ship_to&quot;</span> <span class="p">:</span> <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;name&quot;</span> <span class="p">:</span> <span class="s2">&quot;John Doe&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;company&quot;</span> <span class="p">:</span> <span class="s2">&quot;Resultri&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;street&quot;</span> <span class="p">:</span> <span class="s2">&quot;1015 Mapple Street&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;city&quot;</span> <span class="p">:</span> <span class="s2">&quot;San Francisco&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;state&quot;</span> <span class="p">:</span> <span class="s2">&quot;CA&quot;</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;zip_code&quot;</span> <span class="p">:</span> <span class="mi">94105</span>
</span><span class='line'>  <span class="p">},</span>
</span><span class='line'>  <span class="nt">&quot;items&quot;</span> <span class="p">:</span> <span class="p">[</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;sku&quot;</span> <span class="p">:</span> <span class="s2">&quot;WA34R&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;description&quot;</span> <span class="p">:</span> <span class="s2">&quot;Wireless Qwerty Keyboard&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;quantity&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;unit_price&quot;</span> <span class="p">:</span> <span class="mf">41.5</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;price&quot;</span> <span class="p">:</span> <span class="mf">41.5</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;vat&quot;</span> <span class="p">:</span> <span class="mi">20</span>
</span><span class='line'>    <span class="p">},</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;sku&quot;</span> <span class="p">:</span> <span class="s2">&quot;MW003&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;description&quot;</span> <span class="p">:</span> <span class="s2">&quot;MiWatch&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;quantity&quot;</span> <span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;unit_price&quot;</span> <span class="p">:</span> <span class="mi">245</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;price&quot;</span> <span class="p">:</span> <span class="mi">490</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;vat&quot;</span> <span class="p">:</span> <span class="mi">20</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="p">]</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Documents above are not necessary complete, I just want to focus on the various operations you can do on these arrays.</p>

<p><em>Note: you can add these documents into your MongoDB instance, I will use the collections <code>customers</code> and <code>orders</code>.</em></p>

<h3>Adding new interest to the user</h3>

<p>To achieve this you have 2 operators that you can use in your update: <a href="1"><code>$push</code></a> and <a href="2"><code>$addToSet</code></a>. Since these one a very simple I won&rsquo;t go into too much details.</p>

<p>The <code>$push</code> will add the value at the end of the list, if the value already exits it will be added (many copies), this is why it makes sense to use the <code>$addToSet</code> operator, that only add the value if the value does not already exist in the array.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="err">db.customers.update(</span>
</span><span class='line'>  <span class="p">{</span> <span class="nt">&quot;_id&quot;</span> <span class="p">:</span> <span class="mi">654321</span>  <span class="p">}</span><span class="err">,</span>
</span><span class='line'>  <span class="p">{</span> <span class="nt">&quot;$addToSet&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="nt">&quot;interested_by&quot;</span> <span class="p">:</span>  <span class="s2">&quot;sports&quot;</span><span class="p">}</span>  <span class="p">}</span>
</span><span class='line'><span class="err">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>This update command above <strong>will not change</strong> the document since the &ldquo;sports&rdquo; value is already in the list.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="err">db.customers.update(</span>
</span><span class='line'>  <span class="p">{</span> <span class="nt">&quot;_id&quot;</span> <span class="p">:</span> <span class="mi">654321</span>  <span class="p">}</span><span class="err">,</span>
</span><span class='line'>  <span class="p">{</span> <span class="nt">&quot;$addToSet&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="nt">&quot;interested_by&quot;</span> <span class="p">:</span>  <span class="s2">&quot;books&quot;</span><span class="p">}</span>  <span class="p">}</span>
</span><span class='line'><span class="err">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>This <strong>will add</strong> the &ldquo;books&rdquo; value at the end of the list.</p>

<p>If the attribute <code>interested_by</code> does not exist in the document, it will be added with one single entry (here the <code>$push</code> is working the same way ).</p>

<p>If the attribute is not an array, the database will not do anything and return the error <a href="3">#16837</a> <em>&ldquo;The field &lsquo;first_name&rsquo; must be an array but is of type String in document&rdquo;</em>.</p>

<p>Here we use <em>interest</em>, but you can imagine doing the same thing for tagging, or any other business use case with a list of values.</p>

<h3>Adding a new item into an order</h3>

<p>The previous case, is very simple since it is a scalar value. Now I need to add a new order line, it is not harder than before:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="err">db.orders.update(</span>
</span><span class='line'>  <span class="p">{</span> <span class="nt">&quot;_id&quot;</span> <span class="p">:</span> <span class="mi">45218468309</span>   <span class="p">}</span><span class="err">,</span>
</span><span class='line'>  <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;$push&quot;</span> <span class="p">:</span> <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;items&quot;</span> <span class="p">:</span> <span class="p">{</span>
</span><span class='line'>        <span class="nt">&quot;sku&quot;</span> <span class="p">:</span> <span class="s2">&quot;MO001&quot;</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;description&quot;</span> <span class="p">:</span> <span class="s2">&quot;Bluetooth mouse&quot;</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;quantity&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;unit_price&quot;</span> <span class="p">:</span> <span class="mf">20.00</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;price&quot;</span> <span class="p">:</span> <span class="mf">20.00</span><span class="p">,</span>
</span><span class='line'>        <span class="nt">&quot;vat&quot;</span> <span class="p">:</span> <span class="mf">20.00</span>
</span><span class='line'>      <span class="p">}</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="err">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>So you can see now that the value is added at the end of the list.</p>

<h3>Updating an item in the order</h3>

<p>Let&rsquo;s look at another requirement. Now I need to update for example the quantity of one of the line. In your relational application it is <em>easy</em> in the sense that you have one single record to update(in reality it is a different story since application are using complex data layer).</p>

<p>You can do the same, meaning you can only update the <em>items</em> directly in the array &ndash; (no need to replace the full document or list like I see too often).</p>

<p>For this, you  just need to use the <code>update</code> and <code>$set</code> and specify the positional operation <code>$</code>.</p>

<p>The <code>$</code> operator is a placeholder for the first entry in the array that match the filter (query document) sent to the update/findAndModify command.</p>

<p>In our example, to update a specific line in the order</p>

<p>The proper way is simply to use the an update and <code>$set</code>,
 but you have to select the exact entry in the array in your filter. For example in our case we want to update the number of mouses and the price, this will look like:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="err">db.orders.update(</span>
</span><span class='line'>  <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;_id&quot;</span> <span class="p">:</span> <span class="mi">45218468309</span><span class="p">,</span>
</span><span class='line'>    <span class="nt">&quot;items.sku&quot;</span> <span class="p">:</span> <span class="s2">&quot;MO001&quot;</span>
</span><span class='line'>  <span class="p">}</span><span class="err">,</span>
</span><span class='line'>  <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;$set&quot;</span> <span class="p">:</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>        <span class="nt">&quot;items.$&quot;</span> <span class="p">:</span> <span class="p">{</span>
</span><span class='line'>          <span class="nt">&quot;sku&quot;</span> <span class="p">:</span> <span class="s2">&quot;MO001&quot;</span><span class="p">,</span>
</span><span class='line'>          <span class="nt">&quot;description&quot;</span> <span class="p">:</span> <span class="s2">&quot;Bluetooth mouse&quot;</span><span class="p">,</span>
</span><span class='line'>          <span class="nt">&quot;quantity&quot;</span> <span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span><span class='line'>          <span class="nt">&quot;unit_price&quot;</span> <span class="p">:</span> <span class="mf">20.00</span><span class="p">,</span>
</span><span class='line'>          <span class="nt">&quot;price&quot;</span> <span class="p">:</span> <span class="mf">40.00</span><span class="p">,</span>
</span><span class='line'>          <span class="nt">&quot;vat&quot;</span> <span class="p">:</span> <span class="mf">20.00</span>
</span><span class='line'>        <span class="p">}</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="err">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you can see, the <code>$</code> operator is telling MongoDB to update one specific entry in the array.</p>

<h2>Remove an item from the Array</h2>

<p>You have learned so far that you can easily query and add values into an array; using the same appraoch you can also remove entry in an array. This is done using 3 operators : <code>$pop</code>, <code>$pull</code> and <code>$pullAll</code></p>

<ul>
<li>The <code>$pop</code> operator removes one element from the end of the array</li>
<li>The <code>$pull</code> operator removes <em>all</em> elements in the array that match a specified value.</li>
<li>The <code>$pullAll</code> operator removes <em>all</em> elements in the array that match any of the specified values.</li>
</ul>


<h4>Remove some interests from a customer</h4>

<p>For example, let&rsquo;s remove the &ldquo;electronics&rdquo; interest from the customer id 654321.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="err">db.customers.update(</span>
</span><span class='line'>  <span class="p">{</span> <span class="nt">&quot;_id&quot;</span> <span class="p">:</span> <span class="mi">654321</span>  <span class="p">}</span><span class="err">,</span>
</span><span class='line'>  <span class="p">{</span> <span class="nt">&quot;$pull&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="nt">&quot;interested_by&quot;</span> <span class="p">:</span>  <span class="s2">&quot;electronics&quot;</span><span class="p">}</span>  <span class="p">}</span>
</span><span class='line'><span class="err">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>If you want to remove sports and music interest from the customer you can use the <code>$pullAll</code> operator as follow:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="err">db.customers.update(</span>
</span><span class='line'>  <span class="p">{</span> <span class="nt">&quot;_id&quot;</span> <span class="p">:</span> <span class="mi">654321</span>  <span class="p">}</span><span class="err">,</span>
</span><span class='line'>  <span class="p">{</span> <span class="nt">&quot;$pullAll&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="nt">&quot;interested_by&quot;</span> <span class="p">:</span>  <span class="p">[</span><span class="s2">&quot;sports&quot;</span><span class="p">,</span><span class="s2">&quot;music&quot;</span><span class="p">]}</span>  <span class="p">}</span>
</span><span class='line'><span class="err">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>Here we use <em>interest</em>, but you can imagine doing the same thing for tagging, or any other business use case with a list of values.</p>

<h4>Remove item into an order</h4>

<p>Using the same operator you can also remove a line order (item) from an order document, for example let&rsquo;s remove the line with the item MO001 (Bluetooth mouse). For this we can use the <code>$pull</code> operator with the proper sku.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="err">db.orders.update(</span>
</span><span class='line'>  <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;_id&quot;</span> <span class="p">:</span> <span class="mi">45218468309</span><span class="p">,</span>
</span><span class='line'>  <span class="p">}</span><span class="err">,</span>
</span><span class='line'>  <span class="p">{</span>
</span><span class='line'>    <span class="nt">&quot;$pull&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="nt">&quot;items&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="nt">&quot;sku&quot;</span> <span class="p">:</span> <span class="s2">&quot;MO001&quot;</span> <span class="p">}</span> <span class="p">}</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="err">);</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Conclusion</h3>

<p>In this article you have learn how to create/edit arrays in JSON documents.</p>

<p>In addition to all the update operators, MongoDB provides many operators for querying arrays such as  <a href="5">$size</a> or <a href="4"><code>$elemMatch</code></a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction to MongoDB Security]]></title>
    <link href="http://tgrall.github.io/blog/2015/02/04/introduction-to-mongodb-security/"/>
    <updated>2015-02-04T18:55:30+01:00</updated>
    <id>http://tgrall.github.io/blog/2015/02/04/introduction-to-mongodb-security</id>
    <content type="html"><![CDATA[<p>Last week at the Paris MUG, I had a quick chat about security and MongoDB, and I have decided to create this post that explains how to configure out of the box security available in MongoDB.</p>

<p>You can find all information about MongoDB Security in following documentation chapter:</p>

<ul>
<li><a href="http://docs.mongodb.org/manual/security/">http://docs.mongodb.org/manual/security/</a></li>
</ul>


<p><img class="center" src="http://tgrall.github.io/images/posts/intro-mongodb-security/password.jpg"></p>

<p>In this post, <em>I won&rsquo;t go</em> into the detail about how to deploy your database in a secured environment (DMZ/Network/IP/Location/&hellip;)</p>

<p>I will focus on <strong>Authentication</strong> and <strong>Authorization</strong>, and provide you the steps to secure the access to your database and data.</p>

<p>I have to mention that by default, when you install and start MongoDB, security is not enabled. Just to make it easier to work with.</p>

<p>The first part of the security is the <strong>Authentication</strong>, you have multiple choices documented <a href="http://docs.mongodb.org/manual/core/authentication/">here</a>. Let&rsquo;s focus on &ldquo;MONGODB-CR&rdquo; mechanism.</p>

<p>The second part is <strong>Authorization</strong> to select what a user can do or not once he is connected to the database. The documentation about authorization is available <a href="http://docs.mongodb.org/manual/core/authorization/">here</a>.</p>

<p>Let&rsquo;s now document how-to:</p>

<ol>
<li>Create an Administrator User</li>
<li>Create Application Users</li>
</ol>


<p>For each type of users I will show how to grant specific permissions.</p>

<!-- more -->


<h2>1. Start MongoDB</h2>

<p>As I said before, by default security is not enabled when you start MongoDB; so the first think to do is to enable it using the <code>--auth</code> parameter.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; mkdir /data/db
</span><span class='line'>
</span><span class='line'>&gt; mongod --auth
</span><span class='line'>
</span><span class='line'>....
</span><span class='line'>....
</span><span class='line'>2015-02-04T06:56:37.875+0100 [conn1] note: no users configured in admin.system.users, allowing localhost access
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<p>MongoDB is starting, and until you have created a user you can connect from localhost to create some users (especially the administrator one). This is what is called the <em>localhost exception</em>.</p>

<p>Note: I am here documenting security in simple configuration, I invite you to look to the documentation when deploying a <a href="http://docs.mongodb.org/v2.2/administration/sharded-clusters/#sharded-cluster-security-considerations">Sharded cluster</a>.</p>

<p>Now that we have started MongoDB, we can create users.</p>

<h2>2. Create Admin User</h2>

<p>The first thing is to create an admin user, that can also create users, So we have to:</p>

<ol>
<li>go to the mongo shell</li>
<li>connect to the `admin&#8217; database</li>
<li>create a user and assign him the role <a href="http://docs.mongodb.org/manual/reference/built-in-roles/#userAdminAnyDatabase"><strong>userAdminAnyDatabase</strong></a></li>
</ol>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>use admin
</span><span class='line'>
</span><span class='line'>var user = {
</span><span class='line'>  "user" : "admin",
</span><span class='line'>  "pwd" : "manager",
</span><span class='line'>  roles : [
</span><span class='line'>      {
</span><span class='line'>          "role" : "userAdminAnyDatabase",
</span><span class='line'>          "db" : "admin"
</span><span class='line'>      }
</span><span class='line'>  ]
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>db.createUser(user);
</span><span class='line'>
</span><span class='line'>exit</span></code></pre></td></tr></table></div></figure>


<p>Now that you have created a user, in a MongoDB running with <code>--auth</code>, anonymous connections won&rsquo;t be able to do do anything with the database.</p>

<p>You can test for example to execute <code>show dbs</code> or <code>db.coll.insert({'x':0})</code> commands, you&rsquo;ll see authorization errors.</p>

<h3>Connect with the Admnistrator user</h3>

<p>Now that we have an admin user you can connect to the database with this user:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>&gt; mongo admin -u admin -p
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p>Our admin user, has the role <strong>userAdminAnyDatabase</strong>. With this role you can manage user; but this role cannot read/write data from application datatabases/collections.</p>

<p>So we need now to create a new user for our &ldquo;eCommerce&rdquo; application.</p>

<h2>3. Create Application User</h2>

<p>Now we will create a new user <em>website</em> that is responsible of the ecommerce database.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; mongo admin -u admin -p
</span><span class='line'>
</span><span class='line'>use ecommerce
</span><span class='line'>
</span><span class='line'>var user = {
</span><span class='line'>  "user" : "website",
</span><span class='line'>  "pwd" : "abc123",
</span><span class='line'>  roles : [
</span><span class='line'>      {
</span><span class='line'>          "role" : "readWrite",
</span><span class='line'>          "db" : "ecommerce"
</span><span class='line'>      }
</span><span class='line'>  ]
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>db.createUser(user);
</span><span class='line'>
</span><span class='line'>exit
</span></code></pre></td></tr></table></div></figure>


<p>This user will be able to read/write on the <em>ecommerce</em> database</p>

<h3>Connect with the application user</h3>

<p>Using the mongo shell you can now connect and create/query data</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; mongo ecommerce -u website -p
</span><span class='line'>
</span><span class='line'>db.products.insert({ "title" : "MongoDB in Action"  });
</span><span class='line'>
</span><span class='line'>db.products.findOne();
</span><span class='line'>
</span><span class='line'>db.products.update({}, {"$set" : { "type" : "book" } })
</span></code></pre></td></tr></table></div></figure>


<p>As you can see this user has the perfect profile for your application.</p>

<p>Note, that if you try to query or modify another database with this user you will receive authorization exceptions.</p>

<h2>Create a reporting user (Read Only)</h2>

<p>You may need in your application, user that can only read data, let&rsquo;s say in all databases. For this you just need to assign the role <strong>readAnyDatabase</strong>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>
</span><span class='line'>&gt; mongo admin -u admin -p
</span><span class='line'>
</span><span class='line'>var user = {
</span><span class='line'>  "user" : "reporting",
</span><span class='line'>  "pwd" : "abc123",
</span><span class='line'>  roles : [
</span><span class='line'>      {
</span><span class='line'>          "role" : "readAnyDatabase",
</span><span class='line'>          "db" : "admin"
</span><span class='line'>      }
</span><span class='line'>  ]
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>db.createUser(user);
</span><span class='line'>
</span><span class='line'>exit</span></code></pre></td></tr></table></div></figure>


<p>This user will be able to query all the databases and collections, including <code>show dbs</code> command.</p>

<p>Let&rsquo;s connect with the reporting user:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; mongo admin -u reporting -p
</span><span class='line'>
</span><span class='line'>show dbs
</span><span class='line'>
</span><span class='line'>use ecommerce
</span><span class='line'>
</span><span class='line'>db.products.find();
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure>


<p>If you try to inser/update/delete document you will receive an exception.</p>

<h2>Add new role to a user</h2>

<p>Let&rsquo;s now see how to add a new role to a user. For example I want to let the admin the power of read and write any database. For this I just need to add the role <strong>readWriteAnyDatabase</strong> to the admin user.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; mongo admin -u admin -p
</span><span class='line'>
</span><span class='line'>db.grantRolesToUser(
</span><span class='line'>  "admin",
</span><span class='line'>  [{ "role" : "readWriteAnyDatabase", "db" : "admin" }]
</span><span class='line'>)
</span><span class='line'>
</span><span class='line'>db.getUser("admin");
</span></code></pre></td></tr></table></div></figure>


<p>Using the <code>db.grantRolesToUser</code> command I have added the role to the admin user, and using the <code>db.getUser</code> I can look at the user profile.</p>

<p>Now, admin user should be able to create new databases, collections and documents, let&rsquo;s try:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>use hr
</span><span class='line'>
</span><span class='line'>db.employees.insert({ "name":"John Doe", "hire_date" : new Date() });
</span><span class='line'>
</span><span class='line'>db.organization.insert({ "name" : "Development" });
</span><span class='line'>
</span><span class='line'>db.employees.findOne();
</span></code></pre></td></tr></table></div></figure>


<h2>Create and use custom roles</h2>

<p>Another feature that is used a lot around security is related to the roles. In some case you want to provide multiple roles to a user, for example:</p>

<ul>
<li>all permission on database <em>ecommerce</em></li>
<li>read the collection <em>employees</em> in database <em>hr</em></li>
</ul>


<p>For this you can create a role that provide all the permissions and assign it to users. Let&rsquo;s do that using admin user.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>use admin
</span><span class='line'>
</span><span class='line'>var role = {
</span><span class='line'>  "role"  : "webSiteManagerRole",
</span><span class='line'>  privileges : [
</span><span class='line'>      {
</span><span class='line'>          "resource": {"db" : "hr", "collection" : "employees"},
</span><span class='line'>          "actions": ["find"]
</span><span class='line'>      }
</span><span class='line'>  ],
</span><span class='line'>  "roles" : [
</span><span class='line'>      {
</span><span class='line'>          "role" : "readWrite",
</span><span class='line'>          "db" : "ecommerce"
</span><span class='line'>      }
</span><span class='line'>  ]
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>db.createRole( role );
</span><span class='line'>
</span><span class='line'>var user = {
</span><span class='line'>  "user" : "master",
</span><span class='line'>  "pwd" : "abc123",
</span><span class='line'>  roles : [
</span><span class='line'>      {
</span><span class='line'>          "role" : "webSiteManagerRole",
</span><span class='line'>          "db" : "admin"
</span><span class='line'>      }
</span><span class='line'>  ]
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>db.createUser(user);
</span><span class='line'>
</span><span class='line'>exit
</span></code></pre></td></tr></table></div></figure>


<p>If you connect now with the user &ldquo;master&rdquo;, you will see that, the user:</p>

<ul>
<li>can do anything you want in the ecommerce database</li>
<li>can read the &ldquo;hr.employees&rdquo; collection, on only this</li>
<li>cannot do anything else.</li>
</ul>


<h2>Roles and Privileges</h2>

<p>As you have seen in the previous section, you can create roles, and assign privileges to these roles. This is very powerful and you can really control each action on the database.</p>

<p>I am inviting you to look in detail to the built-in roles and privileges, this will help you a lot to select the proper ones for your application:</p>

<ul>
<li><a href="http://docs.mongodb.org/manual/reference/built-in-roles/">Built-in Roles</a></li>
<li><a href="http://docs.mongodb.org/manual/reference/privilege-actions/">Privileges</a></li>
</ul>


<h2>Conclusion</h2>

<p>In this blog post I quickly explained, how to:</p>

<ul>
<li>Use MongoDB Authentication</li>
<li>Create Users</li>
<li>Assign Roles and Privileges for Users.</li>
</ul>


<p>It is interesting to know that everything that I have showed you in the shell could be done from a user interface in <a href="http://mms.mongodb.com">MMS</a></p>
]]></content>
  </entry>
  
</feed>
